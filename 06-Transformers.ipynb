{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe6bc3e",
   "metadata": {},
   "source": [
    "# Transformer: 开创序列建模新范式\n",
    "\n",
    "Transformer模型，由Vaswani等人在2017年的论文《Attention Is All You Need》中提出，彻底改变了自然语言处理（NLP）乃至更广泛的序列建模领域。它放弃了传统循环神经网络（RNN）和卷积神经网络（CNN）对序列的顺序处理，转而完全依赖于“自注意力”（Self-Attention）机制，实现了前所未有的并行化能力和长距离依赖捕获能力。\n",
    "\n",
    "## 1.序列建模的挑战：RNN与CNN的局限\n",
    "\n",
    "在Transformer出现之前，序列建模主要依赖于RNN及其变体（如LSTM、GRU）和CNN。\n",
    "\n",
    "**RNN**：\n",
    "\n",
    "RNN通过维护一个隐藏状态($h_t$)来处理序列，每个时间步的隐藏状态都依赖于前一个时间步的隐藏状态和当前输入：\n",
    "$$h_t = f_A (h_{t-1}) + f_B (x_t)$$\n",
    "输出则通常基于当前隐藏状态：\n",
    "$$y_t = f_C (h_t)$$\n",
    "这种固有的序列依赖性使得RNN在训练时难以并行化，因为$h_t$的计算必须等待$h_{t-1}$完成。虽然可以通过截断反向传播(BPTT)进行有限并行，但长序列处理仍然面临梯度消失/爆炸和计算效率低下的问题。\n",
    "\n",
    "**CNN**:\n",
    "\n",
    "CNN通过卷积核在序列上滑动来提取特征。通过堆叠多层CNN，可以扩大感受野，从而捕获更长的依赖关系。不同于RNN，CNN的计算可以在多个位置并行进行。\n",
    "$$b^i = \\text{CNN}(a^i \\text{ for } j \\in \\text{window}(i))$$\n",
    "虽然CNN可以并行化，但其感受野的大小受限于卷积核大小和层数，捕获全局长距离依赖可能需要非常深的层次。\n",
    "\n",
    "## 2.自注意力机制：核心突破\n",
    "\n",
    "Transformer模型的核心是自注意力机制，它允许模型在处理序列中的每个元素时，同时“关注”到序列中的其它元素，并根据它们之间的相关性分配不同的权重。这种机制使得模型能够一步到位地捕获全局依赖，极大地提升了并行化能力。\n",
    "\n",
    "### 2.1. Q、K、V的计算\n",
    "\n",
    "对于输入序列中的每个元素$a^i$（通常是词嵌入或前一层的输出），我们通过三个不同的线性变换（矩阵乘法）来生成查询（Query,$q^i$）、键（Key,$k^i$）和值（Value,$v^i$）向量：\n",
    "$$q^i = W^Q a^i, k^i = W^K a^i, v^i = W^V a^i$$\n",
    "其中，$W^Q,W^K,W^V$是可学习的权重矩阵。\n",
    "\n",
    "### 2.2.缩放点积注意力\n",
    "\n",
    "接下来，计算查询向量$q^i$与所有键向量$k^j$之间的相似度，通常使用点积。为了防止点积结果过大导致softmax函数在梯度较小时饱和，通常会除以键向量维度$d_k$的平方根。\n",
    "$$score(q^i, k^j) = \\frac{q^i \\cdot k^j}{\\sqrt{d_k}}$$\n",
    "然后，将这些相似度分数通过softmax函数进行归一化，得到注意力权重$\\alpha^{i,j}$。这些权重表示当模型处理第$i$个元素时，应该对第$j$个元素给予多少关注。\n",
    "$$\\alpha^{i,j} = \\text{softmax}_j \\left(\\frac{q^i \\cdot k^j}{\\sqrt{d_k}}\\right)$$\n",
    "最后，使用这些注意力权重对所有值向量$v^j$进行加权求和，得到第$i$个元素的上下文向量$b^i$。\n",
    "$$b^i = \\sum\\limits_{j=1}^{N} \\alpha^{i,j} v^j$$\n",
    "其中，$N$是序列长度。\n",
    "\n",
    "整个过程可以高效地通过矩阵乘法并行计算：\n",
    "$$Q = X W^Q, K = X W^K, V = X W^V$$\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "其中，$X$是输入序列的矩阵表示，每一行代表一个元素（例如词嵌入）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9149256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 8, 5, 64])\n",
      "Attention weights shape: torch.Size([2, 8, 5, 6])\n",
      "Output shape (no mask): torch.Size([2, 8, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q: (batch_size, num_heads, seq_len_q, d_k)\n",
    "        # K: (batch_size, num_heads, seq_len_kv, d_k)\n",
    "        # V: (batch_size, num_heads, seq_len_kv, d_v)\n",
    "        # mask: (batch_size, 1, seq_len_q, seq_len_kv) or (1, 1, seq_len_q, seq_len_kv)\n",
    "\n",
    "        # (Q @ K.transpose(-2, -1)) : (batch_size, num_heads, seq_len_q, seq_len_kv)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            # For masked attention (e.g., decoder), set masked positions to a very small number\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) # Use -1e9 instead of float('-inf') for numerical stability\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1) # Softmax along the sequence_kv dimension\n",
    "\n",
    "        # (attention_weights @ V): (batch_size, num_heads, seq_len_q, d_v)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 2\n",
    "    seq_len_q = 5 # query sequence length\n",
    "    seq_len_kv = 6 # key/value sequence length\n",
    "    num_heads = 8\n",
    "    d_model = 512\n",
    "    d_k = d_v = d_model // num_heads # Dimension of Q, K, V for each head\n",
    "\n",
    "    # Simulate Q, K, V for a single head for simplicity, normally they come from linear layers\n",
    "    # In a real MultiHeadAttention, these would be computed for all heads first\n",
    "    Q_single_head = torch.randn(batch_size, num_heads, seq_len_q, d_k)\n",
    "    K_single_head = torch.randn(batch_size, num_heads, seq_len_kv, d_k)\n",
    "    V_single_head = torch.randn(batch_size, num_heads, seq_len_kv, d_v)\n",
    "\n",
    "    # Example mask for causal attention (decoder)\n",
    "    # Prevents attending to future tokens\n",
    "    causal_mask = torch.tril(torch.ones(seq_len_q, seq_len_kv)).bool()\n",
    "    causal_mask = causal_mask.view(1, 1, seq_len_q, seq_len_kv) # (1, 1, seq_len_q, seq_len_kv)\n",
    "\n",
    "    attention_module = ScaledDotProductAttention(d_k)\n",
    "    output, attn_weights = attention_module(Q_single_head, K_single_head, V_single_head, mask=causal_mask)\n",
    "\n",
    "    print(f\"Output shape: {output.shape}\") # (batch_size, num_heads, seq_len_q, d_v)\n",
    "    print(f\"Attention weights shape: {attn_weights.shape}\") # (batch_size, num_heads, seq_len_q, seq_len_kv)\n",
    "\n",
    "    # Without mask for encoder self-attention\n",
    "    output_no_mask, attn_weights_no_mask = attention_module(Q_single_head, K_single_head, V_single_head)\n",
    "    print(f\"Output shape (no mask): {output_no_mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5018ba",
   "metadata": {},
   "source": [
    "## 3.多头注意力机制\n",
    "\n",
    "为了让模型能够从不同角度和不同表示子空间中捕获信息，Transformer引入了多头注意力（Multi-Head Attention）机制。它并行运行多个缩放点积注意力，每个注意力头学习不同的$W^Q,W^K,W^V$矩阵。\n",
    "\n",
    "具体步骤如下：\n",
    "- **1.独立计算**：将输入$X$（或前一层的输出）通过$h$组独立的线性变换，为每个头生成不同的$Q_m, K_m, V_m (m=1,...,h)$。 $$Q_m = X W_m^Q, K_m = X W_m^K, V_m = X W_m^V$$\n",
    "- **2.并行注意力**：对每个头独立执行缩放点积注意力，得到$h$个输出的$head_m$。 $$head_m = \\text{Attention}(Q_m, K_m, V_m)$$\n",
    "- **3.拼接与投影**：将所有头的输出拼接起来，并通过一个最终的线形层$W^O$进行投影，得到最终的多头注意力输出。$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1,...,head_h)W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a44de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Output shape: torch.Size([3, 10, 512])\n",
      "Multi-Head Attention Weights shape: torch.Size([3, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = d_model // num_heads # Dimension of Q, K, V for each head\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query, key, value: (batch_size, seq_len, d_model)\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1. Linear projections for Q, K, V: (batch_size, seq_len, d_model)\n",
    "        Q = self.wq(query)\n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "\n",
    "        # 2. Reshape for multi-head: (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply Scaled Dot-Product Attention\n",
    "        # x: (batch_size, num_heads, seq_len_q, d_k)\n",
    "        # attn_weights: (batch_size, num_heads, seq_len_q, seq_len_kv)\n",
    "        x, attn_weights = self.attention(Q, K, V, mask)\n",
    "\n",
    "        # 4. Concat heads: (batch_size, seq_len_q, num_heads, d_k) -> (batch_size, seq_len_q, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 5. Final linear projection\n",
    "        output = self.wo(x)\n",
    "        return output, attn_weights\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == '__main__':\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    seq_len = 10\n",
    "    batch_size = 3\n",
    "\n",
    "    # Simulate input embeddings\n",
    "    input_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    multi_head_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    output, attn_weights = multi_head_attn(input_embeddings, input_embeddings, input_embeddings)\n",
    "\n",
    "    print(f\"Multi-Head Attention Output shape: {output.shape}\")\n",
    "    print(f\"Multi-Head Attention Weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e323dc",
   "metadata": {},
   "source": [
    "## 4. 位置编码\n",
    "\n",
    "自注意力机制本身是置换不变性的（permutation-invariant），这意味着它不考虑序列中元素的顺序。然而，对于语言等序列数据，顺序信息至关重要。为了解决这个问题，Transformer引入了**位置编码（Positional Encoding）**，将位置信息注入到输入嵌入中。\n",
    "\n",
    "位置编码是一个与词嵌入维度相同的向量，它包含关于词在序列中绝对或相对位置的信息。最常用的位置编码是基于正弦和余弦函数：\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "其中，$pos$是序列中的位置，$i$是位置编码向量中的维度索引，$d_{model}$是模型的维度。\n",
    "将位置编码与词嵌入简单相加：\n",
    "$$a^i = \\text{word\\_embedding}(x^i) + PE(i)$$\n",
    "由于这些位置编码是固定的（不可学习），它们可以编码相对位置信息，并且可以泛化到训练中未见过的更长序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bdf2e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings shape: torch.Size([3, 10, 512])\n",
      "Output with positional encoding shape: torch.Size([3, 10, 512])\n",
      "Are inputs and outputs different? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # Add batch dimension\n",
    "        self.register_buffer('pe', pe) # Register as buffer, not a parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        # pe: (1, max_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == '__main__':\n",
    "    d_model = 512\n",
    "    seq_len = 10\n",
    "    batch_size = 3\n",
    "\n",
    "    # Simulate input embeddings\n",
    "    input_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    pos_encoder = PositionalEncoding(d_model)\n",
    "    output_with_pos = pos_encoder(input_embeddings)\n",
    "\n",
    "    print(f\"Input embeddings shape: {input_embeddings.shape}\")\n",
    "    print(f\"Output with positional encoding shape: {output_with_pos.shape}\")\n",
    "    # Verify that positional encoding is added (output should be different from input)\n",
    "    print(f\"Are inputs and outputs different? {not torch.allclose(input_embeddings, output_with_pos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3a69e",
   "metadata": {},
   "source": [
    "## 5. Transformer的整体架构\n",
    "\n",
    "Transformer模型采用编码器-解码器（Encoder-Decoder）结构。\n",
    "\n",
    "**编码器（Encoder）**：\n",
    "编码器由$N$个相同的层堆叠而成。每层包含两个子层：\n",
    "1.  **多头自注意力层（Multi-Head Self-Attention）**：处理编码器自身的输入序列，捕获序列内部的依赖关系。\n",
    "2.  **前馈网络（Feed-Forward Network, FFN）**：一个简单的全连接前馈网络，对自注意力层的输出进行非线性变换。\n",
    "每个子层后都接着一个“残差连接”（Residual Connection）和“层归一化”（Layer Normalization）。残差连接有助于解决深层网络的梯度消失问题，层归一化则稳定训练并加速收敛。\n",
    "$$x + \\text{Sublayer}(x)$$\n",
    "$$\\text{LayerNorm}(x)$$\n",
    "\n",
    "**解码器（Decoder）**：\n",
    "解码器也由$N$个相同的层堆叠而成。每层包含三个子层：\n",
    "1.  **带掩码的多头自注意力层（Masked Multi-Head Self-Attention）**：与编码器中的自注意力层类似，但增加了一个**掩码**机制。这个掩码确保解码器在生成当前词时，只能关注到序列中已经生成的词，而不能“偷看”未来的词，这对于语言生成任务至关重要。\n",
    "2.  **多头交叉注意力层（Multi-Head Cross-Attention）**：这个注意力层将查询（Q）来自解码器的前一输出，而键（K）和值（V）来自编码器的输出。这允许解码器在生成输出序列时，关注输入序列中的相关部分。\n",
    "3.  **前馈网络（Feed-Forward Network, FFN）**：与编码器中的FFN相同。\n",
    "同样，每个子层后也接着残差连接和层归一化。\n",
    "\n",
    "**输出层**：\n",
    "解码器的最终输出通过一个线性层和一个softmax层，预测下一个词的概率分布。\n",
    "\n",
    "## 6. 正则化与效率优化\n",
    "\n",
    "**层归一化（Layer Normalization）**：\n",
    "Layer Normalization对每个样本的特征进行归一化，使其均值为0、方差为1，与Batch Normalization不同，它不依赖于批量大小，因此更适合序列任务和变长输入。\n",
    "$$LN(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "其中 $\\mu$ 和 $\\sigma$ 是单个样本在特征维度上的均值和标准差，$\\gamma$ 和 $\\beta$ 是可学习的缩放和偏移参数。\n",
    "\n",
    "**长上下文效率优化**：\n",
    "标准Transformer的自注意力机制具有二次时间复杂度$O(N^2)$，对于非常长的序列（如数万甚至数百万个token）会带来巨大的计算和内存开销。为了解决这一问题，研究者提出了多种优化方法：\n",
    "*   **Transformer-XL**：引入循环机制和相对位置编码，允许模型重用前一个段的隐藏状态，从而扩展上下文窗口。\n",
    "*   **Reformer**：使用局部敏感哈希（Locality Sensitive Hashing, LSH）来近似注意力计算，将复杂度从$O(N^2)$降低到$O(N \\log N)$。\n",
    "*   **Longformer**：引入稀疏注意力机制，结合局部窗口注意力、全局注意力等模式，减少计算量。\n",
    "*   **Performer**：利用核方法（Kernel Methods）将softmax注意力近似为线性注意力，将复杂度降低到$O(N)$。\n",
    "\n",
    "## 7. 预训练与微调范式\n",
    "\n",
    "Transformer的巨大成功，很大程度上得益于**预训练-微调（Pre-train and Fine-tune）**范式。\n",
    "\n",
    "**预训练（Pre-training）**：\n",
    "在大规模无标注文本数据上（如维基百科、书籍、网页数据）进行自监督学习，让模型学习语言的通用表示和知识。常见的预训练任务包括：\n",
    "*   **掩码语言模型（Masked Language Modeling, MLM）**：随机掩盖输入序列中的一部分词，然后让模型预测这些被掩盖的词。这使得模型能够学习双向上下文信息（如BERT）。\n",
    "*   **下一个词预测（Next Token Prediction）**：根据前文预测序列中的下一个词。这使得模型能够学习单向生成能力（如GPT系列）。\n",
    "*   **自监督学习**：更广义地，通过输入数据自身的一部分来预测另一部分，而无需人工标注。\n",
    "\n",
    "**微调（Fine-tuning）**：\n",
    "将预训练好的模型作为一个强大的特征提取器，在特定任务（如文本分类、问答、命名实体识别）的小规模有标注数据上进行少量训练。通常，会在预训练模型顶部添加一个轻量级的任务特定层。由于模型已经学习了通用语言知识，微调通常只需要少量数据和较短的训练时间就能达到很好的效果。\n",
    "*   **Adapter**：一种参数高效的微调方法，在预训练模型的Transformer层之间插入小型、可训练的模块（Adapter）。在微调时，只训练这些Adapter模块和顶部的任务头，而冻结预训练模型的大部分参数，大大减少了可训练参数的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a114ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model output shape (logits): torch.Size([4, 10])\n",
      "\n",
      "Trainable parameters in fine-tuned model:\n",
      "pre_trained_model.core_layers.weight: requires_grad=False\n",
      "pre_trained_model.core_layers.bias: requires_grad=False\n",
      "task_head.classifier.weight: requires_grad=True\n",
      "task_head.classifier.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PreTrainedModel(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        # Simplified: A pre-trained model might have many layers\n",
    "        # Here, just a dummy linear layer representing its core\n",
    "        self.core_layers = nn.Linear(d_model, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.core_layers(x))\n",
    "\n",
    "class TaskSpecificHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Typically, take the representation of the [CLS] token or average pool\n",
    "        # Here, we assume x is the pooled representation for simplicity\n",
    "        return self.classifier(x)\n",
    "\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, pre_trained_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.pre_trained_model = pre_trained_model\n",
    "        # Freeze pre-trained model parameters\n",
    "        for param in self.pre_trained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.task_head = TaskSpecificHead(pre_trained_model.core_layers.out_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.pre_trained_model(x)\n",
    "        # In a real scenario, features would be pooled/selected for classification\n",
    "        # Here, we just use the last token's feature or average for simplicity\n",
    "        pooled_features = features.mean(dim=1) # Example pooling\n",
    "        output = self.task_head(pooled_features)\n",
    "        return output\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == '__main__':\n",
    "    d_model = 768\n",
    "    num_classes = 10\n",
    "    seq_len = 50\n",
    "    batch_size = 4\n",
    "\n",
    "    # 1. Instantiate pre-trained model\n",
    "    pre_trained_model = PreTrainedModel(d_model)\n",
    "    # Load actual pre-trained weights here if available\n",
    "    # pre_trained_model.load_state_dict(torch.load('pretrained_weights.pth'))\n",
    "\n",
    "    # 2. Create fine-tuned model with a new task head\n",
    "    fine_tuned_model = FineTunedModel(pre_trained_model, num_classes)\n",
    "\n",
    "    # Simulate input data (e.g., encoded text sequence)\n",
    "    input_data = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Forward pass\n",
    "    output = fine_tuned_model(input_data)\n",
    "    print(f\"Fine-tuned model output shape (logits): {output.shape}\")\n",
    "\n",
    "    # Check which parameters are trainable\n",
    "    print(\"\\nTrainable parameters in fine-tuned model:\")\n",
    "    for name, param in fine_tuned_model.named_parameters():\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4458412",
   "metadata": {},
   "source": [
    "# Titans：学习在测试时记忆的新架构\n",
    "\n",
    "尽管Transformer极其强大，但其二次方的计算和内存复杂度限制了其在超长上下文（百万级甚至更高）场景的应用。此外，标准的Transformer和现代线性循环模型在处理长时依赖时，往往受限于固定大小的记忆状态或缺乏有效的遗忘机制。Titans架构正是在此背景下提出，它引入了一种“神经长时记忆模块”，旨在提升模型在超长上下文中的记忆和推理能力，并能在测试时持续学习。\n",
    "\n",
    "## 1. 核心理念与记忆视角\n",
    "\n",
    "Titans将注意力机制和新提出的神经记忆模块视为互补的记忆系统：\n",
    "*   **注意力机制**：由于其有限的上下文窗口和精确的依赖建模，被视为**短期记忆（Short-term Memory）**。它擅长捕获当前上下文内的即时、细粒度依赖。\n",
    "*   **神经记忆模块**：由于其能够学习记忆历史上下文并存储信息到其参数中，被视为**长期记忆（Long-term, more persistent, Memory）**。它专注于高效存储和检索远距离的过去信息，解决传统模型记忆溢出的问题。\n",
    "\n",
    "Titans的动机源于对人类记忆系统的启发，认为有效的学习范式应包含独立而又相互关联的记忆模块，能够主动从数据中学习并抽象化过去的历史。这引发了以下关键问题：\n",
    "*   **Q1：记忆的良好结构是什么？**\n",
    "*   **Q2：合适的记忆更新机制是什么？**\n",
    "*   **Q3：良好的记忆检索过程是什么？**\n",
    "*   **Q4：如何设计一个有效的架构，整合不同的互联记忆模块？**\n",
    "*   **Q5：是否需要深度记忆模块来有效存储/记忆长期历史？**\n",
    "\n",
    "## 2. 深度神经长时记忆模块（Deep Neural Long-Term Memory Module, LMM）\n",
    "\n",
    "Titans的核心创新在于其深度神经长时记忆模块。这个模块被设计为一个“元上下文模型”（meta in-context model），它在测试时也能学习如何记忆/存储数据到其参数中。\n",
    "\n",
    "**2.1 学习机制与“惊喜”度量**\n",
    "\n",
    "LMM的训练被视为一个在线学习问题。其核心思想是，模型对输入的“惊喜度”（surprise）越高，该输入就越值得记忆。惊喜度通过损失函数对记忆参数的梯度来衡量。梯度越大，说明当前输入与过去的记忆差异越大，越“出乎意料”，因此越值得更新记忆。\n",
    "\n",
    "基本更新规则：\n",
    "$$M_t = M_{t-1} - \\theta_t \\nabla l(M_{t-1}; x_t)$$\n",
    "其中，$M_t$是时间步$t$的记忆模块参数，$l(\\cdot; \\cdot)$是关联记忆损失函数，$x_t$是当前输入，$\\theta_t$是控制瞬间惊喜（Momentary Surprise）被整合到记忆中的程度的参数。\n",
    "\n",
    "为了更好地管理记忆容量并捕获序列中的信息流，Titans引入了动量项和数据依赖的衰减机制，从而改进了惊喜度量：\n",
    "$$S_t = \\eta_t S_{t-1} - \\theta_t \\nabla l(M_{t-1}; x_t)$$\n",
    "$$M_t = (1 - \\alpha_t) M_{t-1} + S_t$$\n",
    "这里：\n",
    "*   $S_t$是动量元素，它积累了跨时间的惊喜信息。\n",
    "*   $\\eta_t$是数据依赖的惊喜衰减因子，控制过去惊喜的衰减速度。\n",
    "*   $\\alpha_t \\in [0, 1]$ 是门控机制，作为**自适应遗忘门**，灵活控制记忆的遗忘程度。例如，当$\\alpha_t \\to 0$时，记忆保留过去抽象；当$\\alpha_t \\to 1$时，记忆完全清除。这使得记忆能够根据需要忘记不再相关的信息，有效管理有限容量。\n",
    "\n",
    "这种更新公式与带有动量和权重衰减的梯度下降在形式上相似。权重衰减（即遗忘门$\\alpha_t$）可以看作是一种门控机制，用于忘记不需要的过去数据。\n",
    "\n",
    "**关联记忆损失函数**：\n",
    "LMM的目标是学习关联键值对。给定输入$x_t$，通过线性层投影得到键$k_t = x_t W_K$和值$v_t = x_t W_V$。损失函数定义为：\n",
    "$$l(M_{t-1}; x_t) = ||M_{t-1}(k_t) - v_t||^2$$\n",
    "这意味着记忆模块$M$学习在给定$k_t$时预测出$v_t$。在训练的内循环中，优化$M$的权重，而在外循环中优化整个架构的其他参数（如$W_K, W_V$）。\n",
    "\n",
    "**记忆模块架构**：\n",
    "记忆模块$M$本身可以是一个深度神经网络（如多层感知机MLP）。文章指出，深度记忆模块（即$L_M \\ge 2$层）在实践中比线性模型更有效，因为它能够学习更复杂的历史数据抽象。\n",
    "\n",
    "**2.2 记忆检索**\n",
    "检索信息时，LMM通过其前向传播（推理模式，不更新权重）来提取与查询$q_t = x_t W_O$对应的有用信息：\n",
    "$$Y_t = M^*(q_t)$$\n",
    "$M^*$表示记忆模块在推理模式下的前向传播，不进行权重更新。\n",
    "\n",
    "**2.3 并行化训练**\n",
    "尽管LMM的更新规则看起来是循环的，但Titans通过将 mini-batch 梯度下降过程重构为纯矩阵乘法和求和运算，实现了并行化训练。这借鉴了 Yu Sun 等人 (2024) 的工作，通过分块（chunking）和关联扫描（associative scan）技术，可以在GPU/TPU等硬件加速器上高效运行。这解决了RNN传统上难以并行训练的痛点，同时保持了推理的快速性。\n",
    "\n",
    "#### Python代码示例：神经记忆模块（LMM）的核心更新逻辑\n",
    "\n",
    "这部分代码将展示LMM的核心更新机制，包括惊喜度量、动量更新和遗忘门。请注意，这里的`NeuralMemoryModule`是一个简化的概念模型，其内部的`self.memory_mlp`会是实际学习键值映射的神经网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd723a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Memory State ---\n",
      "-0.01617690548300743\n",
      "\n",
      "--- Time Step 1 ---\n",
      "Retrieved Output shape: torch.Size([2, 64])\n",
      "Memory State mean after update: -0.008026497438549995\n",
      "\n",
      "--- Time Step 2 ---\n",
      "Retrieved Output shape: torch.Size([2, 64])\n",
      "Memory State mean after update: -0.0040644388645887375\n",
      "\n",
      "--- Time Step 3 ---\n",
      "Retrieved Output shape: torch.Size([2, 64])\n",
      "Memory State mean after update: -0.0018436424434185028\n",
      "\n",
      "--- Time Step 4 ---\n",
      "Retrieved Output shape: torch.Size([2, 64])\n",
      "Memory State mean after update: -0.000720654265023768\n",
      "\n",
      "--- Time Step 5 ---\n",
      "Retrieved Output shape: torch.Size([2, 64])\n",
      "Memory State mean after update: -8.186293416656554e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralMemoryModule(nn.Module):\n",
    "    def __init__(self, d_model, memory_depth=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # M is the memory module (e.g., a simple MLP)\n",
    "        # For simplicity, we'll make its parameters trainable\n",
    "        # In a real setup, it might be a deeper MLP\n",
    "        layers = []\n",
    "        in_dim = d_model\n",
    "        for _ in range(memory_depth):\n",
    "            layers.append(nn.Linear(in_dim, d_model))\n",
    "            layers.append(nn.ReLU()) # Or SiLU as mentioned in the paper\n",
    "            in_dim = d_model\n",
    "        self.memory_mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # Learnable parameters for surprise and forgetting\n",
    "        # In the paper, these could be data-dependent,\n",
    "        # here we use simple learnable scalars (can be expanded to be functions of x_t)\n",
    "        self.log_alpha_t = nn.Parameter(torch.zeros(1)) # For (1 - alpha_t)\n",
    "        self.log_eta_t = nn.Parameter(torch.zeros(1))   # For eta_t (surprise decay)\n",
    "        self.log_theta_t = nn.Parameter(torch.zeros(1))  # For theta_t (surprise incorporation)\n",
    "\n",
    "        # Projection layers for K and V from input x_t\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model) # For query projection\n",
    "\n",
    "        # Initial momentum state\n",
    "        self.register_buffer('S_t_minus_1', torch.zeros(d_model, d_model)) # Assuming matrix-valued S\n",
    "\n",
    "        # Initial memory state M_t_minus_1 (parameters of self.memory_mlp are the memory)\n",
    "        # We'll treat the weights of memory_mlp as M_t conceptually.\n",
    "        # The update rule in the paper applies to M_t itself, which here would be\n",
    "        # modifying the weights of memory_mlp directly.\n",
    "        # This is the meta-learning aspect, which is harder to implement as a simple nn.Module.\n",
    "        # For demonstration, we'll simplify and show how a 'memory_state' could be updated.\n",
    "        # A more faithful implementation might involve a custom optimizer or gradient manipulations.\n",
    "\n",
    "        # For this simplified implementation, let's say M_t represents a state that influences output\n",
    "        # rather than being the MLP weights themselves being directly M_t.\n",
    "        # We'll use a dummy memory_state for the loss calculation illustration.\n",
    "        self.memory_state = nn.Parameter(torch.randn(d_model, d_model)) # A dummy matrix-valued memory state\n",
    "\n",
    "    def calculate_associative_loss(self, current_memory_state, k_t, v_t):\n",
    "        # This is the l(M_t-1; x_t) = ||M_t-1(k_t) - v_t||^2 part\n",
    "        # Here, current_memory_state represents M_t-1.\n",
    "        # We simplify M(k) as a matrix-vector product for illustration.\n",
    "        # In the paper, M is an MLP, so M(k) would be self.memory_mlp(k_t)\n",
    "        \n",
    "        # Conceptual M(k_t). If M is an MLP, this would be self.memory_mlp(k_t)\n",
    "        # For a matrix-valued M_t, M_t(k_t) could be M_t @ k_t.\n",
    "        # Let's approximate it as a linear projection for the loss calculation for simplicity.\n",
    "        predicted_v_t = torch.matmul(current_memory_state, k_t.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = F.mse_loss(predicted_v_t, v_t)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x_t, is_training_memory=False):\n",
    "        # x_t: (batch_size, d_model)\n",
    "\n",
    "        k_t = self.wk(x_t) # Key from current input\n",
    "        v_t = self.wv(x_t) # Value from current input\n",
    "        q_t = self.wo(x_t) # Query from current input\n",
    "\n",
    "        # Memory Update Logic (only if is_training_memory=True, otherwise just inference)\n",
    "        if is_training_memory:\n",
    "            # 1. Calculate the surprise term (gradient of loss w.r.t. memory_state)\n",
    "            # This is the tricky part for a simple nn.Module.\n",
    "            # In a true meta-learning setup, you'd compute gradients explicitly or use higher-order differentiation.\n",
    "            # For this example, we'll simulate the gradient term.\n",
    "            \n",
    "            # Create a dummy optimizer for the memory_state (conceptual, not actual training here)\n",
    "            # This is just to demonstrate how gradient would be used.\n",
    "            temp_memory_state_for_grad = self.memory_state.clone().detach().requires_grad_(True)\n",
    "            loss_for_grad = self.calculate_associative_loss(temp_memory_state_for_grad, k_t.mean(dim=0), v_t.mean(dim=0))\n",
    "            loss_for_grad.backward()\n",
    "            gradient_term = temp_memory_state_for_grad.grad\n",
    "\n",
    "            eta_t = torch.exp(self.log_eta_t)\n",
    "            theta_t = torch.exp(self.log_theta_t)\n",
    "            alpha_t = torch.sigmoid(self.log_alpha_t) # Use sigmoid to keep alpha_t in [0,1]\n",
    "\n",
    "            # Update momentum S_t\n",
    "            S_t = eta_t * self.S_t_minus_1 - theta_t * gradient_term\n",
    "            self.S_t_minus_1 = S_t.detach() # Detach to prevent backprop through S_t-1's history\n",
    "\n",
    "            # Update memory M_t (self.memory_state)\n",
    "            self.memory_state.data = (1 - alpha_t) * self.memory_state.data + S_t\n",
    "\n",
    "        # Retrieval (Y_t = M*(q_t))\n",
    "        # Here, M*(q_t) is the forward pass of memory_mlp using q_t.\n",
    "        # The memory_state influences the 'knowledge' of memory_mlp,\n",
    "        # or in a simpler setup, could be directly used in the retrieval.\n",
    "        # Let's say the memory_state *is* the retrieval knowledge for simplicity.\n",
    "        # Output of memory is based on query (q_t) and stored memory (memory_state).\n",
    "        # This is a conceptual retrieval; a real one might be M_mlp(q_t) where M_mlp's weights are M_t.\n",
    "        \n",
    "        # Simulating M*(q_t) by using q_t and memory_state.\n",
    "        # e.g., a linear projection using the memory_state as a weight\n",
    "        # Or, if memory_mlp represents M, then the output from memory is self.memory_mlp(q_t)\n",
    "        \n",
    "        # Option 1: Retrieval as a direct use of memory_state (simpler)\n",
    "        retrieved_info = torch.matmul(q_t, self.memory_state.transpose(-2, -1)) # (batch_size, d_model)\n",
    "\n",
    "        # Option 2: Retrieval via memory_mlp (more faithful to \"deep memory\")\n",
    "        # In this case, `self.memory_state` would be more abstract, controlling `memory_mlp` behavior.\n",
    "        # The paper describes `M_t` as the parameters of the MLP, which is a meta-learning setup.\n",
    "        # For now, let's use Option 1 for illustrative clarity on `memory_state` itself.\n",
    "        # retrieved_info = self.memory_mlp(q_t)\n",
    "\n",
    "\n",
    "        return retrieved_info # This would be Y_t\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == '__main__':\n",
    "    d_model = 64\n",
    "    batch_size = 2\n",
    "    memory_depth = 2\n",
    "\n",
    "    lmm = NeuralMemoryModule(d_model, memory_depth)\n",
    "    optimizer = optim.Adam(lmm.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"--- Initial Memory State ---\")\n",
    "    print(lmm.memory_state.mean().item())\n",
    "\n",
    "    # Simulate training over several time steps (chunks/segments)\n",
    "    for i in range(5):\n",
    "        print(f\"\\n--- Time Step {i+1} ---\")\n",
    "        x_t = torch.randn(batch_size, d_model) # Current input\n",
    "\n",
    "        # In paper, M_t is updated *during* forward pass (test time learning)\n",
    "        # Here, we trigger it via is_training_memory flag\n",
    "        retrieved_output = lmm(x_t, is_training_memory=True) # Memory updates internally\n",
    "\n",
    "        print(f\"Retrieved Output shape: {retrieved_output.shape}\")\n",
    "        print(f\"Memory State mean after update: {lmm.memory_state.mean().item()}\")\n",
    "\n",
    "        # In a full model, this retrieved_output would be used by the main architecture\n",
    "        # For simplicity, we just print here.\n",
    "        # The meta-learning loss would guide the updates to self.memory_mlp's weights\n",
    "        # (or self.memory_state in this simplified model)\n",
    "        # A true meta-learning loop would look different, with inner/outer optimizers.\n",
    "        # This example primarily shows the update rule's components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbc7fd",
   "metadata": {},
   "source": [
    "## 3. 持久记忆（Persistent Memory）\n",
    "\n",
    "除了可适应的神经长时记忆，Titans还引入了**持久记忆（Persistent Memory）**。这是一组可学习但与输入数据**无关**的参数$P = [P_1, P_2, \\dots, P_{N_p}]$。\n",
    "\n",
    "*   **作用**：持久记忆旨在编码任务相关的元信息或通用知识。它类似于模型在训练过程中获得的“先验知识”或“如何做任务”的抽象。\n",
    "*   **整合方式**：持久记忆参数被拼接到输入序列的开头，成为注意力机制的额外上下文：\n",
    "    $$X_{new} = [P_1, P_2, \\dots, P_{N_p}] \\parallel X$$\n",
    "*   **动机**：\n",
    "    1.  **记忆视角**：存储任务知识的抽象，独立于上下文。\n",
    "    2.  **前馈网络视角**：FFN中的权重可以看作是数据无关的注意力权重，持久记忆类似。\n",
    "    3.  **技术视角**：缓解因果掩码对序列开头token的隐性偏见，通过提供可学习的“锚点”来更有效地重新分配注意力权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb25f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent tokens shape: torch.Size([5, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self, d_model, num_persistent_tokens):\n",
    "        super().__init__()\n",
    "        self.persistent_tokens = nn.Parameter(torch.randn(num_persistent_tokens, d_model))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.persistent_tokens\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == '__main__':\n",
    "    d_model = 64\n",
    "    num_persistent_tokens = 5\n",
    "\n",
    "    persistent_mem = PersistentMemory(d_model, num_persistent_tokens)\n",
    "    tokens = persistent_mem()\n",
    "\n",
    "    print(f\"Persistent tokens shape: {tokens.shape}\") # (num_persistent_tokens, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8db69",
   "metadata": {},
   "source": [
    "## 4. Titans架构变体：整合记忆模块\n",
    "\n",
    "Titans提出了三种将神经长时记忆模块和持久记忆整合到深度学习架构中的主要变体。这些变体在效率和效果之间提供了不同的权衡。\n",
    "\n",
    "**4.1 记忆即上下文（Memory as a Context, MAC）**\n",
    "这是Titans的首选设计。在这种架构中，长时记忆被视为当前信息的额外上下文。\n",
    "1.  **序列分块**：将输入序列$X$分成固定大小的块$S^{(i)}$。\n",
    "2.  **记忆检索**：使用当前块$S^{(t)}$作为查询，从前一个时间步的长期记忆$M_{t-1}$中检索相关历史信息$h_t$。\n",
    "    $$h_t = M_{t-1}^*(q_t), \\quad \\text{where } q_t = S^{(t)}W_O$$\n",
    "3.  **注意力输入**：将持久记忆$P$、检索到的历史信息$h_t$和当前块$S^{(t)}$拼接起来，作为注意力模块的输入。\n",
    "    $$Y_t = \\text{Attn}([P \\parallel h_t \\parallel S^{(t)}])$$\n",
    "4.  **记忆更新与输出**：注意力模块的输出$Y_t$用于更新神经长时记忆$M_t$（在测试时仍进行学习），并生成最终输出。\n",
    "    $$M_t = M_{t-1}(Y_t) \\quad (\\text{memory's internal update})$$\n",
    "    $$\\text{Output}_t = Y_t \\text{projected}$$\n",
    "    这种设计允许注意力机制根据当前数据决定是否需要长期记忆信息，从而更好地管理记忆容量。\n",
    "\n",
    "**4.2 记忆即门控（Memory as Gating, MAG）**\n",
    "在这种变体中，长期记忆通过门控机制与核心注意力分支融合。\n",
    "1.  **输入拼接**：将持久记忆$P$拼接到原始输入$X$的开头：$\\tilde{X} = [P \\parallel X]$。\n",
    "2.  **短期注意力**：在拼接后的输入上应用滑动窗口注意力（Sliding Window Attention, SWA）作为短期记忆，生成输出$y$：$y = \\text{SWA}(\\tilde{X})$。\n",
    "3.  **记忆融合**：神经长时记忆$M(X)$直接处理原始输入$X$（或滑动窗口注意力输出），然后通过非线性门控$\\otimes$与短期注意力输出$y$融合，生成最终输出$o$：\n",
    "    $$o = y \\otimes M(X)$$\n",
    "    此设计中，滑动窗口注意力作为精确的短期记忆，而神经记忆模块则作为模型的渐变记忆。\n",
    "\n",
    "**4.3 记忆即层（Memory as a Layer, MAL）**\n",
    "这种变体将神经记忆模块作为一个独立的层放置在注意力模块之前。\n",
    "1.  **输入拼接**：与MAG类似，输入变为$\\tilde{X} = [P \\parallel X]$。\n",
    "2.  **记忆处理**：神经长时记忆模块$M$首先处理整个拼接后的输入序列，生成中间表示$y$：\n",
    "    $$y = M(\\tilde{X})$$\n",
    "3.  **注意力处理**：然后，滑动窗口注意力在$y$上进行操作，生成最终输出$o$：\n",
    "    $$o = \\text{SWA}(y)$$\n",
    "    这种设计中，记忆模块首先对过去和当前上下文进行压缩，然后注意力模块再在此压缩表示上进行操作。然而，这种串联结构可能限制了模型利用注意力与记忆模块互补处理数据的能力。\n",
    "\n",
    "#### Python代码示例：Titans架构变体的概念骨架\n",
    "\n",
    "这些示例将展示如何将LMM和Persistent Memory集成到三种Titans架构的**概念性前向传播**中。请注意，这不包括完整的训练循环和复杂的内部逻辑，仅为展示其结构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924efc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Titans MAC Example ---\n",
      "Titans MAC output shape: torch.Size([2, 99, 64])\n",
      "\n",
      "--- Titans MAG Example ---\n",
      "Titans MAG output shape: torch.Size([2, 100, 64])\n",
      "\n",
      "--- Titans MAL Example ---\n",
      "Titans MAL output shape: torch.Size([2, 100, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume ScaledDotProductAttention, MultiHeadAttention, PositionalEncoding, NeuralMemoryModule, PersistentMemory are defined as above\n",
    "\n",
    "class DummyAttentionLayer(nn.Module):\n",
    "    # A simplified attention-like layer for architectural illustration\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        x = self.norm(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "class TitansMAC(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_persistent_tokens, chunk_size, memory_depth):\n",
    "        super().__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.persistent_memory = PersistentMemory(d_model, num_persistent_tokens)\n",
    "        self.neural_memory = NeuralMemoryModule(d_model, memory_depth) # This M_t learns at test time\n",
    "        self.attention_layer = DummyAttentionLayer(d_model, num_heads)\n",
    "        self.query_projection = nn.Linear(d_model, d_model) # For S^(t)W_O\n",
    "\n",
    "        # Placeholder for initial memory state (parameters of neural_memory)\n",
    "        # In a real model, this would be handled by the NeuralMemoryModule's internal state.\n",
    "        # We need a way to pass the memory state or have the NeuralMemoryModule manage it across calls.\n",
    "        # For this conceptual model, let's assume neural_memory retains its internal state.\n",
    "        \n",
    "    def forward(self, input_sequence, is_training_memory=False):\n",
    "        # input_sequence: (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, d_model = input_sequence.shape\n",
    "\n",
    "        persistent_tokens = self.persistent_memory() # (num_persistent_tokens, d_model)\n",
    "        # Expand persistent tokens to batch size: (batch_size, num_persistent_tokens, d_model)\n",
    "        persistent_tokens_batch = persistent_tokens.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        outputs = []\n",
    "        # In a real setup, `self.neural_memory` would manage its M_t state across chunks\n",
    "        # `neural_memory`'s forward pass would handle update if `is_training_memory` is True\n",
    "        \n",
    "        # Simulating chunking\n",
    "        for i in range(0, seq_len, self.chunk_size):\n",
    "            s_t = input_sequence[:, i : i + self.chunk_size, :] # Current chunk (S^(t))\n",
    "            \n",
    "            if s_t.size(1) == 0: # Handle cases where chunk might be empty due to seq_len not divisible\n",
    "                continue\n",
    "\n",
    "            # Retrieve from long-term memory (conceptual Y_t = M*(q_t) from paper)\n",
    "            # For MAC, query is chunk S_t (or its projection)\n",
    "            q_t_chunk = self.query_projection(s_t) # (batch_size, chunk_size, d_model)\n",
    "            \n",
    "            # The paper says h_t = M_t-1(q_t). Here, we pass the average q_t for LMM to process\n",
    "            # and it will retrieve/update based on its internal state.\n",
    "            # Simplified: M_t-1* is neural_memory(mean_of_q_t_chunk)\n",
    "            # The actual h_t for concatenation would be derived from this.\n",
    "            \n",
    "            # Let's assume neural_memory's forward pass directly gives the historical context\n",
    "            # that's then expanded for the chunk.\n",
    "            # This is a simplification; paper implies M_t-1(k_t) (associative memory) and M*(q_t) (retrieval)\n",
    "            # are tied, and h_t is a retrieved vector for the whole chunk.\n",
    "            \n",
    "            # For simplicity, let's just make the entire chunk update the memory, and\n",
    "            # the retrieved_output from memory is what we concatenate.\n",
    "            # The `is_training_memory` flag will ensure M_t updates.\n",
    "            retrieved_historical_context = self.neural_memory(s_t.mean(dim=1), is_training_memory=is_training_memory)\n",
    "            # Retrieved context is (batch_size, d_model). Need to expand for chunk_size\n",
    "            retrieved_historical_context_expanded = retrieved_historical_context.unsqueeze(1).expand(-1, s_t.size(1), -1)\n",
    "\n",
    "\n",
    "            # Construct input for attention\n",
    "            attn_input = torch.cat([\n",
    "                persistent_tokens_batch[:, :s_t.size(1)], # Take enough persistent tokens\n",
    "                retrieved_historical_context_expanded,\n",
    "                s_t\n",
    "            ], dim=1)\n",
    "\n",
    "            # Apply attention (full causal attention within the window)\n",
    "            # Mask needs to be adjusted based on the concatenated input length\n",
    "            # For simplicity, we omit mask calculation here.\n",
    "            chunk_output = self.attention_layer(attn_input)\n",
    "            \n",
    "            # The output for this chunk. In real MAC, it would be the part corresponding to s_t\n",
    "            # from chunk_output. We need to extract that part.\n",
    "            start_idx = persistent_tokens_batch.size(1) + retrieved_historical_context_expanded.size(1)\n",
    "            outputs.append(chunk_output[:, start_idx : start_idx + s_t.size(1), :])\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "class TitansMAG(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_persistent_tokens, memory_depth):\n",
    "        super().__init__()\n",
    "        self.persistent_memory = PersistentMemory(d_model, num_persistent_tokens)\n",
    "        self.neural_memory = NeuralMemoryModule(d_model, memory_depth)\n",
    "        self.sliding_window_attention = DummyAttentionLayer(d_model, num_heads) # Conceptual SWA\n",
    "\n",
    "        # Gating mechanism components\n",
    "        self.gate_linear_y = nn.Linear(d_model, d_model)\n",
    "        self.gate_linear_m = nn.Linear(d_model, d_model)\n",
    "        self.gate_sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_sequence, is_training_memory=False):\n",
    "        # input_sequence: (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, d_model = input_sequence.shape\n",
    "\n",
    "        persistent_tokens = self.persistent_memory()\n",
    "        persistent_tokens_batch = persistent_tokens.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        # 1. Input concatenation\n",
    "        x_new = torch.cat([persistent_tokens_batch, input_sequence], dim=1)\n",
    "\n",
    "        # 2. Sliding Window Attention (y = SWA(x_new))\n",
    "        y = self.sliding_window_attention(x_new)\n",
    "        # Extract the part corresponding to original sequence\n",
    "        y_original_seq = y[:, persistent_tokens_batch.size(1):, :]\n",
    "\n",
    "        # 3. Neural Memory (M(x)) - Here, M processes the original sequence for its update\n",
    "        # For MAG, neural memory updates directly from input_sequence\n",
    "        m_x = self.neural_memory(input_sequence.mean(dim=1), is_training_memory=is_training_memory)\n",
    "        m_x_expanded = m_x.unsqueeze(1).expand(-1, seq_len, -1) # Expand to sequence length for gating\n",
    "\n",
    "        # 4. Gating (o = y ⊗ M(x))\n",
    "        # Simplified non-linear gating: gate = sigmoid(W_y y + W_m M(x)) * y\n",
    "        # Or element-wise product if M(x) is already of seq_len shape.\n",
    "        \n",
    "        # Let's use the paper's description: o = y * gate(y, M(x))\n",
    "        # Where gate is a non-linear combination\n",
    "        gate_coeff = self.gate_sigmoid(self.gate_linear_y(y_original_seq) + self.gate_linear_m(m_x_expanded))\n",
    "        output = y_original_seq * gate_coeff # Element-wise product\n",
    "\n",
    "        return output\n",
    "\n",
    "class TitansMAL(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_persistent_tokens, memory_depth):\n",
    "        super().__init__()\n",
    "        self.persistent_memory = PersistentMemory(d_model, num_persistent_tokens)\n",
    "        self.neural_memory_layer = NeuralMemoryModule(d_model, memory_depth) # This acts as a layer\n",
    "        self.sliding_window_attention = DummyAttentionLayer(d_model, num_heads) # Conceptual SWA\n",
    "\n",
    "    def forward(self, input_sequence, is_training_memory=False):\n",
    "        # input_sequence: (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len, d_model = input_sequence.shape\n",
    "\n",
    "        persistent_tokens = self.persistent_memory()\n",
    "        persistent_tokens_batch = persistent_tokens.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        # 1. Input concatenation\n",
    "        x_new = torch.cat([persistent_tokens_batch, input_sequence], dim=1)\n",
    "\n",
    "        # 2. Memory as a Layer (y = M(x))\n",
    "        # NeuralMemoryModule's forward pass gives us the processed output.\n",
    "        # Here we make it process the whole x_new sequence.\n",
    "        # This requires NeuralMemoryModule to output a sequence, not just a single vector.\n",
    "        # For simplicity, we'll adapt NeuralMemoryModule to take and return a sequence\n",
    "        # or use its retrieval mechanism for each token.\n",
    "        \n",
    "        # Let's adjust NeuralMemoryModule to process a sequence element-wise\n",
    "        # (simpler than full sequence transformation in a simple example)\n",
    "        # If NeuralMemoryModule is to act as a layer on sequence, its output should match seq_len.\n",
    "        # For simplicity of this conceptual example, let's assume it operates on the mean\n",
    "        # and expands its output.\n",
    "        \n",
    "        # Re-adjusting NeuralMemoryModule to just be a sequential MLP for MAL layer behavior\n",
    "        class MALNeuralMemoryLayer(nn.Module):\n",
    "            def __init__(self, d_model, memory_depth):\n",
    "                super().__init__()\n",
    "                layers = []\n",
    "                in_dim = d_model\n",
    "                for _ in range(memory_depth):\n",
    "                    layers.append(nn.Linear(in_dim, d_model))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    in_dim = d_model\n",
    "                self.mlp = nn.Sequential(*layers)\n",
    "            def forward(self, x):\n",
    "                return self.mlp(x) # Simply apply MLP to each token independently or as a sequence transform\n",
    "\n",
    "        self.neural_memory_layer = MALNeuralMemoryLayer(d_model, self.neural_memory_layer.d_model // 2) # Arbitrary depth for example\n",
    "\n",
    "        y = self.neural_memory_layer(x_new) # Processed by memory layer\n",
    "\n",
    "        # 3. Sliding Window Attention (o = SWA(y))\n",
    "        output = self.sliding_window_attention(y)\n",
    "        # Extract the part corresponding to original sequence\n",
    "        output_original_seq = output[:, persistent_tokens_batch.size(1):, :]\n",
    "\n",
    "        return output_original_seq\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == '__main__':\n",
    "    d_model = 64\n",
    "    num_heads = 4\n",
    "    num_persistent_tokens = 5\n",
    "    chunk_size = 16\n",
    "    memory_depth = 2\n",
    "    seq_len = 100\n",
    "    batch_size = 2\n",
    "\n",
    "    input_sequence = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    print(\"\\n--- Titans MAC Example ---\")\n",
    "    titans_mac = TitansMAC(d_model, num_heads, num_persistent_tokens, chunk_size, memory_depth)\n",
    "    output_mac = titans_mac(input_sequence, is_training_memory=True) # is_training_memory=True for demo\n",
    "    print(f\"Titans MAC output shape: {output_mac.shape}\")\n",
    "\n",
    "    print(\"\\n--- Titans MAG Example ---\")\n",
    "    titans_mag = TitansMAG(d_model, num_heads, num_persistent_tokens, memory_depth)\n",
    "    output_mag = titans_mag(input_sequence, is_training_memory=True)\n",
    "    print(f\"Titans MAG output shape: {output_mag.shape}\")\n",
    "\n",
    "    print(\"\\n--- Titans MAL Example ---\")\n",
    "    titans_mal = TitansMAL(d_model, num_heads, num_persistent_tokens, memory_depth)\n",
    "    output_mal = titans_mal(input_sequence, is_training_memory=True)\n",
    "    print(f\"Titans MAL output shape: {output_mal.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100560b",
   "metadata": {},
   "source": [
    "## 5. Titans的优势与实验验证\n",
    "\n",
    "Titans架构在多项任务上展现出超越Transformer和现代线性循环模型的优越性能，尤其是在长上下文场景。\n",
    "\n",
    "*   **长上下文处理**：在语言建模、常识推理、Needle-in-a-Haystack (NIAH) 等任务中，Titans能够有效处理超过2M的上下文窗口，同时保持高精度。这得益于其神经长时记忆模块能够动态管理和学习长期依赖，而非固定截断或简单累加。\n",
    "*   **理论表达能力**：Titans在理论上比Transformer和大多数现代线性循环模型（如DeltaNet）更具表现力。论文中的**定理4.1**指出，Titans能够解决超出$TC^0$复杂度的状态跟踪任务，这表明它在处理复杂逻辑和长时依赖方面具有更强的能力。\n",
    "*   **深度非线性记忆**：LMM采用深度MLP作为记忆模块，相比于线性模型或固定大小的矩阵/向量记忆，能够学习更复杂、更抽象的历史表示。实验结果也证实了深度记忆模块的有效性。\n",
    "*   **高效遗忘机制**：通过数据依赖的遗忘门$\\alpha_t$和动量项，Titans能更有效地管理记忆容量，避免记忆溢出，从而在长序列中保持性能。这弥补了许多线性循环模型在这方面的不足。\n",
    "*   **架构集成优势**：MAC和MAG等Titans变体通过将记忆模块与注意力机制有机结合，实现了短期精确依赖和长期抽象记忆的互补，通常优于MAL这种简单串联的层式结构。\n",
    "*   **快速并行化训练与推理**：尽管记忆更新涉及在线学习，但Titans通过将核心计算重构为矩阵运算，实现了训练的并行化和推理的高效性。\n",
    "\n",
    "## 6. 与现有模型的连接和泛化\n",
    "\n",
    "Titans的神经长时记忆模块（LMM）可以视为对现有多种先进线性循环模型的泛化和改进：\n",
    "*   **泛化Gated DeltaNet**：LMM通过引入动量项、允许深度非线性记忆以及支持跨分块的非线性循环，扩展了Gated DeltaNet的能力。\n",
    "*   **泛化Longhorn**：LMM通过添加遗忘门，解决了Longhorn等模型可能存在的记忆溢出问题，实现了更好的记忆管理。\n",
    "*   **泛化TTT Layer**：LMM整合了遗忘机制和动量驱动的更新规则，并在实验中验证了深度记忆的优势，这些都是现有TTT Layer所缺乏的。\n",
    "\n",
    "最重要的是，Titans不仅仅是提出了一个新的记忆模块，它更侧重于**如何将这些强大的记忆模块有效地集成到整体深度学习架构中**，形成了MAC、MAG、MAL等创新架构，从而在实践中实现性能的显著提升。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
