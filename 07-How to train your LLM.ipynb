{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32796142",
   "metadata": {},
   "source": [
    "# How to train your LLM\n",
    "\n",
    "## 1. LLM训练的基本挑战：为什么传统方法不起作用？\n",
    "\n",
    "在深度学习中，标准的模型训练通常遵循一个迭代过程，其核心是前向传播（forward pass）和反向传播（backward pass）。一个典型的PyTorch训练循环如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5d549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 0.9285\n",
      "Epoch 2, Batch 1, Loss: 0.9160\n",
      "Epoch 3, Batch 1, Loss: 0.9104\n",
      "Epoch 4, Batch 1, Loss: 0.9056\n",
      "Epoch 5, Batch 1, Loss: 0.9013\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 假设的模型、数据和损失函数\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 1) # 10个输入特征，1个输出\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 模拟数据\n",
    "inputs = torch.randn(100, 10)\n",
    "targets = torch.randn(100, 1)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = SimpleModel()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # 1. 前向传播：计算模型输出\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 2. 计算损失\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # 3. 反向传播：清零梯度、计算梯度、更新参数\n",
    "        optimizer.zero_grad() # 清零上一批次的梯度\n",
    "        loss.backward()       # 计算当前批次的梯度\n",
    "        optimizer.step()      # 根据梯度更新模型参数\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ca679",
   "metadata": {},
   "source": [
    "这段代码对于小型模型是完全有效的。然而，对于大型语言模型（LLMs），这种直接的方法会立即遇到显存（VRAM）不足的问题。原因是LLMs的参数量、中间激活值以及优化器状态都极其庞大，单个GPU的显存往往无法容纳。例如，一个拥有800亿参数的模型，其参数本身就占据了巨大的空间。\n",
    "\n",
    "### 1.1 LLM训练中的显存需求构成\n",
    "\n",
    "为了理解为何LLMs如此“大”，我们需要分析训练过程中显存的主要组成部分：\n",
    "\n",
    "1.  **模型参数（Model Parameters）：** 这是模型本身的权重和偏置。\n",
    "    *   **精度影响：** 传统的浮点数（FP32）精度下，每个参数需要4字节。对于一个80亿（8B）参数的模型，仅参数就需要 $8 \\text{B} \\times 4 \\text{ bytes/parameter} = 32 \\text{ GB}$。\n",
    "    *   **混合精度训练：** 为了节省显存和加速计算，通常采用混合精度训练，将参数存储为半精度浮点数（FP16或BF16），每个参数2字节。这样，一个8B模型在FP16下只需要 $8 \\text{B} \\times 2 \\text{ bytes/parameter} = 16 \\text{ GB}$。\n",
    "    *   **注意：** 尽管FP16存储了参数，但通常会保留一份FP32的副本用于参数更新，以保持数值稳定性，这会增加额外的显存开销。\n",
    "\n",
    "2.  **梯度（Gradients）：** 在反向传播过程中，需要计算每个参数的梯度，其大小与模型参数相同。\n",
    "    *   如果参数是FP16，梯度通常也以FP16存储，因此一个8B模型需要 $16 \\text{ GB}$的梯度显存。\n",
    "\n",
    "3.  **优化器状态（Optimizer States）：** 优化器（如Adam、AdamW）需要为每个模型参数维护内部状态。\n",
    "    *   以Adam优化器为例，它为每个参数维护两个状态：动量（momentum）和方差（variance）。\n",
    "    *   如果模型参数为FP16，但优化器状态通常以FP32存储以确保数值精度和收敛性。因此，每个参数需要 $2 \\text{ states/parameter} \\times 4 \\text{ bytes/state} = 8 \\text{ bytes/parameter}$。\n",
    "    *   对于一个8B模型，Adam优化器状态需要 $8 \\text{B} \\times 8 \\text{ bytes/parameter} = 64 \\text{ GB}$。\n",
    "\n",
    "4.  **激活值（Activations）：** 在前向传播过程中，神经网络每一层的输出（即激活值）都需要被存储，以便在反向传播时计算梯度。\n",
    "    *   激活值的内存占用取决于模型架构、序列长度（context length）和批次大小（batch size）。\n",
    "    *   对于长序列（例如32K tokens）和大型批次，激活值可能成为最大的显存消耗者，甚至可以达到数百GB到数TB的级别。例如，在一个Transformer模型中，自注意力机制的激活值会随着序列长度的平方增长（$O(N^2)$），这是其主要的内存瓶颈之一。\n",
    "\n",
    "**综合显存需求示例（以一个8B模型为例，FP16参数，FP32 Adam优化器）：**\n",
    "*   模型参数（FP16）：$16 \\text{ GB}$\n",
    "*   梯度（FP16）：$16 \\text{ GB}$\n",
    "*   优化器状态（FP32）：$64 \\text{ GB}$\n",
    "*   激活值：假设在一个特定配置下为 $32 \\text{ GB}$（这个值因具体任务和配置而异）。\n",
    "\n",
    "**总计：** $16 \\text{ GB} (\\text{parameters}) + 16 \\text{ GB} (\\text{gradients}) + 64 \\text{ GB} (\\text{optimizer states}) + 32 \\text{ GB} (\\text{activations}) = 128 \\text{ GB}$。\n",
    "\n",
    "而目前市面上主流的消费级或专业级GPU（如NVIDIA T4: 15GB, RTX 4090: 24GB, A100: 40GB, H100: 80GB）的单卡显存远低于128GB。这明确表明，单个GPU无法训练像8B这样的LLM，更不用说更大的模型了。\n",
    "\n",
    "### 1.2 批次大小与梯度累积\n",
    "\n",
    "为了获得稳定和高质量的梯度更新，LLM训练通常需要非常大的全局批次大小（Global Batch Size）。然而，受限于单卡显存，我们不可能直接使用如此大的批次。解决方案是**梯度累积（Gradient Accumulation）**。\n",
    "\n",
    "**梯度累积原理：**\n",
    "梯度累积允许我们模拟一个更大的批次大小，而不必一次性将所有数据加载到显存中。它通过将多个“迷你批次（mini-batch）”的梯度进行累加，然后才执行一次参数更新。\n",
    "\n",
    "$ \\text{全局批次大小 (Global Batch Size)} = \\text{迷你批次大小 (Mini Batch Size)} \\times \\text{累积步数 (Accumulation Steps)} $\n",
    "\n",
    "例如，如果我们的迷你批次大小是16，并且我们希望模拟一个1920的全局批次大小，那么累积步数将是 $1920 / 16 = 120$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf525ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective Global Batch Size: 1920\n",
      "Epoch 1, Global Step 1, Loss: 0.9545\n",
      "Epoch 2, Global Step 1, Loss: 1.7919\n",
      "Epoch 3, Global Step 1, Loss: 2.0058\n",
      "Epoch 4, Global Step 1, Loss: 1.6336\n",
      "Epoch 5, Global Step 1, Loss: 0.8568\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 假设的模型、数据和损失函数 (与上例相同)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 模拟数据\n",
    "inputs = torch.randn(1920, 10) # 假设总数据量\n",
    "targets = torch.randn(1920, 1)\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "\n",
    "# 设置迷你批次大小和梯度累积步数\n",
    "mini_batch_size = 16\n",
    "gradient_accumulation_steps = 120\n",
    "global_batch_size = mini_batch_size * gradient_accumulation_steps\n",
    "print(f\"Effective Global Batch Size: {global_batch_size}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=mini_batch_size, shuffle=True) # 通常会 shuffle\n",
    "\n",
    "model = SimpleModel()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # 1. 前向传播：计算模型输出\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 2. 计算损失 (通常会除以累积步数，以确保损失的平均值与真实大批次一致)\n",
    "        loss = loss_fn(outputs, targets) / gradient_accumulation_steps\n",
    "\n",
    "        # 3. 反向传播：只计算梯度，不清零，不更新参数\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. 每累积N个迷你批次后，清零梯度并更新参数\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()      # 更新参数\n",
    "            optimizer.zero_grad() # 清零梯度\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Global Step {batch_idx // gradient_accumulation_steps + 1}, Loss: {loss.item() * gradient_accumulation_steps:.4f}\") # 乘以累积步数恢复原始损失\n",
    "\n",
    "    # 在一个epoch结束后，如果还有未更新的梯度，也进行更新和清零\n",
    "    if (batch_idx + 1) % gradient_accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1}, Final Global Step (partial), Loss: {loss.item() * gradient_accumulation_steps:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6382c5c",
   "metadata": {},
   "source": [
    "**总结主要挑战：**\n",
    "\n",
    "*   **参数、梯度和优化器状态**占用大量显存。\n",
    "*   **激活值**随着序列长度和批次大小呈指数级增长，成为主要的显存瓶颈。\n",
    "*   **计算效率**：由于数据量巨大，需要高效的计算内核和并行策略。\n",
    "\n",
    "为了应对这些挑战，业界开发了多种优化技术和框架，主要分为以下三类：\n",
    "\n",
    "*   **优化参数、梯度和优化器状态的显存占用：** 通过数据并行、模型并行等策略，特别是使用DeepSpeed ZeRO等技术。\n",
    "*   **优化激活值的显存占用：** 主要通过梯度检查点（Gradient Checkpointing）、高效的注意力机制（如Flash Attention）和定制化内核（如Liger Kernel）。\n",
    "*   **减小模型大小和推理显存需求：** 通过模型量化（Quantization）。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 第一部分：优化参数、梯度和优化器状态\n",
    "\n",
    "当LLM模型参数、梯度和优化器状态的总和超过单个GPU的显存容量时，我们需要利用多GPU甚至多节点并行训练。\n",
    "\n",
    "### 2.1 如何利用多GPU？\n",
    "\n",
    "多GPU训练的主要挑战在于如何有效地分配和同步数据。\n",
    "*   模型过大，无法在单个GPU上容纳。\n",
    "*   大批次训练需要巨大的计算资源和内存。\n",
    "*   长输入序列导致自注意力机制的内存占用呈$O(N^2)$增长。\n",
    "\n",
    "为了解决这些问题，我们通常采用并行训练策略。在最简单的**数据并行（Data Parallelism）**中，每个GPU都拥有模型的完整副本，并处理不同批次的数据。然而，这并不能解决模型本身过大的问题，因为每个GPU仍然需要加载整个模型。\n",
    "\n",
    "### 2.2 DeepSpeed - 零冗余优化器（ZeRO）\n",
    "\n",
    "DeepSpeed的ZeRO（Zero Redundancy Optimizer）是一种革命性的数据并行优化技术，它通过消除训练过程中的内存冗余来解决LLM的显存瓶颈。ZeRO将优化器状态、梯度和模型参数在不同的GPU之间进行分区，从而使得每个GPU只需要存储一小部分。\n",
    "\n",
    "#### ZeRO-1：优化器状态分区\n",
    "\n",
    "*   **原理：** 每个GPU只存储优化器状态的一部分。在参数更新时，所有GPU需要将各自的优化器状态进行“全收集（all-gather）”，然后本地完成参数更新，再丢弃不需要的部分。\n",
    "*   **显存节省：** 将优化器状态的内存需求从 $O(P)$ 降低到 $O(P/N)$，其中 $P$ 是模型参数总量， $N$ 是GPU数量。\n",
    "\n",
    "#### ZeRO-2：优化器状态 + 梯度分区\n",
    "\n",
    "*   **原理：** 除了优化器状态，梯度也被分区存储。在反向传播过程中，当计算出特定层的梯度时，它们会立即被“规约分散（reduce-scatter）”到负责更新相应参数的GPU上。这意味着每个GPU只收集和存储它所需参数的梯度子集。\n",
    "*   **显存节省：** 将梯度内存需求从 $O(P)$ 降低到 $O(P/N)$。\n",
    "\n",
    "#### ZeRO-3：优化器状态 + 梯度 + 参数分区\n",
    "\n",
    "*   **原理：** 这是ZeRO最激进的阶段，将模型参数本身也进行了分区。在模型前向和反向传播过程中，每个GPU只加载当前层所需的参数子集。一旦该层计算完成，这些参数就会被卸载，从而释放显存。这涉及到频繁的“全收集”操作来按需获取参数。\n",
    "*   **显存节省：** 将模型参数的内存需求从 $O(P)$ 降低到 $O(P/N)$。\n",
    "\n",
    "**ZeRO各阶段的内存节省效果（以8B模型为例）：**\n",
    "| 组件          | 传统数据并行 (DP=8) | ZeRO-1 | ZeRO-2 | ZeRO-3 |\n",
    "| :------------ | :------------------ | :----- | :----- | :----- |\n",
    "| 模型参数      | P                   | P      | P      | P/N    |\n",
    "| 梯度          | P                   | P      | P/N    | P/N    |\n",
    "| 优化器状态    | 2P                  | 2P/N   | 2P/N   | 2P/N   |\n",
    "| **总计**      | **4P**              | **3P+2P/N** | **P+3P/N** | **3P/N** |\n",
    "| 8B模型总计*   | $128 \\text{ GB}$     | $96 \\text{ GB}$ | $64 \\text{ GB}$ | $48 \\text{ GB}$ |\n",
    "\n",
    "*注：这里P指FP16的参数大小，总计计算忽略了激活值，仅考虑参数、梯度和优化器状态的组合。实际总计还会加上激活值。表中数字仅为ZeRO减少了哪部分存储的相对大小。\n",
    "\n",
    "**DeepSpeed ZeRO的优势：**\n",
    "*   **显著的显存节省：** 使得在更少的GPU上训练更大的模型成为可能。\n",
    "*   **良好的可扩展性：** 能够有效扩展到数百甚至数千个GPU。\n",
    "*   **与数据并行兼容：** ZeRO是在数据并行基础上进行内存优化的，因此易于与现有数据并行代码集成。\n",
    "\n",
    "**DeepSpeed ZeRO配置示例：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你已经安装了 deepspeed 和 transformers\n",
    "# !pip install deepspeed transformers accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import deepspeed\n",
    "\n",
    "# 1. 准备一个dummy模型和tokenizer\n",
    "# 为了演示，我们使用一个非常小的预训练模型\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # LLMs通常没有pad_token，用eos_token代替\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 2. 准备一个dummy数据集\n",
    "def get_dummy_dataset(num_samples=100, max_length=128):\n",
    "    texts = [\"This is a sample text for training LLM.\" * 5 for _ in range(num_samples)]\n",
    "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
    "    # 对于CausalLM，labels通常是input_ids\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n",
    "    return torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], encodings[\"labels\"])\n",
    "\n",
    "train_dataset = get_dummy_dataset()\n",
    "\n",
    "# 3. 创建DeepSpeed配置文件 (deepspeed_config.json)\n",
    "# 你需要创建一个JSON文件，例如 deepspeed_config.json\n",
    "# 内容如下：\n",
    "\"\"\"\n",
    "{\n",
    "  \"train_batch_size\": 16,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    },\n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0,\n",
    "  \"steps_per_print\": 10\n",
    "}\n",
    "\"\"\"\n",
    "# 这里的 \"stage\": 3 表示启用 ZeRO-3\n",
    "# \"offload_optimizer\" 和 \"offload_param\" 可以将优化器状态和参数卸载到CPU内存，进一步节省GPU显存，但会增加CPU-GPU通信开销。\n",
    "\n",
    "# 4. 使用Hugging Face Trainer结合DeepSpeed\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4, # 这里的batch size会与deepspeed_config.json中的train_batch_size交互\n",
    "    gradient_accumulation_steps=1, # 这里的累积步数会与deepspeed_config.json中的gradient_accumulation_steps交互\n",
    "    deepspeed=\"./deepspeed_config.json\", # 指向DeepSpeed配置文件\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\", # For demo, no saving\n",
    "    do_train=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 5. 启动训练 (需要通过 deepspeed 命令行工具启动)\n",
    "# 在终端中运行：\n",
    "# deepspeed --num_gpus=1 your_script_name.py\n",
    "# (your_script_name.py 是包含上述代码的文件名)\n",
    "# 注意：实际运行时，你需要确保你的环境中有一个或多个GPU，并且DeepSpeed配置与你的硬件兼容。\n",
    "# 对于此dummy模型，ZeRO-3可能不是必需的，但其配置方式是通用的。\n",
    "# trainer.train() # 这行代码在 deepspeed 命令行启动时会被执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08270b86",
   "metadata": {},
   "source": [
    "#### ZeRO Offload：显存与速度的权衡\n",
    "\n",
    "ZeRO Offload是一种将优化器状态甚至模型参数从GPU显存卸载到CPU内存（RAM）的技术。\n",
    "*   **优点：** 进一步减少GPU显存占用，使得在显存较小的GPU上训练更大的模型成为可能。\n",
    "*   **缺点：** CPU和GPU之间的数据传输速度远低于GPU内部传输（如NVLink）。频繁的数据传输会导致训练速度显著下降。\n",
    "\n",
    "下图清晰地展示了有无Offload对训练速度的影响：\n",
    "\n",
    "| 配置                                                 | CPU内存占用 | GPU显存占用 | 速度 (秒/步) |\n",
    "| :--------------------------------------------------- | :---------- | :---------- | :----------- |\n",
    "| 4 GPU: 仅Offload优化器状态到CPU（zero_init=1）       | 201.93GB    | 1.96GB      | 74s / 步     |\n",
    "| 8 GPU: 无Offload（zero_init=0）                      | 358.98GB    | 18.78GB     | 7.3s / 步    |\n",
    "\n",
    "从表中可以看出，当使用4个GPU并将优化器状态卸载到CPU时，每个GPU仅占用1.96GB显存，但每步训练需要74秒。而当使用8个GPU且不进行任何卸载时，每个GPU占用18.78GB显存，但每步训练仅需7.3秒。这表明Offload虽然能显著减少显存占用，但以牺牲训练速度为代价。因此，是否使用Offload取决于具体的硬件配置（CPU RAM容量和速度、GPU数量和显存）以及对训练速度的要求。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 第二部分：优化激活值\n",
    "\n",
    "激活值是训练LLM时的另一个主要显存消耗来源，尤其是当序列长度很长时。\n",
    "\n",
    "### 3.1 Kernel优化\n",
    "\n",
    "软件栈层次：\n",
    "*   **Naive PyTorch：** 使用Python编写的PyTorch操作，通常是高层抽象，易于使用但可能效率不高。\n",
    "*   **`torch.compile()`：** PyTorch 2.0引入的新特性，可以将PyTorch代码编译成优化的计算图，然后将其发送到不同的后端（如CUDA、CPU）执行。它通过图级别的优化来提高性能，减少Python开销，并能够融合操作。\n",
    "*   **Triton：** 一种专门用于编写高性能GPU内核的领域特定语言（DSL）。它比CUDA C++更易于编写，但提供了接近CUDA C++的性能。许多新兴的优化技术（如Flash Attention）都是用Triton实现的。\n",
    "*   **CUDA：** NVIDIA的并行计算平台和编程模型，允许开发者直接编写在GPU上运行的程序。它是最低层级的抽象，提供对GPU硬件的精细控制，但编写复杂且门槛高。\n",
    "\n",
    "层次越低，对硬件的控制越精细，理论上性能越高，但开发难度也越大。\n",
    "\n",
    "**`torch.compile()` 示例：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edae646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的模型\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(100, 200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "# 创建模型实例\n",
    "model = MyModel()\n",
    "example_input = torch.randn(64, 100) # 批次大小64，特征100\n",
    "\n",
    "# 未编译的模型\n",
    "# output_uncompiled = model(example_input)\n",
    "\n",
    "# 使用 torch.compile 编译模型\n",
    "# mode='reduce-overhead' 减少Python解释器开销\n",
    "# mode='max-autotune' 会进行大量的编译尝试和基准测试以找到最快的配置，可能需要较长时间\n",
    "compiled_model = torch.compile(model, mode=\"default\")\n",
    "\n",
    "# 第一次运行时会有编译开销\n",
    "output_compiled = compiled_model(example_input)\n",
    "\n",
    "# 后续调用会直接使用编译后的图，速度更快\n",
    "# output_compiled_again = compiled_model(example_input)\n",
    "\n",
    "print(\"Model compiled successfully with torch.compile!\")\n",
    "# 可以比较运行时间，但需要使用性能分析工具如 timeit 或 torch.utils.benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb788b1e",
   "metadata": {},
   "source": [
    "### 3.2 激活值再计算（梯度检查点）\n",
    "\n",
    "由于激活值可能占用大量显存，一种常见的优化技术是**梯度检查点（Gradient Checkpointing）**，也被称为激活值再计算（Activation Recomputation）。\n",
    "\n",
    "*   **原理：** 在前向传播过程中，不再存储所有层的激活值。只存储“重要”的激活值（例如，每隔几层的激活值）。在反向传播时，对于那些未存储激活值的层，其激活值会在需要时重新计算。\n",
    "*   **优点：** 显著减少激活值的显存占用，通常将 $O(N)$ 降低到 $O(\\sqrt{N})$ 或 $O(1)$，具体取决于实现。\n",
    "*   **缺点：** 增加了计算开销，因为部分前向计算在反向传播时需要重新执行。这是一个典型的计算时间与内存的权衡。\n",
    "\n",
    "PyTorch中可以通过`torch.utils.checkpoint`模块实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d004b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# 假设一个深度网络，包含多个Sequential块\n",
    "class DeepSequentialModel(nn.Module):\n",
    "    def __init__(self, num_blocks=10):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128)\n",
    "            ) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            # 使用 checkpoint 包裹每个块\n",
    "            # 这意味着在反向传播时，如果需要这个块的激活值，它会重新计算前向\n",
    "            x = checkpoint(block, x)\n",
    "        return x\n",
    "\n",
    "# 创建模型和输入\n",
    "model = DeepSequentialModel(num_blocks=20).cuda()\n",
    "input_data = torch.randn(4, 128, requires_grad=True).cuda() # 假设输入数据\n",
    "\n",
    "# 比较有无梯度检查点的内存使用\n",
    "# 1. 没有梯度检查点\n",
    "# with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "#     output = model(input_data)\n",
    "#     loss = output.sum()\n",
    "#     loss.backward()\n",
    "# print(\"Without checkpointing memory usage:\")\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "# 2. 使用梯度检查点\n",
    "# 确保在实际使用时，模型或块被checkpoint包装\n",
    "# 运行这个代码块时，torch.utils.checkpoint会自动处理激活值的存储和再计算。\n",
    "# 由于prof工具显示的是峰值，可能需要更复杂的方法来精确测量节省的内存。\n",
    "# 但其原理是减少了中间激活值的存储。\n",
    "# 通常在训练大模型时，会直接配置 Hugging Face Transformers 或 DeepSpeed 等框架来自动启用。\n",
    "\n",
    "print(\"Model initialized with checkpointing capability.\")\n",
    "print(\"The actual memory saving is handled internally by torch.utils.checkpoint.\")\n",
    "# For practical measurement, you'd monitor peak GPU memory using tools like nvidia-smi\n",
    "# or torch.cuda.max_memory_allocated() before and after a forward/backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee1c01a",
   "metadata": {},
   "source": [
    "### 3.3 Flash Attention算法\n",
    "\n",
    "Flash Attention 是一种创新的注意力机制实现，它通过优化GPU内存访问模式来显著提高训练速度并降低显存消耗。\n",
    "\n",
    "*   **传统注意力机制的问题：**\n",
    "    *   **$O(N^2)$ 显存占用：** 计算注意力分数矩阵 $Q K^T$ 及其Softmax输出时，会产生一个大小为 $N \\times N$（其中$N$是序列长度）的中间矩阵，这部分显存开销非常大。\n",
    "    *   **HBM（高带宽内存）频繁读写：** 注意力机制涉及多个矩阵乘法、Softmax和Dropout操作。传统实现会将这些操作分解为独立的内核，导致频繁地将数据从高带宽内存（HBM，即GPU显存）读入到更快的片上SRAM/缓存，然后再写回HBM。HBM读写是GPU性能的瓶颈。\n",
    "\n",
    "*   **Flash Attention的解决方案：**\n",
    "    *   **Tiling（分块）：** 将输入查询（Q）、键（K）和值（V）矩阵分成小块（tile）。\n",
    "    *   **融合操作（Kernel Fusion）：** 将注意力机制的多个步骤（QKV矩阵乘法、Softmax、Dropout、输出矩阵乘法）融合到一个单独的GPU内核中。\n",
    "    *   **SRAM利用：** 在计算每个注意力块时，尽可能将数据保留在GPU芯片上更快的SRAM中，减少对慢速HBM的访问。计算完成后，只将最终结果写回HBM。\n",
    "    *   **在线Softmax：** 在Softmax计算过程中，不存储完整的注意力矩阵，而是实时计算并更新。\n",
    "    *   **重计算（Recomputation）：** 在反向传播时，如果需要中间激活值，Flash Attention会在SRAM中重新计算而不是存储它们。\n",
    "\n",
    "*   **显存效率：** Flash Attention将显存复杂度从$O(N^2)$降低到**$O(N)$**（接近线性）。\n",
    "*   **计算速度：** 显著加快注意力计算，尤其是在长序列上，因为它大幅减少了HBM的读写量。\n",
    "\n",
    "**Flash Attention速度对比：**\n",
    "\n",
    "| 算法             | H100 GPU (80GB SXM5), Head Dim 64, 不同序列长度下的前向传播速度 (TFLOPS/s) |\n",
    "| :--------------- | :-------------------------------------------------------------------------- |\n",
    "| Standard Attention | 512: 52 | 1k: 63 | 2k: 72 | 4k: 73 | 8k: 67 | 16k: 63 |\n",
    "| FlashAttention-2 | 512: 282 | 1k: 308 | 2k: 318 | 4k: 321 | 8k: 322 | 16k: 324 |\n",
    "\n",
    "可以看出，Flash Attention-2在所有序列长度下都比标准注意力快数倍，尤其是在长序列上，优势更为明显。\n",
    "\n",
    "**Flash Attention集成示例：**\n",
    "\n",
    "许多现代LLM框架（如Hugging Face Transformers）已经集成了Flash Attention。通常，你只需要安装`flash_attn`库并在加载模型时指定使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1234014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate flash_attn\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\" # 示例：一个支持Flash Attention的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 加载模型时启用Flash Attention (如果模型支持且环境配置正确)\n",
    "# bitsandbytes 通常用于量化加载，这里只是作为一个常见选项提及\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16, # 推荐使用 bfloat16 进行训练和推理\n",
    "    device_map=\"auto\",          # 自动分配到可用GPU\n",
    "    attn_implementation=\"flash_attention_2\" # 启用 Flash Attention 2\n",
    ")\n",
    "model.eval() # 通常在推理时使用\n",
    "\n",
    "# 示例输入\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 生成文本\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"Model loaded with Flash Attention 2 implementation (if supported by architecture and installed).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf43e56",
   "metadata": {},
   "source": [
    "### 3.4 Liger Kernel\n",
    "\n",
    "Liger Kernel 是一个通过重新实现LLM中的核心计算（特别是使用优化的Triton代码）来提高性能和降低显存占用的项目。它的目标是为Transformer模型的关键操作（如注意力、层归一化、激活函数等）提供高度优化的GPU内核。\n",
    "\n",
    "*   **原理：** 类似于Flash Attention，Liger Kernel也关注于通过定制化的低层级内核来减少内存带宽瓶颈和提高计算效率。它通过Triton语言编写，使得开发高性能内核更加便捷。\n",
    "*   **效益：**\n",
    "    *   **吞吐量（Throughput）：** 显著提高每秒处理的tokens数量。\n",
    "    *   **显存占用（Peak Reserved Memory）：** 降低峰值显存占用，从而支持更大的批次大小或序列长度。\n",
    "\n",
    "**Liger Kernel 使用示例：**\n",
    "\n",
    "Liger Kernel通常以库的形式提供，可以通过替换Hugging Face Transformers中的标准模型类来使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers liger_kernel # 假设 liger_kernel 可用\n",
    "\n",
    "import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer # 标准Transformers模型\n",
    "from liger_kernel.transformers import AutoLigerKernelForCausalLM, AutoTokenizer # Liger Kernel 优化后的模型\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\" # 示例模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 使用 Liger Kernel 优化后的模型类加载模型\n",
    "# 注意：你需要确保你的模型和环境与 Liger Kernel 兼容\n",
    "model = AutoLigerKernelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval() # 通常在推理时使用\n",
    "\n",
    "# 示例输入\n",
    "inputs = tokenizer(\"Hello, how are you today?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 生成文本\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"Model loaded with Liger Kernel (if supported by the library and architecture).\")\n",
    "# Liger Kernel 旨在透明地提供性能优化，使用方式与标准Transformers模型类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38e528",
   "metadata": {},
   "source": [
    "## 4. 第三部分：模型量化\n",
    "\n",
    "模型量化（Quantization）是一种有损压缩技术，它通过降低模型权重和/或激活值的数值精度来减少模型的大小和显存占用。这在推理阶段尤为重要，因为它能让大型模型部署到资源受限的设备上，也可以在训练或微调时减少显存需求。\n",
    "\n",
    "### 4.1 量化原理\n",
    "\n",
    "*   **核心思想：** 将模型的浮点数（如FP32或FP16）参数转换为低精度整数（如INT8、INT4）甚至更低精度格式。\n",
    "*   **过程：**\n",
    "    1.  **量化（Quantization）：** 将高精度浮点数映射到低精度整数。这通常涉及缩放（scaling）和偏移（zero-point）操作。\n",
    "    2.  **低精度存储/计算：** 模型以低精度格式存储和执行计算。\n",
    "    3.  **反量化（Dequantization）：** 在某些情况下，为了进行高精度计算（如在参数更新时），低精度数据会临时转换回高精度浮点数。\n",
    "*   **有损压缩：** 量化是有损的，意味着会损失模型的一些信息，可能导致性能下降。量化的挑战在于在显著减少模型大小的同时，最大限度地保留模型精度。\n",
    "\n",
    "### 4.2 量化类型\n",
    "\n",
    "*   **后训练量化（Post-Training Quantization, PTQ）：** 在模型训练完成后进行量化。这是最简单的方法，不需要重新训练，但可能对模型精度有负面影响。\n",
    "    *   **GPTQ：** 一种后训练量化算法，通过最小化量化误差来压缩模型。\n",
    "    *   **AWQ (Activation-aware Weight Quantization)：** 另一种PTQ方法，在量化权重时考虑激活值的分布，以减少量化误差。\n",
    "*   **量化感知训练（Quantization-Aware Training, QAT）：** 在训练过程中模拟量化效应。模型在训练时“感知”到量化引起的精度损失，并进行调整，从而在量化后保持更高的精度。这种方法更复杂，需要完整的训练过程。\n",
    "\n",
    "### 4.3 量化工具和格式\n",
    "\n",
    "*   **GGML家族（如Llama.cpp）：** 一系列用于CPU和GPU推理的C/C++库，支持多种低精度格式（如Q4_0, Q5_0, Q8_0等），并针对CPU进行了高度优化。它使得在普通硬件上运行大型LLM成为可能。\n",
    "*   **BitsAndBytes：** 一个PyTorch库，提供了4位和8位量化功能，并可以与Hugging Face Transformers无缝集成，在训练和推理时使用低精度权重。\n",
    "*   **FP8（8位浮点数）：** 一种新兴的低精度格式，在NVIDIA Hopper架构（如H100）上得到硬件支持，为深度学习提供了高性能和低内存的优势。\n",
    "\n",
    "**量化效果示例：**\n",
    "\n",
    "以Meta-Llama-3.1-8B-Instruct-Q8_0.gguf模型为例：\n",
    "*   原始8B模型（FP16）大小： $8 \\text{B} \\times 2 \\text{ bytes} = 16 \\text{ GB}$\n",
    "*   Q8_0 量化模型大小：$8 \\text{B} \\times 1 \\text{ byte} = 8 \\text{ GB}$（每个参数量化到8位整数）\n",
    "\n",
    "这使得一个8B模型可以轻松地加载到15GB显存的T4 GPU上进行推理。\n",
    "\n",
    "**BitsAndBytes 量化加载模型示例：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\" # 示例模型，通常需要huggingface token权限\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 定义量化配置\n",
    "# load_in_4bit=True 启用4位量化\n",
    "# bnb_4bit_quant_type=\"nf4\" 使用 NF4 (NormalFloat4) 量化，这是推荐的4位量化类型\n",
    "# bnb_4bit_compute_dtype=torch.bfloat16 设置4位量化计算时的数据类型，通常是 bfloat16\n",
    "# bnb_4bit_use_double_quant=True 启用双重量化，进一步节省内存\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 加载量化模型\n",
    "# device_map=\"auto\" 将模型的不同部分自动分配到可用设备（CPU/GPU）\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(f\"Model loaded in 4-bit mode. Type of first layer's weight: {type(model.model.layers[0].self_attn.q_proj.weight)}\")\n",
    "    model.eval() # 切换到评估模式\n",
    "\n",
    "    # 示例推理 (与非量化模型相同)\n",
    "    inputs = tokenizer(\"Tell me a story about a brave knight.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "    # with torch.no_grad():\n",
    "    #     outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    # print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model. Ensure you have access to '{model_name}' and enough GPU VRAM (even for 4-bit, it needs some initial load space). Error: {e}\")\n",
    "\n",
    "print(\"Model quantization demonstration complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
