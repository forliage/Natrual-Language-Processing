{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a037ca",
   "metadata": {},
   "source": [
    "# Model Compression and Data-Efficient Fine-Tuning\n",
    "\n",
    "在深度学习领域，特别是自然语言处理（NLP）中，模型的大小和计算需求正以前所未有的速度增长。这种增长带来了巨大的训练和推理成本，使得在实际应用中部署这些大型模型变得极具挑战性。为了应对这些挑战，研究人员开发了多种模型压缩和数据高效微调技术，旨在减少模型的资源消耗，同时保持甚至提升性能。\n",
    "\n",
    "### 模型压缩\n",
    "\n",
    "模型压缩旨在减小模型的体积，降低其计算复杂度，从而使其在资源受限的环境中能够更高效地运行。主要的方法包括量化、剪枝和蒸馏。\n",
    "\n",
    "#### 1. 背景\n",
    "\n",
    "大型语言模型（LLMs）的出现，如OpenAI的ChatGPT，已经彻底改变了NLP的应用格局。ChatGPT拥有超过一亿的周活跃用户，表明了LLMs在实际部署中的巨大潜力。然而，伴随而来的是惊人的计算成本。\n",
    "\n",
    "例如，训练像Llama 2这样的模型需要大量的GPU时间和能源。根据估算，一个7B参数的Llama 2模型需要184,320个GPU小时，耗费70B参数的模型更是高达1,720,320个GPU小时。这些训练成本已经很高，但实际部署中的推理成本往往更高，甚至可能在每周的基础上超过训练成本。尽管模型变得越来越大（例如，GPT-3拥有175B参数，Megatron-Turing NLG更是高达530B参数），但如何以经济、高效和公平的方式部署这些NLP系统，同时又不牺牲性能，成为了一个核心问题。\n",
    "\n",
    "#### 2. 模型压缩的原理与方法\n",
    "\n",
    "**为什么模型压缩是可能的？**\n",
    "这部分问题的答案在于神经网络的**过参数化（over-parametrization）**现象。研究表明，过参数化的模型更容易优化。例如，Du和Lee（2018）在研究中指出，对于具有二次激活函数的浅层神经网络，当隐藏节点数$k$大于$\\sqrt{2n}$（其中$n$为训练数据点数）时，局部搜索算法能够找到全局最优解。这说明了即使参数数量远超样本量，模型依然可以表现良好，并且损失函数具有“良性”的几何结构，使得优化更容易进行，为模型压缩提供了理论基础。\n",
    "\n",
    "##### 2.1 量化（Quantization）\n",
    "\n",
    "量化是一种通过降低模型参数（权重和激活值）的数值精度来减小模型大小和计算量的方法。\n",
    "\n",
    "**后训练量化（Post-Training Quantization, PTQ）**\n",
    "PTQ是在模型训练完成后进行的量化。其主要思想是：先使用任意精度训练模型，然后将权重进行量化。例如，一个65B参数的模型，如果使用4字节（32位浮点数，FP32）表示，需要260GB的存储空间。将其量化为4位（4b）可以减少到32.5GB，2位（2b）减少到16.25GB，1位（1b）甚至可以减少到8.1GB。这大大降低了模型在内存中的占用。\n",
    "\n",
    "**浮点数与低精度浮点类型**\n",
    "浮点数通常遵循IEEE 754标准，表示为$(-1)^s \\cdot M \\cdot 2^E$，其中$s$是符号位，$M$是小数部分（尾数），$E$是指数部分。\n",
    "*   **float16 (fp16)**：1位符号位，5位指数位，10位小数位。相较于FP32（8位指数，23位小数），fp16牺牲了精度以节省存储。\n",
    "*   **bfloat16 (bf16)**：1位符号位，8位指数位，7位小数位。bf16与FP32有相同的指数范围，但在小数位上牺牲了更多精度。这使得bf16在保留动态范围方面优于fp16，对于大型模型训练尤其重要。\n",
    "\n",
    "**Int8 量化**\n",
    "Int8量化将浮点数权重转换为8位整数。一种常见的Int8量化方法是**绝对最大值（absmax）量化**。\n",
    "其公式为：\n",
    "$X_{i8} = \\text{round}\\left(\\frac{127 \\cdot X_{f16}}{\\max_{i,j}(|X_{f16,i,j}|)}\\right)$\n",
    "这个公式将浮点数值$X_{f16}$缩放到$[-127, 127]$的范围内。\n",
    "**示例：**\n",
    "给定浮点数列表 $[0.5, 20, -0.0001, -0.01, -0.1]$，其中最大绝对值为20。\n",
    "量化后的值为：\n",
    "$\\text{round}(127/20 \\cdot [0.5, 20, -0.0001, -0.01, -0.1])$\n",
    "$\\rightarrow \\text{round}([3.175, 127, -0.000635, -0.0635, -0.635])$\n",
    "$\\rightarrow [3, 127, 0, 0, -1]$\n",
    "\n",
    "**PyTorch 中的概念性 Int8 量化代码：**\n",
    "虽然PyTorch提供了高级的量化API（如`torch.quantization`），但为了理解核心机制，可以概念性地实现absmax量化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4bd73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([ 5.0000e-01,  2.0000e+01, -1.0000e-04, -1.0000e-02, -1.0000e-01])\n",
      "Quantized tensor (Int8): tensor([  3, 127,   0,   0,  -1], dtype=torch.int8)\n",
      "Dequantized tensor: tensor([ 0.4724, 20.0000,  0.0000,  0.0000, -0.1575])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def absmax_quantize(tensor, num_bits=8):\n",
    "    max_val = torch.max(torch.abs(tensor))\n",
    "    scale = (2**(num_bits - 1) - 1) / max_val\n",
    "    quantized_tensor = torch.round(tensor * scale)\n",
    "    return quantized_tensor.to(torch.int8)\n",
    "\n",
    "def absmax_dequantize(quantized_tensor, max_val, num_bits=8):\n",
    "    scale = (2**(num_bits - 1) - 1) / max_val\n",
    "    dequantized_tensor = quantized_tensor / scale\n",
    "    return dequantized_tensor.to(torch.float32)\n",
    "\n",
    "# 示例\n",
    "float_tensor = torch.tensor([0.5, 20.0, -0.0001, -0.01, -0.1], dtype=torch.float32)\n",
    "print(f\"Original tensor: {float_tensor}\")\n",
    "\n",
    "# 计算max_val用于量化和反量化\n",
    "max_abs_val = torch.max(torch.abs(float_tensor))\n",
    "\n",
    "# 量化\n",
    "quantized_tensor = absmax_quantize(float_tensor, num_bits=8)\n",
    "print(f\"Quantized tensor (Int8): {quantized_tensor}\")\n",
    "\n",
    "# 反量化\n",
    "dequantized_tensor = absmax_dequantize(quantized_tensor, max_abs_val, num_bits=8)\n",
    "print(f\"Dequantized tensor: {dequantized_tensor}\")\n",
    "\n",
    "# 实际PyTorch量化模块的使用更复杂，需要设置QConfig、准备模型等\n",
    "# import torch.quantization\n",
    "# model = MyModel() # 假设有一个模型\n",
    "# model.eval()\n",
    "# quantized_model = torch.quantization.quantize_dynamic(\n",
    "#     model, {torch.nn.Linear, torch.nn.LSTM}, dtype=torch.qint8\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cd77f",
   "metadata": {},
   "source": [
    "**极端示例：二值神经网络（Binarized Neural Networks, BNNs）**\n",
    "BNNs是量化的一种极端形式，其中权重和激活值被限制为只有-1或1（或0或1）两个值。这大大减少了内存占用和计算量（乘法变为符号操作）。\n",
    "尽管BNNs在MNIST等小型数据集上能达到与全精度模型相近的性能，但训练BNNs具有挑战性，因为在反向传播过程中，梯度需要进行特殊处理（通常使用Straight-Through Estimator, STE）。\n",
    "\n",
    "**概念性二值化权重：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105ddf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "tensor([[-2.4234, -1.0369, -1.2044, -2.0569, -0.6045],\n",
      "        [ 0.3308,  0.6959, -2.1401, -0.2031, -0.9295],\n",
      "        [-2.8993,  0.8299,  2.4200,  1.4792, -1.8319],\n",
      "        [-0.9400,  1.0778, -1.2508, -3.8205, -1.1609],\n",
      "        [-0.9304, -3.1237, -4.7748,  2.8670, -1.8177]])\n",
      "Binarized weights:\n",
      "tensor([[-1., -1., -1., -1., -1.],\n",
      "        [ 1.,  1., -1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  1., -1.],\n",
      "        [-1.,  1., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def binarize_weights(weights):\n",
    "    \"\"\"\n",
    "    概念性地将权重二值化为 -1 或 1。\n",
    "    在实际BNN训练中，需要使用STE处理梯度。\n",
    "    \"\"\"\n",
    "    binarized_w = torch.sign(weights)\n",
    "    # 确保没有0值，如果weights中有0，sign会返回0\n",
    "    binarized_w[binarized_w == 0] = 1 # 或者 -1，取决于实际设计\n",
    "    return binarized_w\n",
    "\n",
    "# 示例权重\n",
    "weights = torch.randn(5, 5) * 2 - 1 # 模拟一些浮点权重\n",
    "print(f\"Original weights:\\n{weights}\")\n",
    "\n",
    "binarized_weights = binarize_weights(weights)\n",
    "print(f\"Binarized weights:\\n{binarized_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265f3f6",
   "metadata": {},
   "source": [
    "**模型感知量化（Model-Aware Quantization）：GOBO**\n",
    "GOBO（Zadeh et al. 2020）观察到BERT模型中各层的权重倾向于遵循高斯分布。其核心思想是，对于权重分布主体部分（例如99.9%）进行8桶量化，而对于尾部（0.01%）的异常值则不进行量化，保留其高精度。这种方法确保了量化对模型性能影响最小。\n",
    "\n",
    "**LLM.int8()**\n",
    "LLM.int8（Dettmers et al. 2022）解决了Transformer LLMs中量化面临的挑战，特别是处理矩阵乘法中的**离群值（outliers）**。传统的均匀量化对离群值敏感。LLM.int8采用了一种**混合精度分解（mixed-precision decomposition）**策略：将离群值保留在FP16精度，而其余部分则量化为Int8。\n",
    "其过程包括：\n",
    "1.  找出每向量的常量（Cx, Cw）。\n",
    "2.  对输入矩阵X进行量化：$X_{i8} = \\text{round}(X_{f16} \\cdot (127/C_x))$。\n",
    "3.  对权重矩阵W进行量化：$W_{i8} = \\text{round}(W_{f16} \\cdot (127/C_w))$。\n",
    "4.  进行Int8矩阵乘法得到Out$_{i32}$。\n",
    "5.  反量化：$Out_{f16} = Out_{i32} \\cdot (C_x C_w) / (127 \\cdot 127)$。\n",
    "这种方法使得在单个GPU上推理175B参数的模型成为可能。\n",
    "\n",
    "**PyTorch LLM.int8 代码示例（使用bitsandbytes库）：**\n",
    "在实际应用中，LLM.int8通常通过专门的库实现，如`bitsandbytes`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31583206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes accelerate transformers torch\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 加载一个支持int8的Hugging Face模型\n",
    "# 注意：这需要较多的RAM来加载模型，即使是int8\n",
    "# model_id = \"decapoda-research/llama-7b-hf\" # Llama 7B for example\n",
    "# For practical demonstration, let's use a smaller model if GPU memory is limited\n",
    "model_id = \"EleutherAI/gpt-neo-125m\" \n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # 加载int8量化模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        load_in_8bit=True, # 启用bitsandbytes的int8加载\n",
    "        device_map=\"auto\" # 自动分配到可用的GPU设备\n",
    "    )\n",
    "    print(\"Model loaded in 8-bit mode successfully!\")\n",
    "    print(model)\n",
    "\n",
    "    # 验证模型参数类型\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dtype == torch.int8:\n",
    "            print(f\"Parameter '{name}' is of type {param.dtype}\")\n",
    "            break # 找到一个int8参数即可\n",
    "\n",
    "    # 进行推理\n",
    "    input_text = \"Hello, my name is\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\") # 确保输入在GPU上\n",
    "\n",
    "    # Generate some text\n",
    "    outputs = model.generate(inputs.input_ids, max_new_tokens=20)\n",
    "    print(f\"Generated text: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or using 8-bit model: {e}\")\n",
    "    print(\"Consider using a GPU with sufficient memory or a smaller model.\")\n",
    "    print(\"If bitsandbytes is not installed correctly, try: pip install bitsandbytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b2fb8",
   "metadata": {},
   "source": [
    "**硬件考量**\n",
    "并非所有数据类型（如Int3）都受到硬件的广泛支持。PyTorch目前仅支持特定的数据类型，例如不支持Int4。一些先进的量化方法甚至需要定制的硬件加速器才能发挥最大效用，这在通用商品硬件中并不常见。\n",
    "\n",
    "**量化感知训练（Quantization-Aware Training, QAT）**\n",
    "QAT在训练过程中模拟量化效应，使得模型在训练时就能适应量化带来的精度损失，从而在量化后保持更好的性能。\n",
    "\n",
    "*   **ZeroQuant：** 针对大型Transformer模型的后训练量化方法。它通过逐层量化感知蒸馏来初始化量化网络，并训练每层模拟其全精度对应层的输出。\n",
    "*   **Q-LoRA（Dettmers et al. 2023）：** 一种在训练过程中进一步压缩内存需求的方法。它采用4位量化模型，并利用GPU内存分页技术来防止内存溢出（OOM）。Q-LoRA使得在一个48GB GPU上训练一个65B参数的模型成为可能。\n",
    "\n",
    "**PyTorch Q-LoRA 代码示例（使用PEFT库）：**\n",
    "Q-LoRA通常与LoRA（Parameter-Efficient Fine-tuning）结合使用，通过`peft`库实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft transformers bitsandbytes torch accelerate\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# 1. 配置4位量化\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # NormalFloat4\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # 使用bf16进行计算，避免精度损失\n",
    ")\n",
    "\n",
    "model_name = \"facebook/opt-125m\" # 使用一个小型OPT模型进行演示\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. 准备模型进行kbit训练（适配LoRA）\n",
    "model.gradient_checkpointing_enable() # 节省显存\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 3. 配置LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # LoRA的秩\n",
    "    lora_alpha=32, # LoRA的缩放因子\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # 针对注意力机制中的Q和V投影层\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # 任务类型\n",
    ")\n",
    "\n",
    "# 4. 获取PEFT模型\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"Model with LoRA and 4-bit quantization:\")\n",
    "model.print_trainable_parameters() # 打印可训练参数数量\n",
    "\n",
    "# 模型现在可以用于微调了，训练循环与标准PyTorch训练类似\n",
    "# 例如：\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "# training_args = TrainingArguments(...)\n",
    "# trainer = Trainer(model=model, args=training_args, ...)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2e69b",
   "metadata": {},
   "source": [
    "##### 2.2 剪枝（Pruning）\n",
    "\n",
    "剪枝是一种通过移除模型中不重要或冗余的参数来减小模型大小的方法。\n",
    "\n",
    "**剪枝与量化的区别：**\n",
    "*   **量化：** 保持模型结构不变，但减少参数的比特数（精度）。\n",
    "*   **剪枝：** 将一部分参数设为零（移除），其余参数不变。\n",
    "\n",
    "**幅度剪枝（Magnitude Pruning）**\n",
    "幅度剪枝是一种非结构化剪枝方法，即通过将权重绝对值最小的X%参数设置为零。这种方法简单直观，但由于稀疏性不规则，可能难以在通用硬件上实现实际的加速。通常，剪枝后需要对模型进行重新训练（retrain）以恢复性能。\n",
    "\n",
    "**彩票假说（Lottery Ticket Hypothesis）**\n",
    "Frankle et al. (2018) 提出了“彩票假说”：一个随机初始化的全连接网络中包含一个或多个“中奖彩票”子网络，如果单独训练这些子网络，它们能够达到与原始网络相同甚至更好的性能。这表明了神经网络的过参数化可能包含了许多冗余的连接。\n",
    "\n",
    "**Wanda**\n",
    "Wanda（Sun et al. 2023）是一种新的剪枝方法，它结合了权重幅度和激活的L2范数来决定剪枝哪些参数。具体来说，剪枝的判据是$S = |W| \\cdot ||X||_2$，即权重绝对值与对应激活的L2范数的乘积。这种方法旨在剪枝对输出影响最小的权重。\n",
    "\n",
    "**概念性剪枝代码（PyTorch）：**\n",
    "PyTorch的`torch.nn.utils.prune`模块提供了丰富的剪枝功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce74863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters:\n",
      "linear.weight before pruning:\n",
      "Parameter containing:\n",
      "tensor([[-1.9534e-01, -2.9356e-01,  2.3845e-01,  7.7037e-02, -2.4731e-01,\n",
      "          1.5645e-01,  2.4912e-01,  1.8919e-01,  2.5840e-01,  8.1407e-02],\n",
      "        [ 1.1349e-01,  1.7846e-01,  2.4338e-01,  2.3711e-01, -2.6512e-01,\n",
      "         -1.9502e-01, -9.4054e-02, -1.4815e-01,  9.4680e-03, -3.6257e-02],\n",
      "        [ 3.0086e-01,  2.7735e-01, -1.1782e-01,  4.4896e-02, -2.0178e-01,\n",
      "         -3.1067e-01, -1.7419e-01,  9.9636e-02, -2.3708e-01,  6.8712e-02],\n",
      "        [-7.3138e-04, -1.2681e-01,  1.2014e-01, -1.6106e-01, -1.3089e-01,\n",
      "         -1.7997e-01, -1.1545e-01,  2.3043e-02,  2.2303e-01,  2.6058e-01],\n",
      "        [ 1.8555e-01,  3.0814e-01, -1.5441e-01,  1.1571e-01, -2.2193e-01,\n",
      "         -4.9096e-02, -3.1000e-01,  1.0859e-01, -4.5180e-02,  6.2124e-02],\n",
      "        [ 7.7372e-03,  1.6895e-01, -1.3120e-01, -1.3788e-01, -4.3604e-02,\n",
      "         -1.6843e-01, -3.0346e-01,  1.3830e-02, -2.8300e-01,  2.2539e-01],\n",
      "        [ 2.7985e-01,  1.8171e-01, -2.3435e-01,  1.5801e-01,  9.7305e-05,\n",
      "         -2.4989e-04, -1.5690e-01, -1.8580e-01,  1.6370e-01,  8.3035e-02],\n",
      "        [-1.9671e-01,  1.1646e-01, -1.5630e-01,  1.4977e-01,  9.1531e-02,\n",
      "          1.2732e-01, -2.8348e-01,  6.4231e-02,  2.7113e-01,  2.5264e-02],\n",
      "        [ 2.4690e-01, -2.5836e-01, -1.8780e-01,  1.7731e-01,  9.0132e-02,\n",
      "         -3.0438e-01,  2.1295e-02,  1.4997e-01, -1.9465e-03, -1.7118e-01],\n",
      "        [ 6.2442e-02,  1.2679e-01,  2.8807e-01,  5.9313e-02,  1.8563e-01,\n",
      "         -1.3387e-01, -1.9701e-01,  1.0511e-01, -7.8985e-02, -1.1090e-01]],\n",
      "       requires_grad=True)\n",
      "output.weight before pruning:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0224, -0.3044,  0.0137,  0.1417, -0.1658, -0.2607,  0.3137,  0.1840,\n",
      "          0.0121, -0.1694]], requires_grad=True)\n",
      "\n",
      "'Linear.weight' is not pruned yet, proceeding to apply pruning.\n",
      "Applying 50.0% magnitude pruning to 'Linear.weight'...\n",
      "\n",
      "Model parameters after 50.0% magnitude pruning on 'Linear.weight':\n",
      "Linear.weight_orig (original values):\n",
      "Parameter containing:\n",
      "tensor([[-1.9534e-01, -2.9356e-01,  2.3845e-01,  7.7037e-02, -2.4731e-01,\n",
      "          1.5645e-01,  2.4912e-01,  1.8919e-01,  2.5840e-01,  8.1407e-02],\n",
      "        [ 1.1349e-01,  1.7846e-01,  2.4338e-01,  2.3711e-01, -2.6512e-01,\n",
      "         -1.9502e-01, -9.4054e-02, -1.4815e-01,  9.4680e-03, -3.6257e-02],\n",
      "        [ 3.0086e-01,  2.7735e-01, -1.1782e-01,  4.4896e-02, -2.0178e-01,\n",
      "         -3.1067e-01, -1.7419e-01,  9.9636e-02, -2.3708e-01,  6.8712e-02],\n",
      "        [-7.3138e-04, -1.2681e-01,  1.2014e-01, -1.6106e-01, -1.3089e-01,\n",
      "         -1.7997e-01, -1.1545e-01,  2.3043e-02,  2.2303e-01,  2.6058e-01],\n",
      "        [ 1.8555e-01,  3.0814e-01, -1.5441e-01,  1.1571e-01, -2.2193e-01,\n",
      "         -4.9096e-02, -3.1000e-01,  1.0859e-01, -4.5180e-02,  6.2124e-02],\n",
      "        [ 7.7372e-03,  1.6895e-01, -1.3120e-01, -1.3788e-01, -4.3604e-02,\n",
      "         -1.6843e-01, -3.0346e-01,  1.3830e-02, -2.8300e-01,  2.2539e-01],\n",
      "        [ 2.7985e-01,  1.8171e-01, -2.3435e-01,  1.5801e-01,  9.7305e-05,\n",
      "         -2.4989e-04, -1.5690e-01, -1.8580e-01,  1.6370e-01,  8.3035e-02],\n",
      "        [-1.9671e-01,  1.1646e-01, -1.5630e-01,  1.4977e-01,  9.1531e-02,\n",
      "          1.2732e-01, -2.8348e-01,  6.4231e-02,  2.7113e-01,  2.5264e-02],\n",
      "        [ 2.4690e-01, -2.5836e-01, -1.8780e-01,  1.7731e-01,  9.0132e-02,\n",
      "         -3.0438e-01,  2.1295e-02,  1.4997e-01, -1.9465e-03, -1.7118e-01],\n",
      "        [ 6.2442e-02,  1.2679e-01,  2.8807e-01,  5.9313e-02,  1.8563e-01,\n",
      "         -1.3387e-01, -1.9701e-01,  1.0511e-01, -7.8985e-02, -1.1090e-01]],\n",
      "       requires_grad=True)\n",
      "Linear.weight_mask:\n",
      "tensor([[1., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 1., 0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0.]])\n",
      "Linear.weight (masked view):\n",
      "tensor([[-0.1953, -0.2936,  0.2384,  0.0000, -0.2473,  0.0000,  0.2491,  0.1892,\n",
      "          0.2584,  0.0000],\n",
      "        [ 0.0000,  0.1785,  0.2434,  0.2371, -0.2651, -0.1950, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000],\n",
      "        [ 0.3009,  0.2773, -0.0000,  0.0000, -0.2018, -0.3107, -0.1742,  0.0000,\n",
      "         -0.2371,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.1611, -0.0000, -0.1800, -0.0000,  0.0000,\n",
      "          0.2230,  0.2606],\n",
      "        [ 0.1856,  0.3081, -0.0000,  0.0000, -0.2219, -0.0000, -0.3100,  0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [ 0.0000,  0.1690, -0.0000, -0.0000, -0.0000, -0.1684, -0.3035,  0.0000,\n",
      "         -0.2830,  0.2254],\n",
      "        [ 0.2799,  0.1817, -0.2343,  0.1580,  0.0000, -0.0000, -0.1569, -0.1858,\n",
      "          0.1637,  0.0000],\n",
      "        [-0.1967,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.2835,  0.0000,\n",
      "          0.2711,  0.0000],\n",
      "        [ 0.2469, -0.2584, -0.1878,  0.1773,  0.0000, -0.3044,  0.0000,  0.0000,\n",
      "         -0.0000, -0.1712],\n",
      "        [ 0.0000,  0.0000,  0.2881,  0.0000,  0.1856, -0.0000, -0.1970,  0.0000,\n",
      "         -0.0000, -0.0000]], grad_fn=<MulBackward0>)\n",
      "Sparsity of Linear.weight: 50.00%\n",
      "\n",
      "Attempting to make pruning permanent by calling prune.remove() on 'Linear.weight'...\n",
      "Pruning made permanent for 'Linear.weight'. Original '_orig' and '_mask' attributes removed.\n",
      "\n",
      "Model parameters after making pruning permanent for 'Linear.weight':\n",
      "Linear.weight:\n",
      "Parameter containing:\n",
      "tensor([[-0.1953, -0.2936,  0.2384,  0.0000, -0.2473,  0.0000,  0.2491,  0.1892,\n",
      "          0.2584,  0.0000],\n",
      "        [ 0.0000,  0.1785,  0.2434,  0.2371, -0.2651, -0.1950, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000],\n",
      "        [ 0.3009,  0.2773, -0.0000,  0.0000, -0.2018, -0.3107, -0.1742,  0.0000,\n",
      "         -0.2371,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.1611, -0.0000, -0.1800, -0.0000,  0.0000,\n",
      "          0.2230,  0.2606],\n",
      "        [ 0.1856,  0.3081, -0.0000,  0.0000, -0.2219, -0.0000, -0.3100,  0.0000,\n",
      "         -0.0000,  0.0000],\n",
      "        [ 0.0000,  0.1690, -0.0000, -0.0000, -0.0000, -0.1684, -0.3035,  0.0000,\n",
      "         -0.2830,  0.2254],\n",
      "        [ 0.2799,  0.1817, -0.2343,  0.1580,  0.0000, -0.0000, -0.1569, -0.1858,\n",
      "          0.1637,  0.0000],\n",
      "        [-0.1967,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.2835,  0.0000,\n",
      "          0.2711,  0.0000],\n",
      "        [ 0.2469, -0.2584, -0.1878,  0.1773,  0.0000, -0.3044,  0.0000,  0.0000,\n",
      "         -0.0000, -0.1712],\n",
      "        [ 0.0000,  0.0000,  0.2881,  0.0000,  0.1856, -0.0000, -0.1970,  0.0000,\n",
      "         -0.0000, -0.0000]], requires_grad=True)\n",
      "Sparsity of Linear.weight: 50.00%\n",
      "Has weight_orig after remove: False\n",
      "\n",
      "Sparsity of 'output.weight' (should be 0% as it wasn't pruned):\n",
      "Sparsity of Linear.weight: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# 定义一个简单的线性层\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(self.relu(self.linear(x)))\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 在剪枝前，查看原始模型的参数\n",
    "print(\"Original model parameters:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"{name}.weight before pruning:\\n{module.weight}\")\n",
    "\n",
    "# --- 修正点开始 ---\n",
    "# 检查 'linear.weight' 参数是否已经被剪枝。\n",
    "# 我们通过检查是否存在 '_orig' 后缀的属性来判断。\n",
    "parameter_name_to_prune = 'weight' # 我们要剪枝的参数名\n",
    "linear_module = model.linear # 要剪枝的模块\n",
    "\n",
    "if hasattr(linear_module, f\"{parameter_name_to_prune}_orig\"):\n",
    "    print(f\"\\n'{linear_module.__class__.__name__}.{parameter_name_to_prune}' was already pruned, removing previous pruning before applying new one.\")\n",
    "    prune.remove(linear_module, parameter_name_to_prune)\n",
    "else:\n",
    "    print(f\"\\n'{linear_module.__class__.__name__}.{parameter_name_to_prune}' is not pruned yet, proceeding to apply pruning.\")\n",
    "# --- 修正点结束 ---\n",
    "\n",
    "pruning_amount = 0.5 # 剪枝比例\n",
    "# 对 'linear.weight' 进行全局幅度剪枝，移除50%的连接\n",
    "print(f\"Applying {pruning_amount*100}% magnitude pruning to '{linear_module.__class__.__name__}.{parameter_name_to_prune}'...\")\n",
    "prune.l1_unstructured(linear_module, name=parameter_name_to_prune, amount=pruning_amount)\n",
    "\n",
    "# 剪枝后，PyTorch会在原始权重上创建一个“视图”，并引入 weight_orig 和 weight_mask\n",
    "# 此时 linear_module.weight 实际上是 weight_orig * weight_mask\n",
    "print(f\"\\nModel parameters after {pruning_amount*100}% magnitude pruning on '{linear_module.__class__.__name__}.{parameter_name_to_prune}':\")\n",
    "# 由于我们只剪枝了 linear.weight，所以这里直接检查 linear_module\n",
    "if hasattr(linear_module, f\"{parameter_name_to_prune}_orig\"):\n",
    "    print(f\"{linear_module.__class__.__name__}.{parameter_name_to_prune}_orig (original values):\\n{getattr(linear_module, f'{parameter_name_to_prune}_orig')}\")\n",
    "    print(f\"{linear_module.__class__.__name__}.{parameter_name_to_prune}_mask:\\n{getattr(linear_module, f'{parameter_name_to_prune}_mask')}\")\n",
    "    print(f\"{linear_module.__class__.__name__}.{parameter_name_to_prune} (masked view):\\n{getattr(linear_module, parameter_name_to_prune)}\")\n",
    "    # 查看剪枝后的稀疏性\n",
    "    print(f\"Sparsity of {linear_module.__class__.__name__}.{parameter_name_to_prune}: {100. * float(torch.sum(getattr(linear_module, parameter_name_to_prune) == 0)) / getattr(linear_module, parameter_name_to_prune).numel():.2f}%\")\n",
    "else:\n",
    "    print(f\"Error: {linear_module.__class__.__name__}.{parameter_name_to_prune} was expected to be pruned but _orig attribute is missing.\")\n",
    "\n",
    "\n",
    "# 如果需要固化剪枝结果（将稀疏权重变为永久性的，移除 weight_orig 和 weight_mask），\n",
    "# 可以在训练完成后，在保存模型之前调用 prune.remove()。\n",
    "print(f\"\\nAttempting to make pruning permanent by calling prune.remove() on '{linear_module.__class__.__name__}.{parameter_name_to_prune}'...\")\n",
    "if hasattr(linear_module, f\"{parameter_name_to_prune}_orig\"):\n",
    "    prune.remove(linear_module, parameter_name_to_prune)\n",
    "    print(f\"Pruning made permanent for '{linear_module.__class__.__name__}.{parameter_name_to_prune}'. Original '_orig' and '_mask' attributes removed.\")\n",
    "else:\n",
    "    print(f\"Error: '{linear_module.__class__.__name__}.{parameter_name_to_prune}' was not pruned, so no pruning to remove (this should not happen after previous prune call).\")\n",
    "\n",
    "print(f\"\\nModel parameters after making pruning permanent for '{linear_module.__class__.__name__}.{parameter_name_to_prune}':\")\n",
    "# 此时 module.weight 已经包含了剪枝后的值，且不再有 weight_orig 或 weight_mask\n",
    "print(f\"{linear_module.__class__.__name__}.{parameter_name_to_prune}:\\n{getattr(linear_module, parameter_name_to_prune)}\")\n",
    "print(f\"Sparsity of {linear_module.__class__.__name__}.{parameter_name_to_prune}: {100. * float(torch.sum(getattr(linear_module, parameter_name_to_prune) == 0)) / getattr(linear_module, parameter_name_to_prune).numel():.2f}%\")\n",
    "# 再次确认：hasattr(module, 'weight_orig') 应该返回 False\n",
    "print(f\"Has {parameter_name_to_prune}_orig after remove: {hasattr(linear_module, f'{parameter_name_to_prune}_orig')}\")\n",
    "            \n",
    "# 验证 output.weight 没有被剪枝（应该保持0%稀疏性）\n",
    "print(f\"\\nSparsity of 'output.weight' (should be 0% as it wasn't pruned):\")\n",
    "output_module = model.output\n",
    "print(f\"Sparsity of {output_module.__class__.__name__}.weight: {100. * float(torch.sum(output_module.weight == 0)) / output_module.weight.numel():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc7bf9",
   "metadata": {},
   "source": [
    "**非结构化剪枝的挑战**\n",
    "非结构化剪枝（如幅度剪枝）产生的稀疏性在通用商品硬件上不一定能带来内存或速度的实际提升。因为大部分硬件是为密集矩阵运算优化的，处理不规则稀疏矩阵需要额外的开销。这促使了对支持稀疏数据结构和乘法运算的专用硬件或结构化剪枝的研究。\n",
    "\n",
    "**结构化剪枝（Structured Pruning）**\n",
    "结构化剪枝旨在移除整个组件，如注意力头、层或神经元块，从而产生更规则的稀疏模式，更易于硬件加速。\n",
    "*   **Xia et al. (2022)** 提出学习“掩码”来控制哪些组件被关闭。Transformer层由自注意力（self-attention）和前馈（feed-forward）两部分组成。粗粒度掩码可以关闭整个自注意力或前馈组件，细粒度掩码则可以关闭注意力头或隐藏状态维度。\n",
    "*   **Michel and Neubig (2019)** 的研究“Are Sixteen Heads Really Better than One?”表明，Transformer模型中许多注意力头是可以被剪枝的，而对性能影响甚微。这为结构化剪枝提供了经验依据。\n",
    "*   **Coarse-to-Fine Structured Pruning：** 结合了粗粒度（如整个FFN层或MHA层）和细粒度（如注意力头或隐藏维度）剪枝。\n",
    "*   **基于前向传播的剪枝（Pruning w/ Forward Passes, Dery et al. 2024）：** 针对大型模型结构化剪枝需要大量内存的问题，提出了一种无需梯度的剪枝方法。它通过测量模型在不同模块被掩码时的性能，并使用回归模型学习每个模块掩码的影响，从而决定剪枝策略。这种方法可以实现显著的推理加速。\n",
    "\n",
    "##### 2.3 蒸馏（Distillation）\n",
    "\n",
    "蒸馏（Knowledge Distillation）是一种模型压缩技术，通过训练一个小型模型（学生模型）来模仿一个大型、高性能模型（教师模型）的行为。\n",
    "\n",
    "**蒸馏与量化、剪枝的区别：**\n",
    "*   **量化：** 保持模型结构和参数数量，降低参数精度。\n",
    "*   **剪枝：** 移除部分参数，可能改变模型结构。\n",
    "*   **蒸馏：** 训练一个全新的、通常更小的模型，使其学习大型模型的输出分布和/或中间表示。\n",
    "\n",
    "**弱监督（Weak Supervision）**\n",
    "蒸馏可以被视为一种弱监督形式。教师模型为未标记的数据生成“伪标签”（pseudo-labels），学生模型则在这些伪标签上进行训练，就像它们是真实标签一样。这个想法在许多领域都有体现，如自训练（Yarowsky 1995）、协同训练（Blum and Mitchell 1998）和元伪标签（Meta Pseudo Labels, Pham et al. 2020）。\n",
    "\n",
    "**硬目标 vs. 软目标（Hard vs. Soft Targets）**\n",
    "*   **硬目标：** 传统的独热编码标签（one-hot encoding），只关注正确类别的概率为1。\n",
    "*   **软目标：** 教师模型输出的类别概率分布。软目标提供了比硬目标更丰富的类别间关系信息，特别是在高熵（不确定性高）的情况下。Hinton et al. (2015) 发现，使用软目标训练学生模型，即使在训练数据有限的情况下，也能显著提高学生模型的性能。\n",
    "\n",
    "**PyTorch 软目标蒸馏损失函数示例：**\n",
    "软目标蒸馏通常使用KL散度（Kullback-Leibler divergence）来衡量学生模型输出与教师模型输出概率分布之间的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e7da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distillation Loss: 0.807767391204834\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    计算蒸馏损失。\n",
    "    Args:\n",
    "        student_logits: 学生模型的原始输出 (logits)。\n",
    "        teacher_logits: 教师模型的原始输出 (logits)。\n",
    "        temperature: 软化概率分布的温度参数。\n",
    "        alpha: 蒸馏损失在总损失中的权重。\n",
    "    \"\"\"\n",
    "    # 软化教师和学生模型的概率分布\n",
    "    soft_teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    soft_student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "\n",
    "    # 计算KL散度作为蒸馏损失\n",
    "    # reduction='batchmean' 对应 Hinton et al. 论文中的损失定义\n",
    "    distill_loss = F.kl_div(soft_student_log_probs, soft_teacher_probs, reduction='batchmean') * (temperature**2)\n",
    "\n",
    "    # 在实际应用中，通常会结合原始的交叉熵损失 (硬目标损失)\n",
    "    # 例如，如果还有真实标签 hard_labels：\n",
    "    # hard_loss = F.cross_entropy(student_logits, hard_labels)\n",
    "    # total_loss = alpha * hard_loss + (1 - alpha) * distill_loss\n",
    "    # 在这里我们只演示蒸馏损失\n",
    "    return distill_loss\n",
    "\n",
    "# 示例使用\n",
    "student_logits = torch.randn(4, 10) # batch_size=4, num_classes=10\n",
    "teacher_logits = torch.randn(4, 10)\n",
    "\n",
    "loss = distillation_loss(student_logits, teacher_logits, temperature=2.0)\n",
    "print(f\"Distillation Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30109900",
   "metadata": {},
   "source": [
    "**“重生”神经网络（Born Again Neural Networks, BANs）**\n",
    "BANs（Furlanello, Lipton, et al. 2018）是一种迭代蒸馏方法。在每一轮中，前一轮训练的学生模型将成为当前轮训练新学生模型的教师模型。通过这种方式，可以逐步提高模型的性能，甚至超越原始教师模型的性能。\n",
    "\n",
    "**序列级蒸馏（Sequence-Level Distillation）**\n",
    "Kim and Rush (2016) 将蒸馏的概念扩展到序列生成任务。有两种主要方法：\n",
    "1.  **词级蒸馏（Word-level distillation）：** 匹配教师模型在每个时间步的词语分布。\n",
    "    $L_{\\text{WORD-KD}} = -\\sum_{j=1}^J \\sum_{k=1}^{|V|} q(t_j = k|s, t_{<j}) \\log p(t_j = k|s, t_{<j})$\n",
    "    其中，$q$是教师模型的概率，$p$是学生模型的概率。\n",
    "2.  **序列级蒸馏（Sequence-level distillation）：** 最大化教师模型生成序列的概率。\n",
    "    $L_{\\text{SEQ-KD}} \\approx -\\sum_{t \\in T} \\mathbf{1}\\{t = \\hat{y}\\} \\log p(t|s)$\n",
    "    其中，$\\hat{y}$是教师模型生成的序列。\n",
    "通常会结合一个传统的序列负对数似然损失（NLL）和序列级蒸馏损失：\n",
    "$L = (1-\\alpha)L_{\\text{SEQ-NLL}} + \\alpha L_{\\text{SEQ-KD}}$\n",
    "\n",
    "**DistilBERT**\n",
    "DistilBERT（Sanh et al. 2019）是一个著名的蒸馏案例，它将BERT模型缩小了一半层数，总参数量减少了40%，同时保留了97%的BERT性能。其技巧包括：\n",
    "*   使用BERT的交替层来初始化DistilBERT。\n",
    "*   结合了传统的监督损失和基于蒸馏的损失。\n",
    "*   在教师模型和学生模型的隐藏状态向量之间添加了余弦相似度损失。\n",
    "*   研究发现，监督损失（硬目标）对DistilBERT的帮助不大，主要是蒸馏损失在起作用。\n",
    "\n",
    "**自指导（Self-Instruct）**\n",
    "自指导（Wang et al. 2022）利用大型语言模型（LLM）本身来生成指令微调（instruction-tuning）数据集。这个过程包括指令生成、任务识别、实例生成和过滤。生成的指令-实例对可以用于训练更小的模型，使其能够遵循人类指令。这进一步展示了蒸馏和自监督技术在数据生成中的强大潜力，例如可以用于训练思维链（chain-of-thought）模型（ORCA, Mukherjee et al. 2023）或生成更复杂的指令（Evol-Instruct, Xu et al. 2023）。\n",
    "\n",
    "**Prompt2Model**\n",
    "Prompt2Model（Viswanathan et al. 2023）是一个利用提示（prompt）来生成可部署模型的系统。它通过一个描述任务的提示来检索或生成数据，并选择或训练一个合适的预训练模型，最终输出一个针对特定任务的、可部署的模型。\n",
    "\n",
    "**合成数据生成工具包**\n",
    "Patel et al. (2024) 提出了一个用于合成数据生成的工具包，涵盖了从数据源加载、提示工程、模型选择到训练器的整个流程。这使得研究人员和开发者能够更系统地创建和利用合成数据进行模型训练和微调。\n",
    "\n",
    "### 数据高效微调（Data-Efficient Fine-tuning）\n",
    "\n",
    "大型预训练语言模型（PLMs）在各种下游任务上表现出色，但其微调面临两个主要问题：\n",
    "1.  **数据稀缺性：** 对于许多下游任务，获取大量标记数据成本高昂。\n",
    "2.  **模型过大：** PLMs的参数量巨大，导致每个下游任务都需要一个完整的模型副本，增加了存储和部署成本。\n",
    "\n",
    "数据高效微调旨在解决这些问题，它侧重于在少量数据上实现高性能，并减少微调过程中需要更新的参数量。\n",
    "\n",
    "#### 1. 参数高效微调（Parameter-Efficient Fine-tuning, PEFT）\n",
    "\n",
    "PEFT 方法旨在通过只更新少量额外参数或模型原有参数的一个小子集，来适配预训练模型到下游任务，而不是更新整个模型。\n",
    "\n",
    "**核心思想：**\n",
    "PEFT方法不是为每个下游任务复制一个完整的PLM副本，而是共享一个大型预训练模型，并为每个任务添加少量任务特定的参数。\n",
    "\n",
    "**工作原理：**\n",
    "通过引入特殊的子模块来修改隐藏表示。例如，对于输入$h$，新的隐藏表示$h'$可以表示为$h' = h + \\Delta h$，其中$\\Delta h$是由这些新增的子模块生成的。\n",
    "\n",
    "##### 1.1 Adapter\n",
    "\n",
    "Adapter（Houlsby et al. 2019）是PEFT的早期方法之一。它在Transformer的每个层中插入小型模块，通常位于多头注意力和前馈网络之后。在微调过程中，只有这些Adapter模块的参数会被更新，而原始的Transformer层保持冻结。这大大减少了可训练参数的数量。\n",
    "\n",
    "##### 1.2 LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA（Hu et al. 2021）是一种更高效的PEFT方法。它的核心思想是，在微调过程中，预训练模型权重的变化量$\\Delta W$通常是低秩的。因此，可以通过两个较小的低秩矩阵$A$和$B$的乘积来近似这个变化量，即$\\Delta W = BA$。\n",
    "LoRA通常应用于Transformer层中的线性层，如查询（Query）、键（Key）、值（Value）和输出投影。\n",
    "具体来说，对于一个权重矩阵$W_0 \\in \\mathbb{R}^{d \\times k}$，LoRA引入了两个矩阵$B \\in \\mathbb{R}^{d \\times r}$和$A \\in \\mathbb{R}^{r \\times k}$，其中秩$r \\ll \\min(d, k)$。在微调时，只训练$A$和$B$，而$W_0$保持冻结。更新后的权重为$W_0 + BA$。\n",
    "\n",
    "**LoRA的数学表示：**\n",
    "对于输入$h \\in \\mathbb{R}^d$，标准的Transformer层中的线性变换为$W_0 h$。\n",
    "引入LoRA后，新的变换变为$(W_0 + BA)h = W_0 h + B(Ah)$。\n",
    "其中，$A$将$h$映射到低维空间$r$，然后$B$将这个低维表示映射回原始维度$d$。\n",
    "例如，在前馈层的上投影部分，维度通常为$d_{\\text{FFW}} \\times d_{\\text{model}}$。LoRA通过引入一个秩为$r$的低秩分解，使得可训练参数数量大幅减少。\n",
    "\n",
    "**LoRA的优势：**\n",
    "1.  **参数量大幅减少：** 相较于全量微调，LoRA的额外参数量通常小于0.1%。\n",
    "2.  **防止过拟合：** 由于可训练参数很少，LoRA在小数据集上表现出色，且不容易过拟合。\n",
    "3.  **更好的域外性能：** 在一些跨领域任务上，LoRA表现出比全量微调更好的泛化能力。\n",
    "\n",
    "**PyTorch LoRA 代码示例（概念性实现）：**\n",
    "实际使用中，通常使用Hugging Face的`peft`库来简化LoRA的集成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c41dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA layer trainable parameters:\n",
      "  lora_A: requires_grad=True\n",
      "  lora_B: requires_grad=True\n",
      "  original_layer.weight: requires_grad=False\n",
      "  original_layer.bias: requires_grad=False\n",
      "Original layer weight requires_grad: False\n",
      "Output shape: torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_linear_layer, rank):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_linear_layer\n",
    "        self.in_features = original_linear_layer.in_features\n",
    "        self.out_features = original_linear_layer.out_features\n",
    "        self.rank = rank\n",
    "\n",
    "        # LoRA A 和 B 矩阵\n",
    "        self.lora_A = nn.Parameter(torch.randn(self.in_features, self.rank))\n",
    "        self.lora_B = nn.Parameter(torch.randn(self.rank, self.out_features))\n",
    "\n",
    "        # LoRA的缩放因子，通常与 rank 相关\n",
    "        self.scaling = 1.0 / rank\n",
    "\n",
    "        # 冻结原始层的权重\n",
    "        self.original_layer.weight.requires_grad = False\n",
    "        if original_linear_layer.bias is not None:\n",
    "            self.original_layer.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 原始线性变换的输出\n",
    "        original_output = self.original_layer(x)\n",
    "\n",
    "        # LoRA路径的输出\n",
    "        lora_output = x @ self.lora_A @ self.lora_B * self.scaling\n",
    "\n",
    "        return original_output + lora_output\n",
    "\n",
    "# 示例使用：替换模型中的一个nn.Linear层\n",
    "# 假设我们有一个预训练的线性层\n",
    "pretrained_linear = nn.Linear(100, 50)\n",
    "# 通常，你会从一个预训练模型中获取这个层，并加载其权重\n",
    "\n",
    "# 创建LoRA层\n",
    "lora_linear = LoRALayer(pretrained_linear, rank=4)\n",
    "\n",
    "# 打印可训练参数\n",
    "print(\"LoRA layer trainable parameters:\")\n",
    "for name, param in lora_linear.named_parameters():\n",
    "    print(f\"  {name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "# 检查原始层权重是否冻结\n",
    "print(f\"Original layer weight requires_grad: {lora_linear.original_layer.weight.requires_grad}\")\n",
    "\n",
    "# 进行一次前向传播\n",
    "input_tensor = torch.randn(1, 100)\n",
    "output_tensor = lora_linear(input_tensor)\n",
    "print(f\"Output shape: {output_tensor.shape}\")\n",
    "\n",
    "# 实际上，PEFT库会自动帮你替换模型中的层\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# config = LoraConfig(r=4, lora_alpha=8, target_modules=[\"query\", \"value\"])\n",
    "# model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa2b7d",
   "metadata": {},
   "source": [
    "##### 1.3 前缀/提示微调（Prefix/Prompt Tuning）\n",
    "\n",
    "前缀/提示微调通过优化一个小的、任务特定的连续“前缀”或“提示”向量来微调模型，而保持原始Transformer参数冻结。\n",
    "*   **提示微调（Prompt Tuning）：** 只优化输入嵌入层中的提示向量。这些向量与输入文本的嵌入拼接，然后送入冻结的Transformer模型。\n",
    "*   **前缀微调（Prefix Tuning）：** 优化一个更长的“前缀”，它被添加到Transformer的每一层中，而不仅仅是输入嵌入层。这意味着每层的隐藏状态都会被修改。\n",
    "\n",
    "**前缀/提示微调的优势：**\n",
    "*   **冻结模型参数：** 原始的Transformer参数完全冻结，大大减少了可训练参数。\n",
    "*   **引入人类知识：** 提示本身可以注入人类知识或领域信息。\n",
    "*   **数据稀缺性下的优异表现：** 在少量训练数据的情况下，提示微调通常比全量微调或Adapter表现更好。\n",
    "\n",
    "**PyTorch 概念性提示/前缀微调代码：**\n",
    "这里的代码是高度简化的，实际实现会更复杂，尤其是在整合到Transformer模型内部时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd736a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PromptModel(nn.Module):\n",
    "    def __init__(self, original_lm, num_tokens=10, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        self.original_lm = original_lm\n",
    "        # 冻结原始LM的所有参数\n",
    "        for param in self.original_lm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 创建可训练的提示嵌入\n",
    "        self.prompt_embeddings = nn.Parameter(torch.randn(num_tokens, embedding_dim))\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # 获取输入文本的嵌入\n",
    "        inputs_embeds = self.original_lm.get_input_embeddings()(input_ids)\n",
    "\n",
    "        # 将提示嵌入拼接到输入嵌入之前\n",
    "        # 假设batch_size是 inputs_embeds 的第一个维度\n",
    "        batch_size = inputs_embeds.shape[0]\n",
    "        expanded_prompt_embeddings = self.prompt_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # 拼接： [batch_size, num_tokens + seq_len, embedding_dim]\n",
    "        combined_embeddings = torch.cat([expanded_prompt_embeddings, inputs_embeds], dim=1)\n",
    "\n",
    "        # 将组合后的嵌入传递给冻结的原始LM\n",
    "        # 注意：这里需要调整原始LM的forward方法以接受embeddings而不是input_ids\n",
    "        # 这是一个简化的表示，实际Transformers库会处理这个\n",
    "        return self.original_lm(inputs_embeds=combined_embeddings)\n",
    "\n",
    "# 实际应用中，你不会手动创建 PromptModel\n",
    "# 而是使用 peft 库的 PromptTuningConfig 或 PrefixTuningConfig\n",
    "# from peft import PromptTuningConfig, get_peft_model\n",
    "# config = PromptTuningConfig(\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     num_virtual_tokens=20,\n",
    "#     prompt_tuning_init_text=\"Translate English to French:\",\n",
    "#     tokenizer_name_or_path=\"t5-small\"\n",
    "# )\n",
    "# model = get_peft_model(original_t5_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6c759",
   "metadata": {},
   "source": [
    "**训练策略**\n",
    "选择PEFT方法时，需要考虑两个视角：\n",
    "*   **参数视角：** 哪些参数被更新？（例如，Adapter更新Adapter参数，LoRA更新低秩矩阵，提示微调更新提示嵌入）\n",
    "*   **数据视角：** 使用多少训练样本？（例如，零样本、少样本、全数据）\n",
    "\n",
    "不同的PEFT方法在不同数据量下表现各异。例如：\n",
    "*   对于大型预训练模型（如GPT-3）和全数据量，**无提示微调（Promptless Fine-tuning）**（即传统的全量微调或Adapter、LoRA）可能是一个不错的选择。\n",
    "*   对于少样本训练，**固定提示微调（Fixed-Prompt Tuning）**和**提示+LM微调（Prompt+LM Fine-tuning）**可能更合适。\n",
    "*   对于零样本或极少样本，**无训练提示（Tuning-free Prompting）**可能有效。\n",
    "\n",
    "#### 2. 早退（Early Exit）\n",
    "\n",
    "早退机制（Early Exit）是一种推理优化技术，通过在模型中间层添加分类器，允许模型在达到足够置信度时提前停止推理，从而节省计算资源和时间。\n",
    "*   **工作原理：** 在Transformer的每一层或每几层后添加一个额外的分类器。在推理时，模型逐层计算，并在每个中间分类器处评估其预测的置信度。一旦置信度达到预设阈值，模型就提前退出，不再计算后续层。\n",
    "*   **优势：** 在保持相近性能的同时，显著减少推理延迟。这对于对实时性要求高的应用场景尤其重要。\n",
    "\n",
    "**PyTorch 概念性早退代码：**\n",
    "在一个简单的多层感知机中模拟早退。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b745a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed full network computation.\n",
      "Final output shape: torch.Size([1, 10])\n",
      "Exiting early at layer 2 with min confidence: 0.1209\n",
      "Final output shape (with early exit): torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EarlyExitMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, exit_points, confidence_threshold=0.9):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        self.classifiers = nn.ModuleList()\n",
    "        # 在指定层后添加分类器\n",
    "        if 1 in exit_points:\n",
    "            self.classifiers.append(nn.Linear(hidden_dim, num_classes)) # Classifier for layer1\n",
    "        if 2 in exit_points:\n",
    "            self.classifiers.append(nn.Linear(hidden_dim, num_classes)) # Classifier for layer2\n",
    "        # Final classifier (always exists implicitly or explicitly as self.output_layer)\n",
    "        \n",
    "        self.exit_points = exit_points\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden1 = F.relu(self.layer1(x))\n",
    "        \n",
    "        # Exit point 1\n",
    "        if 1 in self.exit_points:\n",
    "            logits1 = self.classifiers[0](hidden1) # Assuming classifiers[0] is for layer1\n",
    "            probs1 = F.softmax(logits1, dim=-1)\n",
    "            max_prob1, _ = torch.max(probs1, dim=-1)\n",
    "            if max_prob1.min() > self.confidence_threshold: # Check if all samples in batch are confident\n",
    "                print(f\"Exiting early at layer 1 with min confidence: {max_prob1.min().item():.4f}\")\n",
    "                return logits1\n",
    "\n",
    "        hidden2 = F.relu(self.layer2(hidden1))\n",
    "        \n",
    "        # Exit point 2\n",
    "        if 2 in self.exit_points:\n",
    "            # This logic needs adjustment based on how classifiers are indexed\n",
    "            # For simplicity, assume classifiers are ordered by layer\n",
    "            classifier_idx = 0 if 1 not in self.exit_points else 1 \n",
    "            logits2 = self.classifiers[classifier_idx](hidden2)\n",
    "            probs2 = F.softmax(logits2, dim=-1)\n",
    "            max_prob2, _ = torch.max(probs2, dim=-1)\n",
    "            if max_prob2.min() > self.confidence_threshold:\n",
    "                print(f\"Exiting early at layer 2 with min confidence: {max_prob2.min().item():.4f}\")\n",
    "                return logits2\n",
    "\n",
    "        # Final layer if no early exit\n",
    "        output = self.output_layer(hidden2) # Or another layer if there's a layer3\n",
    "        print(\"Completed full network computation.\")\n",
    "        return output\n",
    "\n",
    "# 示例使用\n",
    "input_dim = 100\n",
    "hidden_dim = 50\n",
    "num_classes = 10\n",
    "exit_points = [1, 2] # 在第1和第2隐藏层之后设置退出点\n",
    "\n",
    "model_early_exit = EarlyExitMLP(input_dim, hidden_dim, num_classes, exit_points, confidence_threshold=0.95)\n",
    "dummy_input = torch.randn(1, input_dim) # 单个样本\n",
    "\n",
    "# 训练阶段，所有分类器都会被训练，推理阶段才会按置信度退出\n",
    "# 实际的早退策略会涉及在训练时对所有分类器进行蒸馏或联合训练。\n",
    "output = model_early_exit(dummy_input)\n",
    "print(f\"Final output shape: {output.shape}\")\n",
    "\n",
    "# 模拟一个非常自信的早期层输出\n",
    "model_confident = EarlyExitMLP(input_dim, hidden_dim, num_classes, exit_points, confidence_threshold=0.1) # 降低阈值更容易触发\n",
    "# 假设第一层的分类器权重使其输出高置信度\n",
    "model_confident.classifiers[0].weight.data.fill_(0.1) # 随意设置，模拟高置信度\n",
    "model_confident.classifiers[0].bias.data.fill_(10.0) # 模拟高置信度\n",
    "\n",
    "output_confident = model_confident(dummy_input)\n",
    "print(f\"Final output shape (with early exit): {output_confident.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7f76b",
   "metadata": {},
   "source": [
    "#### 3. 半监督学习（Semi-supervised Learning）\n",
    "\n",
    "半监督学习利用少量的标记数据和大量的未标记数据进行训练。\n",
    "\n",
    "**模式挖掘训练（Pattern-Exploiting Training, PET）**\n",
    "PET是一种半监督学习方法，通过将下游任务转化为完形填空任务（cloze-style tasks）来利用PLMs的知识。\n",
    "**PET的三个步骤：**\n",
    "1.  **提示微调（Prompt-tuning）：** 使用不同的提示和“语言化器”（verbalizer）在**标记数据集**上微调多个PLM。语言化器将PLM的输出（例如，掩码预测）映射回任务标签。\n",
    "2.  **预测与组合：** 使用这些微调过的PLM预测**未标记数据集**的标签。然后，将不同模型的预测结果进行组合（例如，通过投票或加权平均）生成高质量的软伪标签。\n",
    "3.  **最终训练：** 使用带有伪标签的未标记数据（现在是“软标记”数据）和原始标记数据，训练一个标准的PLM分类器（带有分类头）。\n",
    "\n",
    "PET的核心在于，它通过提示工程将下游任务与PLM的预训练目标对齐，并利用大量无监督数据进行知识增强，从而在少样本场景下取得卓越性能。\n",
    "\n",
    "**半监督伪标签概念代码：**\n",
    "这里的代码演示了从模型预测生成伪标签，并用于后续训练的基本概念。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8dba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model trained.\n",
      "Student model trained using pseudo-labels.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. 模拟一个简单的教师模型\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 2. 模拟一个简单的学生模型（最终分类器）\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 3. 模拟数据集\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None:\n",
    "            return self.data[idx]\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 假设有一些标记数据和大量未标记数据\n",
    "labeled_data = torch.randn(100, 20)\n",
    "labeled_labels = torch.randint(0, 2, (100,)) # 0 or 1\n",
    "unlabeled_data = torch.randn(1000, 20)\n",
    "\n",
    "input_dim = 20\n",
    "output_dim = 2\n",
    "\n",
    "# 训练教师模型 (这里简化为直接在标记数据上训练)\n",
    "# 在PET中，这会是多个prompt-tuned PLMs\n",
    "teacher_model = TeacherModel(input_dim, output_dim)\n",
    "teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "labeled_loader = DataLoader(DummyDataset(labeled_data, labeled_labels), batch_size=16)\n",
    "\n",
    "# Simplified teacher training loop\n",
    "for epoch in range(5):\n",
    "    for data, labels in labeled_loader:\n",
    "        teacher_optimizer.zero_grad()\n",
    "        outputs = teacher_model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        teacher_optimizer.step()\n",
    "print(\"Teacher model trained.\")\n",
    "\n",
    "# 生成伪标签 (Step 2 of PET)\n",
    "teacher_model.eval()\n",
    "pseudo_labels = []\n",
    "pseudo_logits = []\n",
    "unlabeled_loader = DataLoader(DummyDataset(unlabeled_data), batch_size=16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in unlabeled_loader:\n",
    "        logits = teacher_model(data)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # 简单地选择概率最高的作为伪标签\n",
    "        _, predicted_labels = torch.max(probs, 1)\n",
    "        \n",
    "        pseudo_labels.append(predicted_labels.cpu())\n",
    "        pseudo_logits.append(logits.cpu())\n",
    "\n",
    "pseudo_labels = torch.cat(pseudo_labels)\n",
    "pseudo_logits = torch.cat(pseudo_logits)\n",
    "\n",
    "# 结合原始标记数据和伪标记数据\n",
    "# 在PET中，通常使用软标签\n",
    "combined_data = torch.cat([labeled_data, unlabeled_data], dim=0)\n",
    "# For simplicity, let's use hard pseudo-labels here. \n",
    "# In PET, soft pseudo-labels (the logits/probabilities) are preferred for the final training.\n",
    "combined_labels = torch.cat([labeled_labels, pseudo_labels], dim=0) \n",
    "\n",
    "# 训练学生模型 (Step 3 of PET)\n",
    "student_model = StudentModel(input_dim, output_dim)\n",
    "student_optimizer = optim.Adam(student_model.parameters(), lr=0.01)\n",
    "\n",
    "combined_loader = DataLoader(DummyDataset(combined_data, combined_labels), batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for data, labels in combined_loader:\n",
    "        student_optimizer.zero_grad()\n",
    "        outputs = student_model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        student_optimizer.step()\n",
    "print(\"Student model trained using pseudo-labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd577d",
   "metadata": {},
   "source": [
    "### 结论\n",
    "\n",
    "模型压缩和数据高效微调是部署大型语言模型不可或缺的技术。\n",
    "*   **模型压缩**通过量化（降低精度）、剪枝（移除冗余参数）和蒸馏（知识迁移）来减小模型体积和计算量。\n",
    "*   **数据高效微调**则通过参数高效方法（如Adapter、LoRA、提示微调）和推理优化（如早退）来降低微调成本，使其能够在资源受限和数据稀缺的环境中高效运行。半监督学习（如PET）进一步利用未标记数据，以弥补标记数据不足的问题。\n",
    "\n",
    "这些技术的结合，使得在有限的计算资源和数据条件下，也能够有效地训练和部署高性能的AI模型，推动AI技术更广泛的普及和应用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
