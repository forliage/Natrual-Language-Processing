{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c325f477",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "**How do we represent the meaning of a word?**\n",
    "\n",
    "Definition:meaning(Webster dictionary)\n",
    "- the idea that is represented by a word, phrase, etc.\n",
    "- the idea that a person wants to express by using words, signs, etc.\n",
    "- the idea that is expressed in a work of writing, art, etc. Commonest linguistic way of thinking of meaning: \n",
    "- signifier $\\Leftrightarrow$ signified(idea or thing)=denotation\n",
    "\n",
    "**How do we have usable meaning in a computer?**\n",
    "\n",
    "Common answer: Use a taxonomy like WordNet that has hypernyms(is-a) relationships\n",
    "```python\n",
    "from nlyk.corpus import wordnet as wn\n",
    "panda = wn.synset('panda.n.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))\n",
    "```\n",
    "\n",
    "**Problems with this discrete representatiom**\n",
    "- Great as a resource but missing nuances, e.g., **synonyms**:\n",
    "   - adept, expert, good, practiced, proficient, skillful?\n",
    "- Missing new words(impossible to keep up date): wicked, badass, nifty, crack, ace, wizard, genius, ninja\n",
    "- Subjective\n",
    "- Requires human labor to create and adapt\n",
    "- Hard to compute accurate word similarity\n",
    "\n",
    "The vast majority of rule-based and statistical NLP work regards words as atomic symbols.\n",
    "\n",
    "In vector space terms, this is a vector with one 1 and a lot of zeros:$$[0\\quad 0\\quad 0...0\\quad 1\\quad 0...0]$$\n",
    "Dimensionality:20K(speech)-50K(PTB)-500K(big vocab)-13M(Google 1T)\n",
    "\n",
    "We call this a one-hot representation\n",
    "\n",
    "It is a localist representation.\n",
    "\n",
    "Its problem, e.g., for web search\n",
    "- If user searches for [<span style=\"color:pink\">Dell notebook battery size</span>], we would like to match documents with \"<span style=\"color:pink\">Dell laptop battery capacity</span>\"\n",
    "- If user searches for [<span style=\"color:pink\">Seattle motel</span>], we would like to match documents containing \"<span style=\"color:pink\">Seattle hotel</span>\"\n",
    "\n",
    "But\n",
    "\n",
    "motel $[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]^T$\n",
    "\n",
    "hotel $[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] = 0$\n",
    "\n",
    "Our query and document vectors are <span style=\"color:purple\">orthogonal</span>\n",
    "\n",
    "There is no natural notion of similarity in a set of one-hot vectors\n",
    "\n",
    "Could deal with similarity separately;instead we explore a direct approach where vectors encode it\n",
    "\n",
    "**Distributional similarity based representations**\n",
    "\n",
    "You can get a lot of value by representing a word by means of its neighbors\n",
    "\n",
    "<span style=\"color:blue\">\"You shall know a word by the company it keeps\"</span>\n",
    "\n",
    "One of the most successful ideas of modern statistical NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8739f2",
   "metadata": {},
   "source": [
    "**Word meaning is defined in terms of vectors**\n",
    "\n",
    "We will build a dense vector for each word type, chosen so that it is good at predicting other words appearing in its context...those other words also being represented by vectors...it all gets a bit recursive.\n",
    "\n",
    "**Directly learning low-dimensional word vectors**\n",
    "\n",
    "Old idea. Relevant for this lecture \\& deep learning:\n",
    "- Learning representations by back-propagating errors\n",
    "- A neural probabilistic language model\n",
    "- NLP(almost) from Scratch\n",
    "- A recent, even simpler and faster model: word2vec.\n",
    "\n",
    "**How do we select input and output words?**\n",
    "- Method 1: continuous bag-of-word(CBOW)\n",
    "- Method 2: skip-gram(SG)\n",
    "\n",
    "![CBOW and Skip-Gram](images/image1-3.png)\n",
    "\n",
    "$$E=-\\log p(w_{O,1},w_{O,2},...,w_{O,C}|w_I)$$\n",
    "- Loss function is negative probability of predictions of context words\n",
    "- The hidden layer simply selects a row of $W$ based on the input word\n",
    "   $$h=x^TW=W_{(k,\\cdot)}$$\n",
    "- This is then mapped by another matrix to an output probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465c61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26800c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小 (Vocab size): 15\n",
      "词汇表示例 (Vocab example): [('wise', 0), ('leads', 1), ('is', 2), ('rules', 3), ('the', 4)]\n",
      "------------------------------\n",
      "生成训练数据对 130 个\n",
      "训练数据示例 (center_word_idx, context_word_idx):\n",
      "  (the, king)\n",
      "  (the, is)\n",
      "  (king, the)\n",
      "  (king, is)\n",
      "  (king, a)\n"
     ]
    }
   ],
   "source": [
    "# 1. 准备一个简单的语料库\n",
    "# 这个语料库特意设计得让 'king' 和 'queen' 以及 'man' 和 'woman' 出现在相似的上下文中\n",
    "corpus = [\n",
    "    \"The king is a strong man\",\n",
    "    \"The queen is a wise woman\",\n",
    "    \"A man can be a king\",\n",
    "    \"A woman can be a queen\",\n",
    "    \"The king rules the country\",\n",
    "    \"The queen leads the people\"\n",
    "]\n",
    "\n",
    "# 2. 数据预处理\n",
    "# 将所有句子合并成一个单词列表\n",
    "words = []\n",
    "for sentence in corpus:\n",
    "    words.extend(sentence.lower().split())\n",
    "\n",
    "# 创建词汇表\n",
    "# 使用 defaultdict 方便处理未见过的词\n",
    "vocab = defaultdict(lambda: len(vocab))\n",
    "# 使用 set 去重，然后构建词汇表\n",
    "unique_words = set(words)\n",
    "for word in unique_words:\n",
    "    vocab[word] # 这行代码会触发 defaultdict 的 lambda 函数，为每个词分配一个唯一索引\n",
    "\n",
    "# 创建反向映射，方便从索引找回单词\n",
    "ix_to_word = {i: word for word, i in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"词汇表大小 (Vocab size): {vocab_size}\")\n",
    "print(f\"词汇表示例 (Vocab example): {list(vocab.items())[:5]}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. 生成 Skip-gram 训练数据\n",
    "# Skip-gram 的任务是：给定一个中心词，预测它周围的上下文词\n",
    "# 我们定义一个 \"窗口大小\" (window_size)，表示中心词左右各看几个词\n",
    "window_size = 2\n",
    "training_data = []\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    center_word_idx = vocab[word]\n",
    "    # 遍历窗口内的上下文词\n",
    "    for j in range(i - window_size, i + window_size + 1):\n",
    "        # 确保索引在合法范围内，并且不是中心词本身\n",
    "        if j >= 0 and j < len(words) and i != j:\n",
    "            context_word_idx = vocab[words[j]]\n",
    "            training_data.append((center_word_idx, context_word_idx))\n",
    "\n",
    "print(f\"生成训练数据对 {len(training_data)} 个\")\n",
    "print(f\"训练数据示例 (center_word_idx, context_word_idx):\")\n",
    "# 打印前5个训练样本，并显示其对应的单词\n",
    "for center_idx, context_idx in training_data[:5]:\n",
    "    print(f\"  ({ix_to_word[center_idx]}, {ix_to_word[context_idx]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078a399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 定义中心词的嵌入层 (W)\n",
    "        # 这是一个大小为 (vocab_size, embedding_dim) 的矩阵\n",
    "        # 当输入一个词的索引时，它会返回该词的嵌入向量\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 定义上下文词的嵌入层 (W')\n",
    "        # 这也是一个大小为 (vocab_size, embedding_dim) 的矩阵\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center_word_idx):\n",
    "        # 1. 获取中心词的嵌入向量\n",
    "        # center_word_idx 的 shape: (batch_size)\n",
    "        # center_embedding 的 shape: (batch_size, embedding_dim)\n",
    "        center_embedding = self.center_embeddings(center_word_idx)\n",
    "        \n",
    "        # 2. 计算中心词向量与所有上下文词向量的点积\n",
    "        # self.context_embeddings.weight 的 shape: (vocab_size, embedding_dim)\n",
    "        # 我们希望得到每个词作为上下文的得分，所以进行矩阵乘法\n",
    "        # (batch_size, embedding_dim) @ (embedding_dim, vocab_size) -> (batch_size, vocab_size)\n",
    "        scores = torch.matmul(center_embedding, self.context_embeddings.weight.t())\n",
    "        \n",
    "        # 返回的是 logits（原始得分），后续会传入 CrossEntropyLoss\n",
    "        # CrossEntropyLoss 会在内部自动计算 log_softmax\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7833f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "EMBEDDING_DIM = 10  # 词向量维度，实际应用中通常是 50, 100, 300\n",
    "LEARNING_RATE = 0.01 # 学习率\n",
    "EPOCHS = 50          # 训练轮数\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = SkipGramModel(vocab_size, EMBEDDING_DIM)\n",
    "# 损失函数：交叉熵损失。它适用于多分类问题，并且会自动处理 LogSoftmax\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# 优化器：Adam 是一个常用的、效果很好的优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1924624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 2.1238\n",
      "Epoch 20/50, Loss: 2.0294\n",
      "Epoch 30/50, Loss: 1.9959\n",
      "Epoch 40/50, Loss: 1.9798\n",
      "Epoch 50/50, Loss: 1.9700\n",
      "\n",
      "训练完成!\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for center_word_idx, context_word_idx in training_data:\n",
    "        # 将输入数据转换为 PyTorch Tensor\n",
    "        # PyTorch 的 Embedding 层和 CrossEntropyLoss 都需要 LongTensor 类型的索引\n",
    "        center_tensor = torch.LongTensor([center_word_idx])\n",
    "        context_tensor = torch.LongTensor([context_word_idx])\n",
    "        \n",
    "        # 1. 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. 前向传播，得到预测得分\n",
    "        scores = model(center_tensor)\n",
    "        \n",
    "        # 3. 计算损失\n",
    "        # scores 的 shape 是 (1, vocab_size)，context_tensor 的 shape 是 (1)\n",
    "        # CrossEntropyLoss 正好需要这种格式的输入\n",
    "        loss = loss_function(scores, context_tensor)\n",
    "        \n",
    "        # 4. 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. 更新参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # 每 10 轮打印一次损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(training_data):.4f}\")\n",
    "\n",
    "print(\"\\n训练完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bed9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 词向量相似度测试 ---\n",
      "与 'king' 最相似的词:\n",
      "  - man (相似度: 0.621)\n",
      "  - country (相似度: 0.610)\n",
      "  - leads (相似度: 0.444)\n",
      "  - people (相似度: 0.340)\n",
      "  - wise (相似度: 0.320)\n",
      "--------------------\n",
      "与 'queen' 最相似的词:\n",
      "  - people (相似度: 0.456)\n",
      "  - strong (相似度: 0.416)\n",
      "  - woman (相似度: 0.295)\n",
      "  - the (相似度: 0.267)\n",
      "  - rules (相似度: 0.227)\n",
      "--------------------\n",
      "与 'man' 最相似的词:\n",
      "  - king (相似度: 0.621)\n",
      "  - country (相似度: 0.459)\n",
      "  - woman (相似度: 0.453)\n",
      "  - be (相似度: 0.366)\n",
      "  - is (相似度: 0.352)\n"
     ]
    }
   ],
   "source": [
    "# 提取学习到的词向量 (我们通常使用中心词的嵌入作为最终的词向量)\n",
    "word_vectors = model.center_embeddings.weight.data\n",
    "\n",
    "def find_most_similar(word, top_n=5):\n",
    "    \"\"\"\n",
    "    寻找与给定单词最相似的单词\n",
    "    \"\"\"\n",
    "    if word not in vocab:\n",
    "        print(f\"'{word}' 不在词汇表中。\")\n",
    "        return\n",
    "\n",
    "    # 获取输入单词的向量\n",
    "    input_vec = word_vectors[vocab[word]].unsqueeze(0) # 增加一个维度以进行广播\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    # (1, dim) @ (dim, vocab_size) -> (1, vocab_size)\n",
    "    similarities = torch.matmul(input_vec, word_vectors.t()) / (torch.norm(input_vec) * torch.norm(word_vectors, dim=1))\n",
    "    similarities = similarities.squeeze(0) # 降维\n",
    "    \n",
    "    # 排序并获取 top_n 结果\n",
    "    # argsort 默认是升序，所以我们取最后的 n+1 个（因为最相似的是它自己）\n",
    "    top_indices = torch.argsort(similarities, descending=True)[1:top_n+1]\n",
    "    \n",
    "    print(f\"与 '{word}' 最相似的词:\")\n",
    "    for idx in top_indices:\n",
    "        sim_score = similarities[idx].item()\n",
    "        print(f\"  - {ix_to_word[idx.item()]} (相似度: {sim_score:.3f})\")\n",
    "\n",
    "# --- 验证结果 ---\n",
    "# 注意：由于我们的语料库非常小，结果可能不完美，但应该能展示出一些有趣的模式。\n",
    "# 例如，'king' 应该与 'queen', 'man' 比较相似。\n",
    "print(\"\\n--- 词向量相似度测试 ---\")\n",
    "find_most_similar('king')\n",
    "print(\"-\" * 20)\n",
    "find_most_similar('queen')\n",
    "print(\"-\" * 20)\n",
    "find_most_similar('man')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
