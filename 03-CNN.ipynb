{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a9bc77",
   "metadata": {},
   "source": [
    "# Concolutional Neural Networks\n",
    "\n",
    "**Why CNN for Image?**\n",
    "\n",
    "Use 1st layer as module to build classifiers. Use 2nd layer as module.\n",
    "Can the network be simplified by considering the properties of images?\n",
    "\n",
    "Some patterns are much smaller than the whole image. A neuron does not have to see the whole image to discover the pattern. Connecting to small region with less parameters.\n",
    "\n",
    "Subsampling the pixels will not change the object. We can subsample the pixels to make image smaller $\\Longrightarrow$ Less parameters for the network to process the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ff9de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 MNIST 数据集...\n",
      "数据加载和预处理完成。\n",
      "训练集图像形状: (60000, 1, 32, 32)\n",
      "训练集标签形状: (60000, 10)\n",
      "测试集图像形状: (10000, 1, 32, 32)\n",
      "测试集标签形状: (10000, 10)\n",
      "\n",
      "开始训练...\n",
      "    批次 100/937 - 当前损失: 0.4665\n",
      "    批次 200/937 - 当前损失: 0.2899\n",
      "    批次 300/937 - 当前损失: 0.3256\n",
      "    批次 400/937 - 当前损失: 0.2282\n",
      "    批次 500/937 - 当前损失: 0.3417\n",
      "    批次 600/937 - 当前损失: 0.1903\n",
      "    批次 700/937 - 当前损失: 0.1106\n",
      "    批次 800/937 - 当前损失: 0.1167\n",
      "    批次 900/937 - 当前损失: 0.2131\n",
      "** Epoch 1/5 - 平均损失: 0.3166 - 耗时: 113.55s **\n",
      "    批次 100/937 - 当前损失: 0.1171\n",
      "    批次 200/937 - 当前损失: 0.0636\n",
      "    批次 300/937 - 当前损失: 0.1013\n",
      "    批次 400/937 - 当前损失: 0.1675\n",
      "    批次 500/937 - 当前损失: 0.1098\n",
      "    批次 600/937 - 当前损失: 0.1250\n",
      "    批次 700/937 - 当前损失: 0.0361\n",
      "    批次 800/937 - 当前损失: 0.0477\n",
      "    批次 900/937 - 当前损失: 0.0844\n",
      "** Epoch 2/5 - 平均损失: 0.1052 - 耗时: 55.09s **\n",
      "    批次 100/937 - 当前损失: 0.0388\n",
      "    批次 200/937 - 当前损失: 0.0832\n",
      "    批次 300/937 - 当前损失: 0.0196\n",
      "    批次 400/937 - 当前损失: 0.1256\n",
      "    批次 500/937 - 当前损失: 0.0107\n",
      "    批次 600/937 - 当前损失: 0.1346\n",
      "    批次 700/937 - 当前损失: 0.0821\n",
      "    批次 800/937 - 当前损失: 0.0681\n",
      "    批次 900/937 - 当前损失: 0.0280\n",
      "** Epoch 3/5 - 平均损失: 0.0685 - 耗时: 57.33s **\n",
      "    批次 100/937 - 当前损失: 0.1334\n",
      "    批次 200/937 - 当前损失: 0.0199\n",
      "    批次 300/937 - 当前损失: 0.0207\n",
      "    批次 400/937 - 当前损失: 0.0064\n",
      "    批次 500/937 - 当前损失: 0.0219\n",
      "    批次 600/937 - 当前损失: 0.0338\n",
      "    批次 700/937 - 当前损失: 0.1900\n",
      "    批次 800/937 - 当前损失: 0.0511\n",
      "    批次 900/937 - 当前损失: 0.0072\n",
      "** Epoch 4/5 - 平均损失: 0.0535 - 耗时: 43.13s **\n",
      "    批次 100/937 - 当前损失: 0.1276\n",
      "    批次 200/937 - 当前损失: 0.0680\n",
      "    批次 300/937 - 当前损失: 0.0117\n",
      "    批次 400/937 - 当前损失: 0.0094\n",
      "    批次 500/937 - 当前损失: 0.0411\n",
      "    批次 600/937 - 当前损失: 0.0106\n",
      "    批次 700/937 - 当前损失: 0.1463\n",
      "    批次 800/937 - 当前损失: 0.0452\n",
      "    批次 900/937 - 当前损失: 0.0708\n",
      "** Epoch 5/5 - 平均损失: 0.0446 - 耗时: 44.74s **\n",
      "\n",
      "训练完成。\n",
      "\n",
      "正在评估模型...\n",
      "测试集准确率: 98.50%\n",
      "模型已保存至 lenet5_numpy.pkl\n"
     ]
    }
   ],
   "source": [
    "# LeNet-5\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle  # 用于保存和加载模型\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 工具函数 (Utility Functions)\n",
    "# =============================================================================\n",
    "\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\"计算im2col所需的索引，im2col是一种将卷积运算转换为矩阵乘法的技巧，可以极大加速计算\"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_width) % stride == 0\n",
    "    out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "    out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return (k, i, j)\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" im2col 实现：将输入图像的局部区域展平成列向量 \"\"\"\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" col2im 实现：im2col的逆操作，用于反向传播 \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def to_categorical_numpy(y, num_classes=10):\n",
    "    \"\"\"将类别向量(整数)转换为二进制(one-hot)矩阵\"\"\"\n",
    "    y_int = y.astype(int)\n",
    "    y_one_hot = np.zeros((len(y_int), num_classes))\n",
    "    y_one_hot[np.arange(len(y_int)), y_int] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 基础层定义 (Base Layer Definitions)\n",
    "# =============================================================================\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"所有层的基类\"\"\"\n",
    "    def __init__(self):\n",
    "        self.params = {}  # 存储权重 W 和偏置 b\n",
    "        self.grads = {}   # 存储 W 和 b 对应的梯度\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 激活函数层 (Activation Layers)\n",
    "# =============================================================================\n",
    "\n",
    "class Tanh(Layer):\n",
    "    \"\"\"Tanh激活函数\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        self.cache = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        \"\"\"反向传播\"\"\"\n",
    "        output = np.tanh(self.cache)\n",
    "        return grad_out * (1 - output**2)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    \"\"\"Softmax函数，通常用于输出层\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.cache = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.cache\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        \"\"\"占位符，实际计算在损失函数中完成\"\"\"\n",
    "        return grad_out\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 功能层 (Functional Layers)\n",
    "# =============================================================================\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"全连接层\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, name=\"dense\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.params['W'] = np.random.randn(input_dim, output_dim) * np.sqrt(1. / input_dim)\n",
    "        self.params['b'] = np.zeros(output_dim)\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.dot(x, self.params['W']) + self.params['b']\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        x = self.cache\n",
    "        self.grads['W'] = np.dot(x.T, grad_out)\n",
    "        self.grads['b'] = np.sum(grad_out, axis=0)\n",
    "        return np.dot(grad_out, self.params['W'].T)\n",
    "\n",
    "class Flatten(Layer):\n",
    "    \"\"\"展平层\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = x.shape\n",
    "        N = x.shape[0]\n",
    "        return x.reshape(N, -1)\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out.reshape(self.cache)\n",
    "    \n",
    "class Conv2D(Layer):\n",
    "    \"\"\"二维卷积层\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, name=\"conv\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.params['W'] = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1\n",
    "        self.params['b'] = np.zeros(out_channels)\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        KH, KW = self.kernel_size, self.kernel_size\n",
    "        out_h = (H + 2 * self.padding - KH) // self.stride + 1\n",
    "        out_w = (W + 2 * self.padding - KW) // self.stride + 1\n",
    "        self.x_col = im2col_indices(x, KH, KW, self.padding, self.stride)\n",
    "        self.W_col = self.params['W'].reshape(self.out_channels, -1)\n",
    "        out = self.W_col @ self.x_col + self.params['b'].reshape(-1, 1)\n",
    "        out = out.reshape(self.out_channels, out_h, out_w, N)\n",
    "        out = out.transpose(3, 0, 1, 2)\n",
    "        self.cache = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        N, C, H, W = self.cache.shape\n",
    "        self.grads['b'] = np.sum(grad_out, axis=(0, 2, 3))\n",
    "        grad_out_reshaped = grad_out.transpose(1, 2, 3, 0).reshape(self.out_channels, -1)\n",
    "        dW_col = grad_out_reshaped @ self.x_col.T\n",
    "        self.grads['W'] = dW_col.reshape(self.params['W'].shape)\n",
    "        dx_col = self.W_col.T @ grad_out_reshaped\n",
    "        dx = col2im_indices(dx_col, self.cache.shape, self.kernel_size, self.kernel_size, self.padding, self.stride)\n",
    "        return dx\n",
    "\n",
    "class AvgPool2D(Layer):\n",
    "    \"\"\"二维平均池化层\"\"\"\n",
    "    def __init__(self, pool_size, stride, name=\"avg_pool\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        N, C, H, W = x.shape\n",
    "        pool_h, pool_w = self.pool_size, self.pool_size\n",
    "        out_h = (H - pool_h) // self.stride + 1\n",
    "        out_w = (W - pool_w) // self.stride + 1\n",
    "        out = np.zeros((N, C, out_h, out_w))\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                h_start, h_end = i * self.stride, i * self.stride + pool_h\n",
    "                w_start, w_end = j * self.stride, j * self.stride + pool_w\n",
    "                window = x[:, :, h_start:h_end, w_start:w_end]\n",
    "                out[:, :, i, j] = np.mean(window, axis=(2, 3))\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        x = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        pool_h, pool_w = self.pool_size, self.pool_size\n",
    "        out_h, out_w = grad_out.shape[2], grad_out.shape[3]\n",
    "        dx = np.zeros_like(x)\n",
    "        pool_area = pool_h * pool_w\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                h_start, h_end = i * self.stride, i * self.stride + pool_h\n",
    "                w_start, w_end = j * self.stride, j * self.stride + pool_w\n",
    "                grad = grad_out[:, :, i, j][:, :, np.newaxis, np.newaxis] / pool_area\n",
    "                dx[:, :, h_start:h_end, w_start:w_end] += grad\n",
    "        return dx\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 损失函数和优化器 (Loss Function and Optimizer)\n",
    "# =============================================================================\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    \"\"\"交叉熵损失函数\"\"\"\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred, self.y_true = y_pred, y_true\n",
    "        m = y_true.shape[0]\n",
    "        epsilon = 1e-12\n",
    "        loss = -np.sum(y_true * np.log(y_pred + epsilon)) / m\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        m = self.y_true.shape[0]\n",
    "        grad = (self.y_pred - self.y_true) / m\n",
    "        return grad\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"随机梯度下降优化器\"\"\"\n",
    "    def __init__(self, layers, learning_rate=0.01):\n",
    "        self.layers = layers\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                for key in layer.params:\n",
    "                    layer.params[key] -= self.lr * layer.grads[key]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'grads'):\n",
    "                for key in layer.grads:\n",
    "                    layer.grads[key].fill(0)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. LeNet-5 模型定义\n",
    "# =============================================================================\n",
    "\n",
    "class LeNet5:\n",
    "    \"\"\"LeNet-5 模型\"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            Conv2D(in_channels=1, out_channels=6, kernel_size=5, padding=0, name=\"conv1\"),\n",
    "            Tanh(),\n",
    "            AvgPool2D(pool_size=2, stride=2, name=\"pool1\"),\n",
    "            Conv2D(in_channels=6, out_channels=16, kernel_size=5, padding=0, name=\"conv2\"),\n",
    "            Tanh(),\n",
    "            AvgPool2D(pool_size=2, stride=2, name=\"pool2\"),\n",
    "            Flatten(),\n",
    "            Dense(input_dim=5*5*16, output_dim=120, name=\"dense1\"),\n",
    "            Tanh(),\n",
    "            Dense(input_dim=120, output_dim=84, name=\"dense2\"),\n",
    "            Tanh(),\n",
    "            Dense(input_dim=84, output_dim=10, name=\"dense3\"),\n",
    "            Softmax()\n",
    "        ]\n",
    "        \n",
    "        # ******** 代码修正处 ********\n",
    "        # 修正筛选条件，只选择params字典不为空的层\n",
    "        self.param_layers = [l for l in self.layers if hasattr(l, 'params') and l.params]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "    \n",
    "    def save_model(self, file_path):\n",
    "        params = {}\n",
    "        for layer in self.param_layers: # 现在这个列表是正确的\n",
    "            params[layer.name] = layer.params\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        print(f\"模型已保存至 {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for layer in self.param_layers:\n",
    "            if layer.name in params:\n",
    "                layer.params = params[layer.name]\n",
    "        print(f\"模型已从 {file_path} 加载\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. 训练过程 (Training Process)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"正在加载 MNIST 数据集...\")\n",
    "    try:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "    except ImportError:\n",
    "        print(\"请安装 scikit-learn 以加载数据集: pip install scikit-learn\")\n",
    "        exit()\n",
    "\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "    X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "    \n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    \n",
    "    x_train = x_train.reshape((-1, 28, 28))\n",
    "    x_test = x_test.reshape((-1, 28, 28))\n",
    "\n",
    "    x_train = np.expand_dims(x_train, axis=1)\n",
    "    x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "    x_train = np.pad(x_train, ((0,0), (0,0), (2,2), (2,2)), 'constant')\n",
    "    x_test = np.pad(x_test, ((0,0), (0,0), (2,2), (2,2)), 'constant')\n",
    "    \n",
    "    y_train_one_hot = to_categorical_numpy(y_train, 10)\n",
    "    y_test_one_hot = to_categorical_numpy(y_test, 10)\n",
    "    \n",
    "    print(\"数据加载和预处理完成。\")\n",
    "    print(\"训练集图像形状:\", x_train.shape)\n",
    "    print(\"训练集标签形状:\", y_train_one_hot.shape)\n",
    "    print(\"测试集图像形状:\", x_test.shape)\n",
    "    print(\"测试集标签形状:\", y_test_one_hot.shape)\n",
    "\n",
    "    model = LeNet5()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.param_layers, learning_rate=0.1)\n",
    "\n",
    "    epochs = 5\n",
    "    batch_size = 64\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    \n",
    "    print(\"\\n开始训练...\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = y_train_one_hot[permutation]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx, end_idx = i * batch_size, (i + 1) * batch_size\n",
    "            x_batch = x_train_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(y_pred, y_batch)\n",
    "            epoch_loss += loss\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"    批次 {i+1}/{num_batches} - 当前损失: {loss:.4f}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"** Epoch {epoch+1}/{epochs} - 平均损失: {avg_loss:.4f} - 耗时: {end_time - start_time:.2f}s **\")\n",
    "\n",
    "    print(\"\\n训练完成。\")\n",
    "\n",
    "    print(\"\\n正在评估模型...\")\n",
    "    y_test_pred_probs = model.forward(x_test)\n",
    "    y_test_pred_labels = np.argmax(y_test_pred_probs, axis=1)\n",
    "    \n",
    "    accuracy = np.mean(y_test_pred_labels == y_test)\n",
    "    print(f\"测试集准确率: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    model.save_model('lenet5_numpy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1137530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR-10 数据集...\n",
      "!!! 警告: 为快速演示，仅使用一小部分数据进行训练。!!!\n",
      "数据加载和预处理完成。\n",
      "训练集图像形状: (1280, 3, 32, 32)\n",
      "训练集标签形状: (1280, 10)\n",
      "\n",
      "开始训练... (Epochs=2, Batch Size=32)\n",
      "    Epoch 1, 批次 10/40 - 当前损失: 2.4539\n",
      "    Epoch 1, 批次 20/40 - 当前损失: 2.3031\n",
      "    Epoch 1, 批次 30/40 - 当前损失: 2.2378\n",
      "    Epoch 1, 批次 40/40 - 当前损失: 2.2460\n",
      "** Epoch 1/2 - 平均损失: 2.3283 - 耗时: 10.26s **\n",
      "    Epoch 2, 批次 10/40 - 当前损失: 2.2842\n",
      "    Epoch 2, 批次 20/40 - 当前损失: 2.3652\n",
      "    Epoch 2, 批次 30/40 - 当前损失: 2.2850\n",
      "    Epoch 2, 批次 40/40 - 当前损失: 2.3594\n",
      "** Epoch 2/2 - 平均损失: 2.3033 - 耗时: 11.56s **\n",
      "\n",
      "训练完成。\n",
      "\n",
      "正在评估模型...\n",
      "在500个测试样本上的准确率: 17.00%\n",
      "模型已保存至 alexnet_numpy.pkl\n"
     ]
    }
   ],
   "source": [
    "# AlexNet\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle  # 用于保存和加载模型\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 工具函数 (Utility Functions)\n",
    "# =============================================================================\n",
    "# 这部分与LeNet-5实现中的工具函数相同，用于加速卷积\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "    out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "    return (k, i, j)\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "# 新增：自己实现的 to_categorical 函数\n",
    "def to_categorical_numpy(y, num_classes=10):\n",
    "    \"\"\"将类别向量(整数)转换为二进制(one-hot)矩阵\"\"\"\n",
    "    y_int = y.astype(int)\n",
    "    y_one_hot = np.zeros((len(y_int), num_classes))\n",
    "    y_one_hot[np.arange(len(y_int)), y_int] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 基础层及新层定义 (Base & New Layer Definitions)\n",
    "# =============================================================================\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"所有层的基类\"\"\"\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self.mode = 'train' # 模式：'train' 或 'test'\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# --- 激活函数 ---\n",
    "class ReLU(Layer):\n",
    "    \"\"\"ReLU激活函数\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        x = self.cache\n",
    "        return grad_out * (x > 0)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    \"\"\"Softmax函数\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.cache = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.cache\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out\n",
    "\n",
    "# --- 功能层 ---\n",
    "class Dense(Layer):\n",
    "    \"\"\"全连接层\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, name=\"dense\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.params['W'] = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim) # He 初始化\n",
    "        self.params['b'] = np.zeros(output_dim)\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.dot(x, self.params['W']) + self.params['b']\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        x = self.cache\n",
    "        self.grads['W'] = np.dot(x.T, grad_out)\n",
    "        self.grads['b'] = np.sum(grad_out, axis=0)\n",
    "        return np.dot(grad_out, self.params['W'].T)\n",
    "\n",
    "class Flatten(Layer):\n",
    "    \"\"\"展平层\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out.reshape(self.cache)\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    \"\"\"二维卷积层\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, name=\"conv\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.params['W'] = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * np.sqrt(2. / (in_channels * kernel_size * kernel_size)) # He 初始化\n",
    "        self.params['b'] = np.zeros(out_channels)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.cache = None\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_w = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        self.x_col = im2col_indices(x, self.kernel_size, self.kernel_size, self.padding, self.stride)\n",
    "        self.W_col = self.params['W'].reshape(self.out_channels, -1)\n",
    "        out = self.W_col @ self.x_col + self.params['b'].reshape(-1, 1)\n",
    "        out = out.reshape(self.out_channels, out_h, out_w, N).transpose(3, 0, 1, 2)\n",
    "        self.cache = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        self.grads['b'] = np.sum(grad_out, axis=(0, 2, 3))\n",
    "        grad_out_reshaped = grad_out.transpose(1, 2, 3, 0).reshape(self.out_channels, -1)\n",
    "        self.grads['W'] = (grad_out_reshaped @ self.x_col.T).reshape(self.params['W'].shape)\n",
    "        dx_col = self.W_col.T @ grad_out_reshaped\n",
    "        dx = col2im_indices(dx_col, self.cache.shape, self.kernel_size, self.kernel_size, self.padding, self.stride)\n",
    "        return dx\n",
    "    \n",
    "class MaxPool2D(Layer):\n",
    "    \"\"\"二维最大池化层\"\"\"\n",
    "    def __init__(self, pool_size, stride, name=\"max_pool\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        pool_h, pool_w = self.pool_size, self.pool_size\n",
    "        out_h = (H - pool_h) // self.stride + 1\n",
    "        out_w = (W - pool_w) // self.stride + 1\n",
    "        x_reshaped = x.reshape(N * C, 1, H, W)\n",
    "        self.x_col = im2col_indices(x_reshaped, pool_h, pool_w, padding=0, stride=self.stride)\n",
    "        max_idx = np.argmax(self.x_col, axis=0)\n",
    "        out = self.x_col[max_idx, np.arange(self.x_col.shape[1])]\n",
    "        out = out.reshape(out_h, out_w, N, C).transpose(2, 3, 0, 1)\n",
    "        self.cache = (x, max_idx)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        x, max_idx = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        pool_h, pool_w = self.pool_size, self.pool_size\n",
    "        grad_out_flat = grad_out.transpose(2, 3, 0, 1).ravel()\n",
    "        dx_col = np.zeros_like(self.x_col)\n",
    "        dx_col[max_idx, np.arange(self.x_col.shape[1])] = grad_out_flat\n",
    "        dx = col2im_indices(dx_col, (N * C, 1, H, W), pool_h, pool_w, padding=0, stride=self.stride)\n",
    "        dx = dx.reshape(x.shape)\n",
    "        return dx\n",
    "\n",
    "class Dropout(Layer):\n",
    "    \"\"\"Dropout层\"\"\"\n",
    "    def __init__(self, p=0.5, name=\"dropout\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.p = p # p是保留神经元的概率\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 'train':\n",
    "            self.mask = (np.random.rand(*x.shape) < self.p) / self.p\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out * self.mask\n",
    "\n",
    "class LocalResponseNorm(Layer):\n",
    "    \"\"\"局部响应归一化层\"\"\"\n",
    "    def __init__(self, n=5, alpha=1e-4, beta=0.75, k=2, name=\"lrn\"):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        self.name = name\n",
    "        self.cache = None\n",
    "        self.norm_factor_cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        N, C, H, W = x.shape\n",
    "        half_n = self.n // 2\n",
    "        sq_sum = np.zeros_like(x)\n",
    "        x_sq = x**2\n",
    "        for i in range(C):\n",
    "            start = max(0, i - half_n)\n",
    "            end = min(C, i + half_n + 1)\n",
    "            sq_sum[:, i, :, :] = np.sum(x_sq[:, start:end, :, :], axis=1)\n",
    "        norm_factor = (self.k + self.alpha * sq_sum)**self.beta\n",
    "        self.norm_factor_cache = norm_factor\n",
    "        return x / norm_factor\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        # LRN的反向传播非常复杂，使用简化版本\n",
    "        return grad_out\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 损失函数和优化器 (Loss & Optimizer)\n",
    "# =============================================================================\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.y_pred, self.y_true = None, None\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred, self.y_true = y_pred, y_true\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-12)) / m\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        m = self.y_true.shape[0]\n",
    "        grad = (self.y_pred - self.y_true) / m\n",
    "        return grad\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, layers, learning_rate=0.01):\n",
    "        self.layers = layers\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params') and layer.params:\n",
    "                for key in layer.params:\n",
    "                    layer.params[key] -= self.lr * layer.grads[key]\n",
    "\n",
    "# =============================================================================\n",
    "# 3. AlexNet 模型定义 (AlexNet-style for CIFAR-10)\n",
    "# =============================================================================\n",
    "\n",
    "class AlexNet:\n",
    "    \"\"\"适用于CIFAR-10的AlexNet风格模型\"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            Conv2D(in_channels=3, out_channels=32, kernel_size=3, padding=1, name=\"conv1\"), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2, name=\"pool1\"), LocalResponseNorm(name=\"lrn1\"),\n",
    "            Conv2D(in_channels=32, out_channels=64, kernel_size=3, padding=1, name=\"conv2\"), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2, name=\"pool2\"), LocalResponseNorm(name=\"lrn2\"),\n",
    "            Conv2D(in_channels=64, out_channels=128, kernel_size=3, padding=1, name=\"conv3\"), ReLU(),\n",
    "            Conv2D(in_channels=128, out_channels=128, kernel_size=3, padding=1, name=\"conv4\"), ReLU(),\n",
    "            Conv2D(in_channels=128, out_channels=64, kernel_size=3, padding=1, name=\"conv5\"), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2, name=\"pool3\"),\n",
    "            Flatten(),\n",
    "            Dense(input_dim=4*4*64, output_dim=512, name=\"dense1\"), ReLU(), Dropout(p=0.5, name=\"dropout1\"),\n",
    "            Dense(input_dim=512, output_dim=512, name=\"dense2\"), ReLU(), Dropout(p=0.5, name=\"dropout2\"),\n",
    "            Dense(input_dim=512, output_dim=10, name=\"dense3\"), Softmax()\n",
    "        ]\n",
    "        self.param_layers = [l for l in self.layers if hasattr(l, 'params') and l.params]\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        for layer in self.layers:\n",
    "            layer.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        params = {layer.name: layer.params for layer in self.param_layers}\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        print(f\"模型已保存至 {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for layer in self.param_layers:\n",
    "            if layer.name in params:\n",
    "                layer.params = params[layer.name]\n",
    "        print(f\"模型已从 {file_path} 加载\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 训练过程 (Training Process)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- 数据加载和预处理 (使用Scikit-learn) ---\n",
    "    print(\"正在加载 CIFAR-10 数据集...\")\n",
    "    try:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "    except ImportError:\n",
    "        print(\"请安装 scikit-learn 以加载数据集: pip install scikit-learn\")\n",
    "        exit()\n",
    "\n",
    "    # 从OpenML获取数据，这可能需要一些时间，但它会自动缓存数据\n",
    "    cifar = fetch_openml('CIFAR_10', version=1, as_frame=False, parser='auto')\n",
    "    X, y = cifar.data, cifar.target\n",
    "\n",
    "    # 1. 归一化和类型转换\n",
    "    X = X.astype('float32') / 255.0\n",
    "    y = y.astype(np.uint8)\n",
    "\n",
    "    # 2. Reshape: 数据被展平为 (70000, 3072)，需要reshape为 (70000, 3, 32, 32)\n",
    "    # 我们的层期望 (N, C, H, W) 格式\n",
    "    X = X.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    # 3. 手动划分训练集和测试集 (CIFAR-10标准划分是50k/10k/10k)\n",
    "    # fetch_openml 合并了训练和测试集，总共70000个。我们用前50000训练，后10000测试。\n",
    "    x_train, x_test = X[:50000], X[50000:60000]\n",
    "    y_train, y_test = y[:50000], y[50000:60000]\n",
    "\n",
    "    # 4. One-Hot 编码标签\n",
    "    y_train_one_hot = to_categorical_numpy(y_train, 10)\n",
    "    y_test_one_hot = to_categorical_numpy(y_test, 10)\n",
    "    \n",
    "    # 为了让代码能快速跑完看到结果，我们只使用一小部分数据\n",
    "    # 如果您想在完整数据集上训练，请注释掉下面两行\n",
    "    print(\"!!! 警告: 为快速演示，仅使用一小部分数据进行训练。!!!\")\n",
    "    x_train, y_train_one_hot = x_train[:1280], y_train_one_hot[:1280]\n",
    "\n",
    "    print(\"数据加载和预处理完成。\")\n",
    "    print(\"训练集图像形状:\", x_train.shape)\n",
    "    print(\"训练集标签形状:\", y_train_one_hot.shape)\n",
    "\n",
    "    # --- 模型初始化 ---\n",
    "    model = AlexNet()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.param_layers, learning_rate=0.01)\n",
    "\n",
    "    # --- 训练参数 ---\n",
    "    epochs = 2\n",
    "    batch_size = 32\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    \n",
    "    print(f\"\\n开始训练... (Epochs={epochs}, Batch Size={batch_size})\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        model.set_mode('train')\n",
    "        \n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = y_train_one_hot[permutation]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            x_batch = x_train_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            y_pred = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(y_pred, y_batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"    Epoch {epoch+1}, 批次 {i+1}/{num_batches} - 当前损失: {loss:.4f}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"** Epoch {epoch+1}/{epochs} - 平均损失: {avg_loss:.4f} - 耗时: {end_time - start_time:.2f}s **\")\n",
    "\n",
    "    print(\"\\n训练完成。\")\n",
    "\n",
    "    # --- 模型评估 ---\n",
    "    print(\"\\n正在评估模型...\")\n",
    "    model.set_mode('test')\n",
    "    \n",
    "    x_test_subset = x_test[:500]\n",
    "    y_test_subset_labels = y_test[:500]\n",
    "\n",
    "    y_test_pred_probs = model.forward(x_test_subset)\n",
    "    y_test_pred_labels = np.argmax(y_test_pred_probs, axis=1)\n",
    "    \n",
    "    accuracy = np.mean(y_test_pred_labels == y_test_subset_labels)\n",
    "    print(f\"在500个测试样本上的准确率: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    model.save_model('alexnet_numpy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362dab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR-10 数据集...\n",
      "!!! 警告: 为快速演示，仅使用 512 个样本进行训练。!!!\n",
      "数据加载和预处理完成。\n",
      "训练集图像形状: (512, 3, 32, 32)\n",
      "训练集标签形状: (512, 10)\n",
      "\n",
      "开始训练... (Epochs=1, Batch Size=16)\n",
      "    Epoch 1, 批次 1/32 - 当前损失: 2.4863\n",
      "    Epoch 1, 批次 2/32 - 当前损失: 2.6632\n",
      "    Epoch 1, 批次 3/32 - 当前损失: 2.4926\n",
      "    Epoch 1, 批次 4/32 - 当前损失: 2.3549\n",
      "    Epoch 1, 批次 5/32 - 当前损失: 2.2613\n",
      "    Epoch 1, 批次 6/32 - 当前损失: 2.4613\n",
      "    Epoch 1, 批次 7/32 - 当前损失: 2.2503\n",
      "    Epoch 1, 批次 8/32 - 当前损失: 2.3767\n",
      "    Epoch 1, 批次 9/32 - 当前损失: 2.3326\n",
      "    Epoch 1, 批次 10/32 - 当前损失: 2.3408\n",
      "    Epoch 1, 批次 11/32 - 当前损失: 2.2883\n",
      "    Epoch 1, 批次 12/32 - 当前损失: 2.3356\n",
      "    Epoch 1, 批次 13/32 - 当前损失: 2.3310\n",
      "    Epoch 1, 批次 14/32 - 当前损失: 2.2095\n",
      "    Epoch 1, 批次 15/32 - 当前损失: 2.2351\n",
      "    Epoch 1, 批次 16/32 - 当前损失: 2.2691\n",
      "    Epoch 1, 批次 17/32 - 当前损失: 2.4499\n",
      "    Epoch 1, 批次 18/32 - 当前损失: 2.2512\n",
      "    Epoch 1, 批次 19/32 - 当前损失: 2.2001\n",
      "    Epoch 1, 批次 20/32 - 当前损失: 2.4192\n",
      "    Epoch 1, 批次 21/32 - 当前损失: 2.2683\n",
      "    Epoch 1, 批次 22/32 - 当前损失: 2.2919\n",
      "    Epoch 1, 批次 23/32 - 当前损失: 2.2672\n",
      "    Epoch 1, 批次 24/32 - 当前损失: 2.4168\n",
      "    Epoch 1, 批次 25/32 - 当前损失: 2.0934\n",
      "    Epoch 1, 批次 26/32 - 当前损失: 2.1332\n",
      "    Epoch 1, 批次 27/32 - 当前损失: 2.3217\n",
      "    Epoch 1, 批次 28/32 - 当前损失: 2.1897\n",
      "    Epoch 1, 批次 29/32 - 当前损失: 2.1516\n",
      "    Epoch 1, 批次 30/32 - 当前损失: 2.2354\n",
      "    Epoch 1, 批次 31/32 - 当前损失: 2.3212\n",
      "    Epoch 1, 批次 32/32 - 当前损失: 2.2180\n",
      "** Epoch 1/1 - 平均损失: 2.3099 - 耗时: 10.54s **\n",
      "\n",
      "训练完成。\n",
      "\n",
      "正在评估模型...\n",
      "在 200 个测试样本上的准确率: 14.00%\n",
      "模型已保存至 vgg11_numpy.pkl\n"
     ]
    }
   ],
   "source": [
    "# VGGNet\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle  # 用于保存和加载模型\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 工具函数 (Utility Functions)\n",
    "# =============================================================================\n",
    "# 这部分与之前的实现完全相同，用于加速卷积和进行one-hot编码\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "    out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "    return (k, i, j)\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def to_categorical_numpy(y, num_classes=10):\n",
    "    \"\"\"将类别向量(整数)转换为二进制(one-hot)矩阵\"\"\"\n",
    "    y_int = y.astype(int)\n",
    "    y_one_hot = np.zeros((len(y_int), num_classes))\n",
    "    y_one_hot[np.arange(len(y_int)), y_int] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 基础层定义 (Layer Definitions)\n",
    "# =============================================================================\n",
    "# 这部分与AlexNet的实现基本相同，是构建任何CNN的基础\n",
    "class Layer:\n",
    "    \"\"\"所有层的基类\"\"\"\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self.mode = 'train'\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.maximum(0, x)\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out * (self.cache > 0)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.cache = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.cache\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim, name=\"dense\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.params['W'] = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim) # He 初始化\n",
    "        self.params['b'] = np.zeros(output_dim)\n",
    "        self.cache = None\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.dot(x, self.params['W']) + self.params['b']\n",
    "    def backward(self, grad_out):\n",
    "        x = self.cache\n",
    "        self.grads['W'] = np.dot(x.T, grad_out)\n",
    "        self.grads['b'] = np.sum(grad_out, axis=0)\n",
    "        return np.dot(grad_out, self.params['W'].T)\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = None\n",
    "    def forward(self, x):\n",
    "        self.cache = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out.reshape(self.cache)\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, name=\"conv\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.params['W'] = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * np.sqrt(2. / (in_channels * kernel_size * kernel_size)) # He 初始化\n",
    "        self.params['b'] = np.zeros(out_channels)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.cache = None\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_w = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        self.x_col = im2col_indices(x, self.kernel_size, self.kernel_size, self.padding, self.stride)\n",
    "        self.W_col = self.params['W'].reshape(self.out_channels, -1)\n",
    "        out = self.W_col @ self.x_col + self.params['b'].reshape(-1, 1)\n",
    "        out = out.reshape(self.out_channels, out_h, out_w, N).transpose(3, 0, 1, 2)\n",
    "        self.cache = x\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        self.grads['b'] = np.sum(grad_out, axis=(0, 2, 3))\n",
    "        grad_out_reshaped = grad_out.transpose(1, 2, 3, 0).reshape(self.out_channels, -1)\n",
    "        self.grads['W'] = (grad_out_reshaped @ self.x_col.T).reshape(self.params['W'].shape)\n",
    "        dx_col = self.W_col.T @ grad_out_reshaped\n",
    "        dx = col2im_indices(dx_col, self.cache.shape, self.kernel_size, self.kernel_size, self.padding, self.stride)\n",
    "        return dx\n",
    "    \n",
    "class MaxPool2D(Layer):\n",
    "    def __init__(self, pool_size, stride, name=\"max_pool\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.cache = None\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        pool_h, pool_w = self.pool_size, self.pool_size\n",
    "        out_h = (H - pool_h) // self.stride + 1\n",
    "        out_w = (W - pool_w) // self.stride + 1\n",
    "        x_reshaped = x.reshape(N * C, 1, H, W)\n",
    "        self.x_col = im2col_indices(x_reshaped, pool_h, pool_w, padding=0, stride=self.stride)\n",
    "        max_idx = np.argmax(self.x_col, axis=0)\n",
    "        out = self.x_col[max_idx, np.arange(self.x_col.shape[1])]\n",
    "        out = out.reshape(out_h, out_w, N, C).transpose(2, 3, 0, 1)\n",
    "        self.cache = (x, max_idx)\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        x, max_idx = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        pool_h, pool_w = self.pool_size, self.pool_size\n",
    "        grad_out_flat = grad_out.transpose(2, 3, 0, 1).ravel()\n",
    "        dx_col = np.zeros_like(self.x_col)\n",
    "        dx_col[max_idx, np.arange(self.x_col.shape[1])] = grad_out_flat\n",
    "        dx = col2im_indices(dx_col, (N * C, 1, H, W), pool_h, pool_w, padding=0, stride=self.stride)\n",
    "        dx = dx.reshape(x.shape)\n",
    "        return dx\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 损失函数和优化器 (Loss & Optimizer)\n",
    "# =============================================================================\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.y_pred, self.y_true = None, None\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred, self.y_true = y_pred, y_true\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-12)) / m\n",
    "        return loss\n",
    "    def backward(self):\n",
    "        m = self.y_true.shape[0]\n",
    "        grad = (self.y_pred - self.y_true) / m\n",
    "        return grad\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, layers, learning_rate=0.01):\n",
    "        self.layers = layers\n",
    "        self.lr = learning_rate\n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params') and layer.params:\n",
    "                for key in layer.params:\n",
    "                    layer.params[key] -= self.lr * layer.grads[key]\n",
    "\n",
    "# =============================================================================\n",
    "# 3. VGG-11 模型定义\n",
    "# =============================================================================\n",
    "\n",
    "class VGG11:\n",
    "    \"\"\"适用于CIFAR-10的VGG-11模型\"\"\"\n",
    "    def __init__(self):\n",
    "        # VGG-11 包含 8 个卷积层和 3 个全连接层\n",
    "        # 所有卷积层都使用 3x3 的核，padding=1 来保持尺寸\n",
    "        # 每个卷积块后接一个 2x2 的最大池化层\n",
    "        \n",
    "        # 输入: (N, 3, 32, 32)\n",
    "        self.layers = [\n",
    "            # Block 1: 32x32 -> 16x16\n",
    "            Conv2D(in_channels=3, out_channels=64, kernel_size=3, padding=1, name=\"conv1\"), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2, name=\"pool1\"),\n",
    "            \n",
    "            # Block 2: 16x16 -> 8x8\n",
    "            Conv2D(in_channels=64, out_channels=128, kernel_size=3, padding=1, name=\"conv2\"), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2, name=\"pool2\"),\n",
    "\n",
    "            # Block 3: 8x8 -> 4x4\n",
    "            Conv2D(in_channels=128, out_channels=256, kernel_size=3, padding=1, name=\"conv3_1\"), ReLU(),\n",
    "            Conv2D(in_channels=256, out_channels=256, kernel_size=3, padding=1, name=\"conv3_2\"), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2, name=\"pool3\"),\n",
    "\n",
    "            # Block 4: 4x4 -> 2x2\n",
    "            Conv2D(in_channels=256, out_channels=512, kernel_size=3, padding=1, name=\"conv4_1\"), ReLU(),\n",
    "            Conv2D(in_channels=512, out_channels=512, kernel_size=3, padding=1, name=\"conv4_2\"), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2, name=\"pool4\"),\n",
    "\n",
    "            # Block 5: 2x2 -> 1x1\n",
    "            Conv2D(in_channels=512, out_channels=512, kernel_size=3, padding=1, name=\"conv5_1\"), ReLU(),\n",
    "            Conv2D(in_channels=512, out_channels=512, kernel_size=3, padding=1, name=\"conv5_2\"), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2, name=\"pool5\"),\n",
    "\n",
    "            # 展平层: 最终特征图尺寸为 1x1x512\n",
    "            Flatten(),\n",
    "            \n",
    "            # 全连接层 (分类器)\n",
    "            # 原始VGG使用4096个神经元，这里简化为512以适应小数据集和加速计算\n",
    "            Dense(input_dim=1*1*512, output_dim=512, name=\"dense1\"), ReLU(),\n",
    "            # 这里可以加Dropout层，但为保持VGG的核心结构简洁，暂时省略\n",
    "            # Dense(input_dim=512, output_dim=512, name=\"dense2\"), ReLU(),\n",
    "            Dense(input_dim=512, output_dim=10, name=\"dense_out\"),\n",
    "            Softmax()\n",
    "        ]\n",
    "        self.param_layers = [l for l in self.layers if hasattr(l, 'params') and l.params]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        params = {layer.name: layer.params for layer in self.param_layers}\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        print(f\"模型已保存至 {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for layer in self.param_layers:\n",
    "            if layer.name in params:\n",
    "                layer.params = params[layer.name]\n",
    "        print(f\"模型已从 {file_path} 加载\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 训练过程 (Training Process)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- 数据加载和预处理 (使用Scikit-learn) ---\n",
    "    print(\"正在加载 CIFAR-10 数据集...\")\n",
    "    try:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "    except ImportError:\n",
    "        print(\"请安装 scikit-learn 以加载数据集: pip install scikit-learn\")\n",
    "        exit()\n",
    "\n",
    "    cifar = fetch_openml('CIFAR_10', version=1, as_frame=False, parser='auto')\n",
    "    X, y = cifar.data, cifar.target\n",
    "\n",
    "    X = X.astype('float32') / 255.0\n",
    "    y = y.astype(np.uint8)\n",
    "    X = X.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    x_train, x_test = X[:50000], X[50000:60000]\n",
    "    y_train, y_test = y[:50000], y[50000:60000]\n",
    "\n",
    "    y_train_one_hot = to_categorical_numpy(y_train, 10)\n",
    "    \n",
    "    # !!! 警告: VGG非常大，在完整数据集上训练极慢 !!!\n",
    "    # !!! 为了让代码能快速跑完看到结果，我们只使用非常小的一部分数据 !!!\n",
    "    num_train_samples = 512 # 可以调整，但越大越慢\n",
    "    print(f\"!!! 警告: 为快速演示，仅使用 {num_train_samples} 个样本进行训练。!!!\")\n",
    "    x_train, y_train_one_hot = x_train[:num_train_samples], y_train_one_hot[:num_train_samples]\n",
    "\n",
    "    print(\"数据加载和预处理完成。\")\n",
    "    print(\"训练集图像形状:\", x_train.shape)\n",
    "    print(\"训练集标签形状:\", y_train_one_hot.shape)\n",
    "\n",
    "    # --- 模型初始化 ---\n",
    "    model = VGG11()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.param_layers, learning_rate=0.001) # VGG通常需要较小的学习率\n",
    "\n",
    "    # --- 训练参数 ---\n",
    "    epochs = 1 # 增加 epoch 会显著增加等待时间\n",
    "    batch_size = 16 # 使用小批量以减少内存占用\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    \n",
    "    print(f\"\\n开始训练... (Epochs={epochs}, Batch Size={batch_size})\")\n",
    "    # --- 训练循环 ---\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = y_train_one_hot[permutation]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            x_batch = x_train_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            y_pred = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(y_pred, y_batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"    Epoch {epoch+1}, 批次 {i+1}/{num_batches} - 当前损失: {loss:.4f}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"** Epoch {epoch+1}/{epochs} - 平均损失: {avg_loss:.4f} - 耗时: {end_time - start_time:.2f}s **\")\n",
    "\n",
    "    print(\"\\n训练完成。\")\n",
    "\n",
    "    # --- 模型评估 ---\n",
    "    print(\"\\n正在评估模型...\")\n",
    "    # 为节省时间，只在测试集的一个子集上评估\n",
    "    num_test_samples = 200\n",
    "    x_test_subset = x_test[:num_test_samples]\n",
    "    y_test_subset_labels = y_test[:num_test_samples]\n",
    "\n",
    "    y_test_pred_probs = model.forward(x_test_subset)\n",
    "    y_test_pred_labels = np.argmax(y_test_pred_probs, axis=1)\n",
    "    \n",
    "    accuracy = np.mean(y_test_pred_labels == y_test_subset_labels)\n",
    "    print(f\"在 {num_test_samples} 个测试样本上的准确率: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    model.save_model('vgg11_numpy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d5bdc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR-10 数据集...\n",
      "!!! 警告: 为快速演示，仅使用 64 个样本进行训练。!!!\n",
      "数据加载和预处理完成。\n",
      "\n",
      "开始训练... (Epochs=1, Batch Size=8)\n",
      "    Epoch 1, 批次 1/8 - 损失: 3.1695 - 耗时: 0.24s\n",
      "    Epoch 1, 批次 2/8 - 损失: 3.2050 - 耗时: 0.24s\n",
      "    Epoch 1, 批次 3/8 - 损失: 2.4973 - 耗时: 0.23s\n",
      "    Epoch 1, 批次 4/8 - 损失: 2.8459 - 耗时: 0.25s\n",
      "    Epoch 1, 批次 5/8 - 损失: 3.1532 - 耗时: 0.23s\n",
      "    Epoch 1, 批次 6/8 - 损失: 3.4507 - 耗时: 0.25s\n",
      "    Epoch 1, 批次 7/8 - 损失: 2.9522 - 耗时: 0.24s\n",
      "    Epoch 1, 批次 8/8 - 损失: 3.2886 - 耗时: 0.25s\n",
      "** Epoch 1/1 - 平均损失: 3.0703 - 总耗时: 1.94s **\n",
      "\n",
      "训练完成。\n",
      "\n",
      "正在评估模型...\n",
      "在 50 个测试样本上的准确率: 8.00%\n"
     ]
    }
   ],
   "source": [
    "# GoogLeNet(Inception)\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 工具函数 (Utility Functions)\n",
    "# =============================================================================\n",
    "# (这部分代码无变化，保持不变)\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "    out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "    i0 = np.repeat(np.arange(field_height), field_width); i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "    return (k, i, j)\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0: return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def to_categorical_numpy(y, num_classes=10):\n",
    "    y_int = y.astype(int)\n",
    "    y_one_hot = np.zeros((len(y_int), num_classes))\n",
    "    y_one_hot[np.arange(len(y_int)), y_int] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 基础层定义 (Layer Definitions)\n",
    "# =============================================================================\n",
    "class Layer:\n",
    "    def __init__(self): self.params, self.grads = {}, {}\n",
    "    def forward(self, inputs): raise NotImplementedError\n",
    "    def backward(self, grad_out): raise NotImplementedError\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x): self.cache = x; return np.maximum(0, x)\n",
    "    def backward(self, grad_out): return grad_out * (self.cache > 0)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.cache = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.cache\n",
    "    def backward(self, grad_out): return grad_out\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.params['W'] = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
    "        self.params['b'] = np.zeros(output_dim)\n",
    "    def forward(self, x): self.cache = x; return x @ self.params['W'] + self.params['b']\n",
    "    def backward(self, grad_out):\n",
    "        self.grads['W'] = self.cache.T @ grad_out\n",
    "        self.grads['b'] = np.sum(grad_out, axis=0)\n",
    "        return grad_out @ self.params['W'].T\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def forward(self, x): self.cache = x.shape; return x.reshape(x.shape[0], -1)\n",
    "    def backward(self, grad_out): return grad_out.reshape(self.cache)\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, in_c, out_c, kernel, stride=1, padding=0, name=\"\"):\n",
    "        super().__init__()\n",
    "        self.name, self.stride, self.padding, self.kernel, self.out_c = name, stride, padding, kernel, out_c\n",
    "        self.params['W'] = np.random.randn(out_c, in_c, kernel, kernel) * np.sqrt(2./(in_c*kernel*kernel))\n",
    "        self.params['b'] = np.zeros(out_c)\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        h_out = (H + 2*self.padding - self.kernel)//self.stride + 1\n",
    "        w_out = (W + 2*self.padding - self.kernel)//self.stride + 1\n",
    "        self.x_col = im2col_indices(x, self.kernel, self.kernel, self.padding, self.stride)\n",
    "        W_col = self.params['W'].reshape(self.out_c, -1)\n",
    "        out = (W_col @ self.x_col) + self.params['b'].reshape(-1, 1)\n",
    "        out = out.reshape(self.out_c, h_out, w_out, N).transpose(3, 0, 1, 2)\n",
    "        self.cache = x\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        self.grads['b'] = np.sum(grad_out, axis=(0, 2, 3))\n",
    "        grad_reshaped = grad_out.transpose(1, 2, 3, 0).reshape(self.out_c, -1)\n",
    "        self.grads['W'] = (grad_reshaped @ self.x_col.T).reshape(self.params['W'].shape)\n",
    "        W_col = self.params['W'].reshape(self.out_c, -1)\n",
    "        dx_col = W_col.T @ grad_reshaped\n",
    "        return col2im_indices(dx_col, self.cache.shape, self.kernel, self.kernel, self.padding, self.stride)\n",
    "\n",
    "class MaxPool2D(Layer):\n",
    "    def __init__(self, size, stride, name=\"\"):\n",
    "        super().__init__()\n",
    "        self.name, self.size, self.stride = name, size, stride\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        h_out, w_out = (H-self.size)//self.stride+1, (W-self.size)//self.stride+1\n",
    "        x_reshaped = x.reshape(N * C, 1, H, W)\n",
    "        self.x_col = im2col_indices(x_reshaped, self.size, self.size, 0, self.stride)\n",
    "        max_idx = np.argmax(self.x_col, axis=0)\n",
    "        out = self.x_col[max_idx, np.arange(self.x_col.shape[1])]\n",
    "        out = out.reshape(h_out, w_out, N, C).transpose(2, 3, 0, 1)\n",
    "        self.cache = (x, max_idx)\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        x, max_idx = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        grad_flat = grad_out.transpose(2, 3, 0, 1).ravel()\n",
    "        dx_col = np.zeros_like(self.x_col)\n",
    "        dx_col[max_idx, np.arange(self.x_col.shape[1])] = grad_flat\n",
    "        dx = col2im_indices(dx_col, (N*C, 1, H, W), self.size, self.size, 0, self.stride)\n",
    "        return dx.reshape(x.shape)\n",
    "\n",
    "class AvgPool2D(Layer):\n",
    "    def __init__(self, size, stride, name=\"\"):\n",
    "        super().__init__()\n",
    "        self.name, self.size, self.stride = name, size, stride\n",
    "        self.cache = None\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        N, C, H, W = x.shape\n",
    "        h_out = (H - self.size) // self.stride + 1\n",
    "        w_out = (W - self.size) // self.stride + 1\n",
    "        x_reshaped = x.reshape(N * C, 1, H, W)\n",
    "        x_col = im2col_indices(x_reshaped, self.size, self.size, 0, self.stride)\n",
    "        out = np.mean(x_col, axis=0)\n",
    "        out = out.reshape(h_out, w_out, N, C).transpose(2, 3, 0, 1)\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        x = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        # ******** 代码修正处 ********\n",
    "        # 将错误的元组解包修正为两个独立的赋值语句\n",
    "        h_out = (H - self.size) // self.stride + 1\n",
    "        w_out = (W - self.size) // self.stride + 1\n",
    "        # ******** 修正结束 ********\n",
    "        pool_area = self.size * self.size\n",
    "        grad_flat = grad_out.transpose(2, 3, 0, 1).ravel()\n",
    "        num_patches = N * C * h_out * w_out\n",
    "        dx_col = np.ones((self.size * self.size, num_patches)) \n",
    "        dx_col *= (grad_flat / pool_area)\n",
    "        dx = col2im_indices(dx_col, (N * C, 1, H, W), self.size, self.size, 0, self.stride)\n",
    "        return dx.reshape(x.shape)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Inception 模块定义 (无变化)\n",
    "# =============================================================================\n",
    "class InceptionModule(Layer):\n",
    "    def __init__(self, in_c, c_1x1, c_3x3_r, c_3x3, c_5x5_r, c_5x5, c_pool, name=\"\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.b1_conv = Conv2D(in_c, c_1x1, 1, name=f\"{name}_b1_conv\")\n",
    "        self.b2_conv_r = Conv2D(in_c, c_3x3_r, 1, name=f\"{name}_b2_conv_r\"); self.b2_relu_r = ReLU()\n",
    "        self.b2_conv = Conv2D(c_3x3_r, c_3x3, 3, padding=1, name=f\"{name}_b2_conv\")\n",
    "        self.b3_conv_r = Conv2D(in_c, c_5x5_r, 1, name=f\"{name}_b3_conv_r\"); self.b3_relu_r = ReLU()\n",
    "        self.b3_conv = Conv2D(c_5x5_r, c_5x5, 5, padding=2, name=f\"{name}_b3_conv\")\n",
    "        self.b4_pool = MaxPool2D(3, stride=1, name=f\"{name}_b4_pool\")\n",
    "        self.b4_conv = Conv2D(in_c, c_pool, 1, name=f\"{name}_b4_conv\")\n",
    "        self.layers = [self.b1_conv, self.b2_conv_r, self.b2_relu_r, self.b2_conv, self.b3_conv_r, self.b3_relu_r, self.b3_conv, self.b4_pool, self.b4_conv]\n",
    "        self.channel_counts = [c_1x1, c_3x3, c_5x5, c_pool]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.b1_conv.forward(x)\n",
    "        t = self.b2_conv_r.forward(x); t = self.b2_relu_r.forward(t); out2 = self.b2_conv.forward(t)\n",
    "        t = self.b3_conv_r.forward(x); t = self.b3_relu_r.forward(t); out3 = self.b3_conv.forward(t)\n",
    "        x_pad = np.pad(x, ((0,0),(0,0),(1,1),(1,1)), 'constant')\n",
    "        t = self.b4_pool.forward(x_pad); out4 = self.b4_conv.forward(t)\n",
    "        return np.concatenate([out1, out2, out3, out4], axis=1)\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        c1, c2, c3, _ = self.channel_counts\n",
    "        grad1 = grad_out[:, :c1, :, :]; grad2 = grad_out[:, c1:c1+c2, :, :]; grad3 = grad_out[:, c1+c2:c1+c2+c3, :, :]; grad4 = grad_out[:, c1+c2+c3:, :, :]\n",
    "        dx1 = self.b1_conv.backward(grad1)\n",
    "        t = self.b2_conv.backward(grad2); t = self.b2_relu_r.backward(t); dx2 = self.b2_conv_r.backward(t)\n",
    "        t = self.b3_conv.backward(grad3); t = self.b3_relu_r.backward(t); dx3 = self.b3_conv_r.backward(t)\n",
    "        t = self.b4_conv.backward(grad4); dx4_pad = self.b4_pool.backward(t); dx4 = dx4_pad[:, :, 1:-1, 1:-1]\n",
    "        return dx1 + dx2 + dx3 + dx4\n",
    "\n",
    "# =============================================================================\n",
    "# 3. GoogLeNet 模型定义 (无变化)\n",
    "# =============================================================================\n",
    "def get_param_layers(layers):\n",
    "    param_layers = []\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, InceptionModule): param_layers.extend(get_param_layers(layer.layers))\n",
    "        elif hasattr(layer, 'params') and layer.params: param_layers.append(layer)\n",
    "    return param_layers\n",
    "\n",
    "class GoogLeNet:\n",
    "    def __init__(self):\n",
    "        self.stem = [Conv2D(3, 32, 3, padding=1, name=\"stem_conv1\"), ReLU(), Conv2D(32, 64, 3, padding=1, name=\"stem_conv2\"), ReLU(), MaxPool2D(3, stride=2, name=\"stem_pool\")]\n",
    "        self.inception_trunk = [InceptionModule(64,  32, 16, 64, 8, 16, 16, name=\"incep1\"), InceptionModule(128, 64, 32, 96, 16, 32, 32, name=\"incep2\"), MaxPool2D(3, stride=2, name=\"pool2\")]\n",
    "        self.classifier = [AvgPool2D(7, stride=1, name=\"final_pool\"), Flatten(), Dense(224, 10, name=\"final_dense\"), Softmax()]\n",
    "        self.layers = self.stem + self.inception_trunk + self.classifier\n",
    "        self.param_layers = get_param_layers(self.layers)\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers: x = layer.forward(x)\n",
    "        return x\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers): grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 损失函数和优化器 (无变化)\n",
    "# =============================================================================\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true): self.y_pred, self.y_true = y_pred, y_true; m = y_true.shape[0]; return -np.sum(y_true * np.log(y_pred + 1e-12)) / m\n",
    "    def backward(self): m = self.y_true.shape[0]; return (self.y_pred - self.y_true) / m\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, layers, learning_rate=0.01): self.layers, self.lr = layers, learning_rate\n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params') and layer.params:\n",
    "                for key in layer.grads: layer.params[key] -= self.lr * layer.grads.get(key, 0)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. 训练过程 (无变化)\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"正在加载 CIFAR-10 数据集...\")\n",
    "    try: from sklearn.datasets import fetch_openml\n",
    "    except ImportError: print(\"请安装 scikit-learn: pip install scikit-learn\"); exit()\n",
    "\n",
    "    cifar = fetch_openml('CIFAR_10', version=1, as_frame=False, parser='auto')\n",
    "    X, y = cifar.data.astype('float32') / 255.0, cifar.target.astype(np.uint8)\n",
    "    X = X.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    x_train, x_test = X[:50000], X[50000:60000]\n",
    "    y_train, y_test = y[:50000], y[50000:60000]\n",
    "    y_train_one_hot = to_categorical_numpy(y_train, 10)\n",
    "    \n",
    "    num_train_samples = 64\n",
    "    print(f\"!!! 警告: 为快速演示，仅使用 {num_train_samples} 个样本进行训练。!!!\")\n",
    "    x_train, y_train_one_hot = x_train[:num_train_samples], y_train_one_hot[:num_train_samples]\n",
    "\n",
    "    print(\"数据加载和预处理完成。\")\n",
    "    model = GoogLeNet()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.param_layers, learning_rate=0.001)\n",
    "\n",
    "    epochs = 1\n",
    "    batch_size = 8\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    \n",
    "    print(f\"\\n开始训练... (Epochs={epochs}, Batch Size={batch_size})\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = y_train_one_hot[permutation]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch_start_time = time.time()\n",
    "            start_idx, end_idx = i * batch_size, (i+1) * batch_size\n",
    "            x_batch, y_batch = x_train_shuffled[start_idx:end_idx], y_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            y_pred = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(y_pred, y_batch)\n",
    "            epoch_loss += loss\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"    Epoch {epoch+1}, 批次 {i+1}/{num_batches} - 损失: {loss:.4f} - 耗时: {time.time() - batch_start_time:.2f}s\")\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"** Epoch {epoch+1}/{epochs} - 平均损失: {avg_loss:.4f} - 总耗时: {time.time() - start_time:.2f}s **\")\n",
    "\n",
    "    print(\"\\n训练完成。\")\n",
    "    print(\"\\n正在评估模型...\")\n",
    "    num_test_samples = 50\n",
    "    x_test_subset, y_test_subset_labels = x_test[:num_test_samples], y_test[:num_test_samples]\n",
    "    y_test_pred_probs = model.forward(x_test_subset)\n",
    "    y_test_pred_labels = np.argmax(y_test_pred_probs, axis=1)\n",
    "    accuracy = np.mean(y_test_pred_labels == y_test_subset_labels)\n",
    "    print(f\"在 {num_test_samples} 个测试样本上的准确率: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156fb593",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "Hypthesis: the problem is an optimization probelm, deeper models are harder to optimize.\n",
    "- The deeper model should be able to perform at least as well as the shallower model.\n",
    "- A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping.\n",
    "\n",
    "Direct mappings are hard to learn. So instead of learning mapping between output of layer and its input, learn the difference between them---learn the residual.\n",
    "\n",
    "![ResNet](images/image3-1.png)\n",
    "\n",
    "Experiemnts Results:\n",
    "- Able to train very deep networks without degrading(152 layers in ImageNet, 1202 on Cifar)\n",
    "- Deeper networks now achieve lowing training error as expected.\n",
    "- Swept 1st place in all ILSVRC and COCO 2015 competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "535a5388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR-10 数据集...\n",
      "!!! 警告: 为快速演示，仅使用 128 个样本进行训练。!!!\n",
      "数据加载和预处理完成。\n",
      "\n",
      "开始训练... (Epochs=1, Batch Size=16)\n",
      "    Epoch 1, 批次 1/8 - 损失: 6.7784 - 耗时: 0.27s\n",
      "    Epoch 1, 批次 2/8 - 损失: 5.8389 - 耗时: 0.29s\n",
      "    Epoch 1, 批次 3/8 - 损失: 3.1065 - 耗时: 0.29s\n",
      "    Epoch 1, 批次 4/8 - 损失: 2.3059 - 耗时: 0.29s\n",
      "    Epoch 1, 批次 5/8 - 损失: 2.4728 - 耗时: 0.28s\n",
      "    Epoch 1, 批次 6/8 - 损失: 2.4723 - 耗时: 0.27s\n",
      "    Epoch 1, 批次 7/8 - 损失: 2.2864 - 耗时: 0.27s\n",
      "    Epoch 1, 批次 8/8 - 损失: 2.3574 - 耗时: 0.28s\n",
      "** Epoch 1/1 - 总耗时: 2.24s **\n",
      "\n",
      "训练完成。\n",
      "\n",
      "正在评估模型...\n",
      "在 100 个测试样本上的准确率: 8.00%\n"
     ]
    }
   ],
   "source": [
    "# ResNet\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 工具函数 (Utility Functions)\n",
    "# =============================================================================\n",
    "# (复用之前的函数)\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "    out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "    i0 = np.repeat(np.arange(field_height), field_width); i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "    return (k, i, j)\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0: return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def to_categorical_numpy(y, num_classes=10):\n",
    "    y_int = y.astype(int)\n",
    "    y_one_hot = np.zeros((len(y_int), num_classes))\n",
    "    y_one_hot[np.arange(len(y_int)), y_int] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 基础层定义 (Layer Definitions)\n",
    "# =============================================================================\n",
    "# (复用之前已经验证过的基础层)\n",
    "class Layer:\n",
    "    def __init__(self): self.params, self.grads = {}, {}\n",
    "    def forward(self, inputs): raise NotImplementedError\n",
    "    def backward(self, grad_out): raise NotImplementedError\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x): self.cache = x; return np.maximum(0, x)\n",
    "    def backward(self, grad_out): return grad_out * (self.cache > 0)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.cache = exps / np.sum(exps, axis=1, keepdims=True); return self.cache\n",
    "    def backward(self, grad_out): return grad_out\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim, name):\n",
    "        super().__init__(); self.name = name\n",
    "        self.params['W'] = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
    "        self.params['b'] = np.zeros(output_dim)\n",
    "    def forward(self, x): self.cache = x; return x @ self.params['W'] + self.params['b']\n",
    "    def backward(self, grad_out):\n",
    "        self.grads['W'] = self.cache.T @ grad_out; self.grads['b'] = np.sum(grad_out, axis=0)\n",
    "        return grad_out @ self.params['W'].T\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def forward(self, x): self.cache = x.shape; return x.reshape(x.shape[0], -1)\n",
    "    def backward(self, grad_out): return grad_out.reshape(self.cache)\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, in_c, out_c, kernel, stride=1, padding=0, name=\"\"):\n",
    "        super().__init__(); self.name, self.stride, self.padding, self.kernel, self.out_c = name, stride, padding, kernel, out_c\n",
    "        self.params['W'] = np.random.randn(out_c, in_c, kernel, kernel) * np.sqrt(2./(in_c*kernel*kernel))\n",
    "        self.params['b'] = np.zeros(out_c)\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        h_out, w_out = (H + 2*self.padding - self.kernel)//self.stride + 1, (W + 2*self.padding - self.kernel)//self.stride + 1\n",
    "        self.x_col = im2col_indices(x, self.kernel, self.kernel, self.padding, self.stride)\n",
    "        W_col = self.params['W'].reshape(self.out_c, -1)\n",
    "        out = (W_col @ self.x_col) + self.params['b'].reshape(-1, 1)\n",
    "        out = out.reshape(self.out_c, h_out, w_out, N).transpose(3, 0, 1, 2); self.cache = x\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        self.grads['b'] = np.sum(grad_out, axis=(0, 2, 3))\n",
    "        grad_reshaped = grad_out.transpose(1, 2, 3, 0).reshape(self.out_c, -1)\n",
    "        self.grads['W'] = (grad_reshaped @ self.x_col.T).reshape(self.params['W'].shape)\n",
    "        W_col = self.params['W'].reshape(self.out_c, -1)\n",
    "        dx_col = W_col.T @ grad_reshaped\n",
    "        return col2im_indices(dx_col, self.cache.shape, self.kernel, self.kernel, self.padding, self.stride)\n",
    "\n",
    "class GlobalAvgPool2D(Layer):\n",
    "    \"\"\"全局平均池化层\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.cache = x.shape\n",
    "        # 在H和W维度上求平均，保留N和C维度\n",
    "        return np.mean(x, axis=(2, 3))\n",
    "    def backward(self, grad_out):\n",
    "        N, C, H, W = self.cache\n",
    "        # 将梯度广播回原始的H,W维度\n",
    "        grad = grad_out[:, :, np.newaxis, np.newaxis]\n",
    "        return np.repeat(np.repeat(grad, H, axis=2), W, axis=3) / (H * W)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 残差块定义 (The Star of the Show: Residual Block)\n",
    "# =============================================================================\n",
    "class ResidualBlock(Layer):\n",
    "    \"\"\"ResNet的核心，残差块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, name=\"\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        \n",
    "        # 主路径 F(x)\n",
    "        self.conv1 = Conv2D(in_channels, out_channels, 3, stride=stride, padding=1, name=f\"{name}_conv1\")\n",
    "        self.relu1 = ReLU()\n",
    "        self.conv2 = Conv2D(out_channels, out_channels, 3, stride=1, padding=1, name=f\"{name}_conv2\")\n",
    "        \n",
    "        # 捷径路径 (Shortcut)\n",
    "        self.shortcut = Layer() # 占位符\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # 维度不匹配时，使用1x1卷积进行投影\n",
    "            self.shortcut = Conv2D(in_channels, out_channels, 1, stride=stride, name=f\"{name}_shortcut\")\n",
    "        else:\n",
    "            # 维度匹配时，捷径就是恒等映射\n",
    "            self.shortcut.forward = lambda x: x\n",
    "            self.shortcut.backward = lambda grad: grad\n",
    "        \n",
    "        # 最后的激活函数\n",
    "        self.relu2 = ReLU()\n",
    "        \n",
    "        # 收集子层\n",
    "        self.layers = [self.conv1, self.relu1, self.conv2, self.shortcut]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 保存输入，用于捷径连接\n",
    "        identity = x\n",
    "        \n",
    "        # 主路径计算 F(x)\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.relu1.forward(out)\n",
    "        out = self.conv2.forward(out)\n",
    "        \n",
    "        # 捷径路径计算\n",
    "        shortcut_out = self.shortcut.forward(identity)\n",
    "        \n",
    "        # 核心：F(x) + x\n",
    "        out += shortcut_out\n",
    "        \n",
    "        # 最终的激活\n",
    "        out = self.relu2.forward(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        # 链式法则：梯度先通过最后的激活函数\n",
    "        grad = self.relu2.backward(grad_out)\n",
    "        \n",
    "        # 核心：梯度同时流向两个分支\n",
    "        grad_shortcut = self.shortcut.backward(grad)\n",
    "        \n",
    "        # 梯度流经主路径\n",
    "        grad_main = self.conv2.backward(grad)\n",
    "        grad_main = self.relu1.backward(grad_main)\n",
    "        grad_main = self.conv1.backward(grad_main)\n",
    "        \n",
    "        # 最终对输入的梯度是两个分支梯度之和\n",
    "        return grad_main + grad_shortcut\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ResNet 模型定义\n",
    "# =============================================================================\n",
    "def get_param_layers(layers):\n",
    "    param_layers = []\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, ResidualBlock): param_layers.extend(get_param_layers(layer.layers))\n",
    "        elif hasattr(layer, 'params') and layer.params: param_layers.append(layer)\n",
    "    return param_layers\n",
    "\n",
    "class ResNet:\n",
    "    \"\"\"一个为CIFAR-10定制的微型ResNet\"\"\"\n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride, name):\n",
    "        layers = []\n",
    "        # 第一个块可能需要改变步长和通道数\n",
    "        layers.append(block(in_channels, out_channels, stride, name=f\"{name}_block1\"))\n",
    "        # 后续块保持维度不变\n",
    "        for i in range(1, num_blocks):\n",
    "            layers.append(block(out_channels, out_channels, 1, name=f\"{name}_block{i+1}\"))\n",
    "        return layers\n",
    "\n",
    "    def __init__(self):\n",
    "        # Stem: 初始卷积层\n",
    "        self.stem = [Conv2D(3, 16, 3, padding=1, name=\"stem_conv\")]\n",
    "        \n",
    "        # 构建残差层\n",
    "        self.layer1 = self._make_layer(ResidualBlock, 16, 16, 2, stride=1, name=\"res_layer1\") # 32x32\n",
    "        self.layer2 = self._make_layer(ResidualBlock, 16, 32, 2, stride=2, name=\"res_layer2\") # 16x16\n",
    "        self.layer3 = self._make_layer(ResidualBlock, 32, 64, 2, stride=2, name=\"res_layer3\") # 8x8\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = [GlobalAvgPool2D(), Flatten(), Dense(64, 10, name=\"final_dense\"), Softmax()]\n",
    "        \n",
    "        # 整合所有层\n",
    "        self.layers = self.stem + self.layer1 + self.layer2 + self.layer3 + self.classifier\n",
    "        self.param_layers = get_param_layers(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers: x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers): grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 损失函数和优化器\n",
    "# =============================================================================\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true): self.y_pred, self.y_true = y_pred, y_true; m = y_true.shape[0]; return -np.sum(y_true * np.log(y_pred + 1e-12)) / m\n",
    "    def backward(self): m = self.y_true.shape[0]; return (self.y_pred - self.y_true) / m\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, layers, learning_rate=0.01): self.layers, self.lr = layers, learning_rate\n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params') and layer.params:\n",
    "                for key in layer.grads: layer.params[key] -= self.lr * layer.grads.get(key, 0)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. 训练过程\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"正在加载 CIFAR-10 数据集...\")\n",
    "    try: from sklearn.datasets import fetch_openml\n",
    "    except ImportError: print(\"请安装 scikit-learn: pip install scikit-learn\"); exit()\n",
    "\n",
    "    cifar = fetch_openml('CIFAR_10', version=1, as_frame=False, parser='auto')\n",
    "    X, y = cifar.data.astype('float32') / 255.0, cifar.target.astype(np.uint8)\n",
    "    X = X.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    x_train, x_test = X[:50000], X[50000:60000]\n",
    "    y_train, y_test = y[:50000], y[50000:60000]\n",
    "    y_train_one_hot = to_categorical_numpy(y_train, 10)\n",
    "    \n",
    "    # !!! 极度警告: ResNet计算量巨大，必须使用极小的数据集子集 !!!\n",
    "    num_train_samples = 128\n",
    "    print(f\"!!! 警告: 为快速演示，仅使用 {num_train_samples} 个样本进行训练。!!!\")\n",
    "    x_train, y_train_one_hot = x_train[:num_train_samples], y_train_one_hot[:num_train_samples]\n",
    "\n",
    "    print(\"数据加载和预处理完成。\")\n",
    "    model = ResNet()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.param_layers, learning_rate=0.01)\n",
    "\n",
    "    epochs = 1\n",
    "    batch_size = 16\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    \n",
    "    print(f\"\\n开始训练... (Epochs={epochs}, Batch Size={batch_size})\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled, y_train_shuffled = x_train[permutation], y_train_one_hot[permutation]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch_start_time = time.time()\n",
    "            start_idx, end_idx = i * batch_size, (i+1) * batch_size\n",
    "            x_batch, y_batch = x_train_shuffled[start_idx:end_idx], y_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            y_pred = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(y_pred, y_batch)\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"    Epoch {epoch+1}, 批次 {i+1}/{num_batches} - 损失: {loss:.4f} - 耗时: {time.time() - batch_start_time:.2f}s\")\n",
    "        \n",
    "        print(f\"** Epoch {epoch+1}/{epochs} - 总耗时: {time.time() - start_time:.2f}s **\")\n",
    "\n",
    "    print(\"\\n训练完成。\")\n",
    "    print(\"\\n正在评估模型...\")\n",
    "    num_test_samples = 100\n",
    "    x_test_subset, y_test_subset_labels = x_test[:num_test_samples], y_test[:num_test_samples]\n",
    "    y_test_pred_probs = model.forward(x_test_subset)\n",
    "    y_test_pred_labels = np.argmax(y_test_pred_probs, axis=1)\n",
    "    accuracy = np.mean(y_test_pred_labels == y_test_subset_labels)\n",
    "    print(f\"在 {num_test_samples} 个测试样本上的准确率: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "745b0cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR-10 数据集...\n",
      "!!! 警告: 为快速演示，仅使用 32 个样本进行训练。!!!\n",
      "数据加载和预处理完成。\n",
      "\n",
      "开始训练... (Epochs=1, Batch Size=8)\n",
      "    Epoch 1, 批次 1/4 - 损失: 2.3461 - 耗时: 0.31s\n",
      "    Epoch 1, 批次 2/4 - 损失: 2.4282 - 耗时: 0.29s\n",
      "    Epoch 1, 批次 3/4 - 损失: 2.3762 - 耗时: 0.30s\n",
      "    Epoch 1, 批次 4/4 - 损失: 2.2435 - 耗时: 0.27s\n",
      "** Epoch 1/1 - 总耗时: 1.17s **\n",
      "\n",
      "训练完成。\n",
      "\n",
      "正在评估模型...\n",
      "在 50 个测试样本上的准确率: 18.00%\n"
     ]
    }
   ],
   "source": [
    "# DenseNet\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 工具函数 (Utility Functions)\n",
    "# =============================================================================\n",
    "# (这部分代码无变化，保持不变)\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "    out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "    i0 = np.repeat(np.arange(field_height), field_width); i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "    return (k, i, j)\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0: return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def to_categorical_numpy(y, num_classes=10):\n",
    "    y_int = y.astype(int)\n",
    "    y_one_hot = np.zeros((len(y_int), num_classes))\n",
    "    y_one_hot[np.arange(len(y_int)), y_int] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 基础层定义 (Layer Definitions)\n",
    "# =============================================================================\n",
    "# (这部分代码无变化，保持不变)\n",
    "class Layer:\n",
    "    def __init__(self): self.params, self.grads = {}, {}\n",
    "    def forward(self, inputs): raise NotImplementedError\n",
    "    def backward(self, grad_out): raise NotImplementedError\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x): self.cache = x; return np.maximum(0, x)\n",
    "    def backward(self, grad_out): return grad_out * (self.cache > 0)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.cache = exps / np.sum(exps, axis=1, keepdims=True); return self.cache\n",
    "    def backward(self, grad_out): return grad_out\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim, name):\n",
    "        super().__init__(); self.name = name\n",
    "        self.params['W'] = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
    "        self.params['b'] = np.zeros(output_dim)\n",
    "    def forward(self, x): self.cache = x; return x @ self.params['W'] + self.params['b']\n",
    "    def backward(self, grad_out):\n",
    "        self.grads['W'] = self.cache.T @ grad_out; self.grads['b'] = np.sum(grad_out, axis=0)\n",
    "        return grad_out @ self.params['W'].T\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def forward(self, x): self.cache = x.shape; return x.reshape(x.shape[0], -1)\n",
    "    def backward(self, grad_out): return grad_out.reshape(self.cache)\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, in_c, out_c, kernel, stride=1, padding=0, name=\"\"):\n",
    "        super().__init__(); self.name, self.stride, self.padding, self.kernel, self.out_c = name, stride, padding, kernel, out_c\n",
    "        self.params['W'] = np.random.randn(out_c, in_c, kernel, kernel) * np.sqrt(2./(in_c*kernel*kernel))\n",
    "        self.params['b'] = np.zeros(out_c)\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        h_out, w_out = (H + 2*self.padding - self.kernel)//self.stride + 1, (W + 2*self.padding - self.kernel)//self.stride + 1\n",
    "        self.x_col = im2col_indices(x, self.kernel, self.kernel, self.padding, self.stride)\n",
    "        W_col = self.params['W'].reshape(self.out_c, -1)\n",
    "        out = (W_col @ self.x_col) + self.params['b'].reshape(-1, 1)\n",
    "        out = out.reshape(self.out_c, h_out, w_out, N).transpose(3, 0, 1, 2); self.cache = x\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        self.grads['b'] = np.sum(grad_out, axis=(0, 2, 3))\n",
    "        grad_reshaped = grad_out.transpose(1, 2, 3, 0).reshape(self.out_c, -1)\n",
    "        self.grads['W'] = (grad_reshaped @ self.x_col.T).reshape(self.params['W'].shape)\n",
    "        W_col = self.params['W'].reshape(self.out_c, -1)\n",
    "        dx_col = W_col.T @ grad_reshaped\n",
    "        return col2im_indices(dx_col, self.cache.shape, self.kernel, self.kernel, self.padding, self.stride)\n",
    "\n",
    "class AvgPool2D(Layer):\n",
    "    def __init__(self, size, stride, name=\"\"):\n",
    "        super().__init__(); self.name, self.size, self.stride = name, size, stride\n",
    "    def forward(self, x):\n",
    "        self.cache = x; N, C, H, W = x.shape\n",
    "        h_out, w_out = (H-self.size)//self.stride+1, (W-self.size)//self.stride+1\n",
    "        x_reshaped = x.reshape(N * C, 1, H, W)\n",
    "        x_col = im2col_indices(x_reshaped, self.size, self.size, 0, self.stride)\n",
    "        out = np.mean(x_col, axis=0)\n",
    "        return out.reshape(h_out, w_out, N, C).transpose(2, 3, 0, 1)\n",
    "    def backward(self, grad_out):\n",
    "        x = self.cache; N, C, H, W = x.shape\n",
    "        h_out, w_out = (H-self.size)//self.stride+1, (W-self.size)//self.stride+1\n",
    "        pool_area = self.size * self.size\n",
    "        grad_flat = grad_out.transpose(2, 3, 0, 1).ravel()\n",
    "        num_patches = N * C * h_out * w_out\n",
    "        dx_col = np.ones((self.size*self.size, num_patches)) * (grad_flat / pool_area)\n",
    "        dx = col2im_indices(dx_col, (N*C, 1, H, W), self.size, self.size, 0, self.stride)\n",
    "        return dx.reshape(x.shape)\n",
    "\n",
    "class GlobalAvgPool2D(Layer):\n",
    "    def forward(self, x):\n",
    "        self.cache = x.shape; return np.mean(x, axis=(2, 3))\n",
    "    def backward(self, grad_out):\n",
    "        N, C, H, W = self.cache; grad = grad_out[:, :, np.newaxis, np.newaxis]\n",
    "        return np.repeat(np.repeat(grad, H, axis=2), W, axis=3) / (H * W)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DenseNet 核心组件\n",
    "# =============================================================================\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, in_channels, growth_rate, name=\"\"):\n",
    "        super().__init__(); self.name = name\n",
    "        bottleneck_channels = 4 * growth_rate\n",
    "        self.relu1 = ReLU(); self.conv1 = Conv2D(in_channels, bottleneck_channels, 1, name=f\"{name}_conv1\")\n",
    "        self.relu2 = ReLU(); self.conv2 = Conv2D(bottleneck_channels, growth_rate, 3, padding=1, name=f\"{name}_conv2\")\n",
    "        self.layers = [self.relu1, self.conv1, self.relu2, self.conv2]\n",
    "    def forward(self, x):\n",
    "        out = self.relu1.forward(x); out = self.conv1.forward(out)\n",
    "        out = self.relu2.forward(out); out = self.conv2.forward(out)\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        grad = self.conv2.backward(grad_out); grad = self.relu2.backward(grad)\n",
    "        grad = self.conv1.backward(grad); grad = self.relu1.backward(grad)\n",
    "        return grad\n",
    "\n",
    "# ******** 代码修正处 ********\n",
    "class DenseBlock(Layer):\n",
    "    \"\"\"密集连接块\"\"\"\n",
    "    def __init__(self, num_layers, in_channels, growth_rate, name=\"\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.growth_rate = growth_rate\n",
    "        self.dense_layers = []\n",
    "        num_c = in_channels\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(num_c, growth_rate, name=f\"{name}_layer{i+1}\")\n",
    "            self.dense_layers.append(layer)\n",
    "            num_c += growth_rate\n",
    "        self.layers = self.dense_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.dense_layers:\n",
    "            input_features = np.concatenate(features, axis=1)\n",
    "            new_features = layer.forward(input_features)\n",
    "            features.append(new_features)\n",
    "        return np.concatenate(features, axis=1)\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        \"\"\"修正后的反向传播逻辑\"\"\"\n",
    "        for layer in reversed(self.dense_layers):\n",
    "            # 1. 将梯度切分为两部分：\n",
    "            #    grad_new_features: 对应当前层输出的梯度\n",
    "            #    grad_previous_features: 对应之前所有层拼接特征的梯度\n",
    "            grad_new_features = grad_out[:, -self.growth_rate:, :, :]\n",
    "            grad_previous_features = grad_out[:, :-self.growth_rate, :, :]\n",
    "\n",
    "            # 2. 将 grad_new_features 通过当前层反向传播，得到对该层输入的梯度 dx\n",
    "            dx = layer.backward(grad_new_features)\n",
    "\n",
    "            # 3. 更新 grad_out，使其成为对上一轮拼接特征的总梯度\n",
    "            #    这是核心：dx 的维度和 grad_previous_features 的维度是相同的\n",
    "            grad_out = grad_previous_features + dx\n",
    "\n",
    "        # 循环结束后，最终的 grad_out 就是对整个 DenseBlock 初始输入的梯度\n",
    "        return grad_out\n",
    "# ******** 修正结束 ********\n",
    "\n",
    "class TransitionLayer(Layer):\n",
    "    def __init__(self, in_channels, out_channels, name=\"\"):\n",
    "        super().__init__(); self.name = name\n",
    "        self.relu = ReLU(); self.conv = Conv2D(in_channels, out_channels, 1, name=f\"{name}_conv\")\n",
    "        self.pool = AvgPool2D(2, 2, name=f\"{name}_pool\"); self.layers = [self.relu, self.conv, self.pool]\n",
    "    def forward(self, x):\n",
    "        out = self.relu.forward(x); out = self.conv.forward(out); out = self.pool.forward(out)\n",
    "        return out\n",
    "    def backward(self, grad_out):\n",
    "        grad = self.pool.backward(grad_out); grad = self.conv.backward(grad); grad = self.relu.backward(grad)\n",
    "        return grad\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DenseNet 模型定义 (无变化)\n",
    "# =============================================================================\n",
    "def get_param_layers(layers):\n",
    "    param_layers = []\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, (DenseBlock, TransitionLayer, DenseLayer)): param_layers.extend(get_param_layers(layer.layers))\n",
    "        elif hasattr(layer, 'params') and layer.params: param_layers.append(layer)\n",
    "    return param_layers\n",
    "\n",
    "class DenseNet:\n",
    "    def __init__(self, growth_rate=12, compression=0.5):\n",
    "        self.growth_rate = growth_rate; num_c = 16\n",
    "        self.stem = [Conv2D(3, num_c, 3, padding=1, name=\"stem_conv\")]\n",
    "        self.block1 = DenseBlock(num_layers=3, in_channels=num_c, growth_rate=growth_rate, name=\"block1\"); num_c += 3 * growth_rate\n",
    "        num_c_trans1 = int(num_c * compression)\n",
    "        self.trans1 = TransitionLayer(num_c, num_c_trans1, name=\"trans1\"); num_c = num_c_trans1\n",
    "        self.block2 = DenseBlock(num_layers=3, in_channels=num_c, growth_rate=growth_rate, name=\"block2\"); num_c += 3 * growth_rate\n",
    "        self.final_relu = ReLU()\n",
    "        self.classifier = [GlobalAvgPool2D(), Flatten(), Dense(num_c, 10, name=\"final_dense\"), Softmax()]\n",
    "        self.layers = self.stem + [self.block1, self.trans1, self.block2, self.final_relu] + self.classifier\n",
    "        self.param_layers = get_param_layers(self.layers)\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers: x = layer.forward(x)\n",
    "        return x\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers): grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 损失函数和优化器 (无变化)\n",
    "# =============================================================================\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true): self.y_pred, self.y_true = y_pred, y_true; m = y_true.shape[0]; return -np.sum(y_true * np.log(y_pred + 1e-12)) / m\n",
    "    def backward(self): m = self.y_true.shape[0]; return (self.y_pred - self.y_true) / m\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, layers, learning_rate=0.01): self.layers, self.lr = layers, learning_rate\n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params') and layer.params:\n",
    "                for key in layer.grads: layer.params[key] -= self.lr * layer.grads.get(key, 0)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. 训练过程 (无变化)\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"正在加载 CIFAR-10 数据集...\")\n",
    "    try: from sklearn.datasets import fetch_openml\n",
    "    except ImportError: print(\"请安装 scikit-learn: pip install scikit-learn\"); exit()\n",
    "\n",
    "    cifar = fetch_openml('CIFAR_10', version=1, as_frame=False, parser='auto')\n",
    "    X, y = cifar.data.astype('float32') / 255.0, cifar.target.astype(np.uint8)\n",
    "    X = X.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    x_train, x_test = X[:50000], X[50000:60000]\n",
    "    y_train, y_test = y[:50000], y[50000:60000]\n",
    "    y_train_one_hot = to_categorical_numpy(y_train, 10)\n",
    "    \n",
    "    num_train_samples = 32\n",
    "    print(f\"!!! 警告: 为快速演示，仅使用 {num_train_samples} 个样本进行训练。!!!\")\n",
    "    x_train, y_train_one_hot = x_train[:num_train_samples], y_train_one_hot[:num_train_samples]\n",
    "\n",
    "    print(\"数据加载和预处理完成。\")\n",
    "    model = DenseNet()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.param_layers, learning_rate=0.01)\n",
    "\n",
    "    epochs = 1\n",
    "    batch_size = 8\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    \n",
    "    print(f\"\\n开始训练... (Epochs={epochs}, Batch Size={batch_size})\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_train_shuffled, y_train_shuffled = x_train[permutation], y_train_one_hot[permutation]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch_start_time = time.time()\n",
    "            start_idx, end_idx = i * batch_size, (i+1) * batch_size\n",
    "            x_batch, y_batch = x_train_shuffled[start_idx:end_idx], y_train_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            y_pred = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(y_pred, y_batch)\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"    Epoch {epoch+1}, 批次 {i+1}/{num_batches} - 损失: {loss:.4f} - 耗时: {time.time() - batch_start_time:.2f}s\")\n",
    "        \n",
    "        print(f\"** Epoch {epoch+1}/{epochs} - 总耗时: {time.time() - start_time:.2f}s **\")\n",
    "\n",
    "    print(\"\\n训练完成。\")\n",
    "    print(\"\\n正在评估模型...\")\n",
    "    num_test_samples = 50\n",
    "    x_test_subset, y_test_subset_labels = x_test[:num_test_samples], y_test[:num_test_samples]\n",
    "    y_test_pred_probs = model.forward(x_test_subset)\n",
    "    y_test_pred_labels = np.argmax(y_test_pred_probs, axis=1)\n",
    "    accuracy = np.mean(y_test_pred_labels == y_test_subset_labels)\n",
    "    print(f\"在 {num_test_samples} 个测试样本上的准确率: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fd798",
   "metadata": {},
   "source": [
    "### Graph Convolution\n",
    "\n",
    "### Spherical Convolution\n",
    "\n",
    "### DeConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7430d27",
   "metadata": {},
   "source": [
    "**What is wrong with CNNs**\n",
    "\n",
    "- The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster ---Hinton.\n",
    "\n",
    "Computer graphics takes internal representation of objects and produces an image (rendering, i.e., inverse graphics).Human brain does the opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3e9c1",
   "metadata": {},
   "source": [
    "### Capsule Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capsule Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928ee40",
   "metadata": {},
   "source": [
    "### Kolmogorov-Arnold Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc94ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Arnold Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b807a7",
   "metadata": {},
   "source": [
    "### Convolutional KAN(CKAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c682d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CKAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
